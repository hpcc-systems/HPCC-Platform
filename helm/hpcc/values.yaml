# Default values for hpcc.

global:
  # Settings in the global section apply to all HPCC components in all subcharts

  image:
    ## It is recommended to name a specific version rather than latest, for any non-trivial deployment
    #version: latest
    root: "hpccsystems"    # change this if you want to pull your images from somewhere other than DockerHub hpccsystems
    pullPolicy: IfNotPresent
  
  # logging sets the default logging information for all components. Can be overridden locally
  logging:
    detail: 100

  # Specify a defaultEsp to control which eclservices service is returned from Std.File.GetEspURL, and other uses
  # If not specified, the first esp component that exposes eclservices application is assumed.
  # Can also be overridden locally in individual components
  ## defaultEsp: eclservices
  
  egress:
    ## If restricted is set, NetworkPolicies will include egress restrictions to allow connections from pods only to the minimum required by the system
    ## Set to false to disable all egress policy restrictions (not recommended)
    restricted: true
    
    ## The kube-system namespace is not generally labelled by default - to enable more restrictive egress control for dns lookups we need to be told the label
    ## If not provided, DNS lookups on port 53 will be allowed to connect anywhere
    ## The namespace may be labelled using a command such as "kubectl label namespace kube-system name=kube-system"
    # kubeSystemLabel: "kube-system"

    ## To properly allow access to the kubectl API from pods that need it, the cidr of the kubectl endpoint needs to be supplied
    ## This may be obtained via "kubectl get endpoints --namespace default kubernetes"
    ## If these are not supplied, egress controls will allow access to any IPs/ports from any pod where API access is needed
    # kubeApiCidr: 172.17.0.3/32  
    # kubeApiPort: 7443

  cost:
    moneyLocale: "en_US.UTF-8"
    # perCpu: 0.126

## placement
## The placement is responsible for finding the best Node for a Pod. It can be configured
## through placement to include an array of objects to configure the Kubernetes Scheduler.
## It must have a "pods" list which tells which pod the settings will be applied to.
## The list item with suffix "-" and <cluster name> means it is Job which tries to distinguish
## from the same pod name for Development type.
## Here are the HPCC Systems Pods' name or prefixs:
##   eclwatch, hthor, eclservices, eclqueries, esdl-sandbox, sql2ecl
##   mydali, roxie-workunit, myeclccserver, roxie-toposerver
##   <cluster name>-thor-eclagent, <cluster name>-thor-thoragent
##   <cluster name>-hthor-, <cluster-name>-compile-, <cluster name>-thormanager-,
##   <cluster name>-thorworker-, <cluster name>-roxie-agent-
## The settings can also be HPCC Systems component types: dali, esp, eclagent, eclccserver,
## roxie, thor, or a cluster name which is from the array of items of each type.
## Supported placement configuration:
## 1) nodeSelector
## 2) taints/tolerations
## 3) affinity
##    There is no schema check for the content of affinity. Reference
##    https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
## 4) schedulerName: list of scheduler profile names. This requires Kubernetes 1.20.0
##    beta and later releases
## Examples:
#placement:
#- pods: ["eclwatch", "roxie-workunit", "myeclccserver-compile-", "mydali"]
#  nodeSelector:
#    name: npone
#- pods: ["thor-thorworker-", "thor-thor-thoragent", "thor-thormanager-","thor-thor-eclagent","thor-hthor-"]
#  nodeSelector:
#    name: nptwo
#  tolerations:
#  - key: "name"
#    operator: "Equal"
#    value: "nptwo"
#    effect: "NoSchedule"
#- pods: ["thor-thorworker-"]
#  affinity:
#    nodeAffinity:
#      requiredDuringSchedulingIgnoredDuringExecution:
#        nodeSelectorTerms:
#        - matchExpressions:
#           - key: kubernetes.io/e2e-az-name
#             operator: In
#             values:
#             - e2e-az1
#             - e2e-az2
#- pods: ["sql2ecl"]
#  schedulerNames: ["my-scheduler"]
## The settings will be applied to all thor pods
#- pods: ["thor"]
#  nodeSelector:
#    name: nptwo
#  tolerations:
#  - key: "name"
#    operator: "Equal"
#    value: "nptwo"
#    effect: "NoSchedule"

## storage:
##
## If storage.[type].existingClaim is defined, a Persistent Volume Claim must
## exist with that name. If an existingClaim is specified, storageClass and
## storageSize are not used.
##
## If storage.[type].storageClass is defined, storageClassName: <storageClass>
## If set to "-", storageClassName: "", which disables dynamic provisioning
## If undefined (the default) or set to null, no storageClassName spec is
##   set, choosing the default provisioner.  (gp2 on AWS, standard on
##   GKE, AWS & OpenStack)
##
## storage.[type].forcePermissions=true is required by some types of provisioned
## storage, where the mounted filing system has insufficient permissions to be
## read by the hpcc pods. Examples include using hostpath storage (e.g. on
## minikube and docker for desktop), or using NFS mounted storage.

storage:
  planes:
  #   name: <required>
  #   prefix: <path>                        # Root directory for accessing the plane (if pvc defined), or url to access plane.
  #   numDevices: 1                         # number of devices that are part of the plane
  #   replication: nullptr                  # a list or single item indicating which planes the data should be replicated onto
  #   includeDeviceInPath: false            # Is the device number appended to the mount for the physical path for a file?  (Only required in unusual situations)
  #   hosts: <name>                         # Name of the host group for bare metal - must match the name of the storage plane..
  #   secret: <secret-id>                   # what secret is required to access the files.  This could optionally become a list if required (or add secrets:).
  #   options:                              # not sure if it is needed

  dllStorage:
    storageSize: 3Gi
    storageClass: ""
    # existingClaim: ""
    # forcePermissions: false

  daliStorage:
    storageSize: 1Gi
    storageClass: ""
    # existingClaim: ""
    # forcePermissions: false

  dataStorage:
    storageSize: 1Gi
    storageClass: ""
    # existingClaim: ""
    # forcePermissions: false

## The secrets section contains a set of categories, each of which contain a list of secrets.  The categories deterime which
## components have access to the secrets.
## For each secret:
##   name is the name that is is accessed by within the platform
##   secret is the name of the secret that should be published
secrets:
  #timeout: 300 # timeout period for cached secrets.  Should be similar to the k8s refresh period.

  #Secret categories follow, remove the {} if a secret is defined in a section
  storage:
    ## Secrets that are required for accessing storage.  Currently exposed in the engines, but in the future will
    ## likely be restricted to esp (when it becomes the meta-data provider)
    ## For example, to set the secret associated with the azure storage account "mystorageaccount" use
    ##azure-mystorageaccount: storage-myazuresecret

  ecl:
    ## Category for secrets published to all components that run ecl

  system:
    ## Category for secrets published to all components for system level useage

## The vaults section mirrors the secret section but leverages vault for the storage of secrets.
## There is an additional category for vaults named "ecl-user".  In the future "ecl-user" vault
## secrets will be readable directly from ECL code.  Other secret categories are read internally
## by system components and not exposed directly to ECL code.
##
## For each vault:
##   name is the name that is is accessed by within the platform
##   url is the url used to read a secret from the vault.
##   kind is the type of vault being accessed, or the protocol to use to access the secrets
##   client_secret a kubernetes level secret that contains the client_token used to retrive secrets.
##       if a client_secret is not provided "vault kubernetes auth" will be attempted.

vaults:
  storage:

  ecl:

  ecl-user:
    #ECL code will have direct access to these secrets

  esp:

bundles: []
## Specifying bundles here will cause the indicated bundles to be downloaded and installed automatically
## whenever an eclccserver pod is started
# for example
# - name: DataPatterns

dali:
- name: mydali
  #resources:
  #  cpu: "1"
  #  memory: "4G"

eclagent:
- name: hthor
  ## replicas indicates how many eclagent pods should be started
  replicas: 1
  ## maxActive controls how many workunits may be active at once (per replica)
  maxActive: 4
  ## prefix may be used to set a filename prefix applied to any relative filenames used by jobs submitted to this queue
  prefix: hthor
  ## Set to false if you want to launch each workunit in its own container, true to run as child processes in eclagent pod
  useChildProcesses: false
  ## type may be 'hthor' (the default) or 'roxie', to specify that the roxie engine rather than the hthor engine should be used for eclagent workunit processing
  type: hthor
  ## The following resources apply to child hThor pods when useChildProcesses=false, otherwise they apply to hThor pod.
  #resources:
  #  cpu: "1"
  #  memory: "1G"

- name: roxie-workunit
  replicas: 1
  prefix: roxie_workunit
  maxActive: 20
  useChildProcesses: true
  type: roxie
  #resources:
  #  cpu: "1"
  #  memory: "1G"

eclccserver:
- name: myeclccserver
  replicas: 1
  ## Set to false if you want to launch each workunit compile in its own container, true to run as child processes in eclccserver pod.
  useChildProcesses: false
  ## maxActive controls how many workunit compiles may be active at once (per replica)
  maxActive: 4
  ## Specify a list of queues to listen on if you don't want this eclccserver listening on all queues. If empty or missing, listens on all queues
  listen: []
  ## The following resources apply to child compile pods when useChildProcesses=false, otherwise they apply to eclccserver pod.
  #resources:
  #  cpu: "1"
  #  memory: "4G"
    
esp:
- name: eclwatch
  ## Pre-configured esp applications include eclwatch, eclservices, and eclqueries
  application: eclwatch
  auth: none
  tls: off
  replicas: 1
  ## port can be used to change the local port used by the pod. If omitted, the default port (8880) is used
  port: 8888
  ## servicePort controls the port that this service will be exposed on, either internally to the cluster, or externally
  servicePort: 8010
  ## Specify public: true if you want the service available from outside the cluster. Typically, eclwatch and wsecl are published externally, while eclservices is designed for internal use.
  public: true
  #resources:
  #  cpu: "1"
  #  memory: "2G"

- name: eclservices
  application: eclservices
  auth: none
  tls: off
  replicas: 1
  servicePort: 8010
  public: false
  #resources:
  #  cpu: "250m"
  #  memory: "1G"

- name: eclqueries
  application: eclqueries
  auth: none
  tls: off
  replicas: 1
  public: true
  servicePort: 8002
  #resources:
  #  cpu: "250m"
  #  memory: "1G"

- name: esdl-sandbox
  application: esdl-sandbox
  auth: none
  tls: off
  replicas: 1
  public: true
  servicePort: 8899
  #resources:
  #  cpu: "250m"
  #  memory: "1G"

- name: sql2ecl
  application: sql2ecl
  auth: none
  tls: off
  replicas: 1
  public: true
  servicePort: 8510
  #resources:
  #  cpu: "250m"
  #  memory: "1G"

roxie:
- name: roxie
  disabled: false
  prefix: roxie
  services:
  - name: roxie
    port: 9876
    listenQueue: 200
    numThreads: 30
    external: true
  ## replicas indicates the number of replicas per channel
  replicas: 2  
  numChannels: 2
  ## Set serverReplicas to indicate a separate replicaSet of roxie servers, with agent nodes not acting as servers
  serverReplicas: 0
  topoReplicas: 1
  ## Set localAgent to true for a scalable cluster of "single-node" roxie servers, each implementing all channels locally
  localAgent: false
  useAeron: false
  #channelResources:
  #  cpu: "4"
  #  memory: "4G"
  #serverResources:
  #  cpu: "1"
  #  memory: "1G"

thor:
- name: thor
  prefix: thor
  numWorkers: 2
  maxJobs: 4
  maxGraphs: 2
  #managerResources:
  #  cpu: "1"
  #  memory: "2G"
  #workerResources:
  #  cpu: "4"
  #  memory: "4G"
  #eclAgentResources:
  #  cpu: "1"
  #  memory: "2G"
- name: thor2
  prefix: thor2
  numWorkers: 3
  maxJobs: 5
  maxGraphs: 3
