# Default values for hpcc.

global:
  # Settings in the global section apply to all HPCC components in all subcharts

  image:
    ## It is recommended to name a specific version rather than latest, for any non-trivial deployment
    #version: latest
    root: "hpccsystems"    # change this if you want to pull your images from somewhere other than DockerHub hpccsystems
    pullPolicy: IfNotPresent
  
  # logging sets the default logging information for all components. Can be overridden locally
  logging:
    detail: 100

  #set to true to add a daemonset to every node in the cluster that will prepull the platform-core image,
  #for faster pod startup
  prepull: false
  
  # Specify a defaultEsp to control which eclservices service is returned from Std.File.GetEspURL, and other uses
  # If not specified, the first esp component that exposes eclservices application is assumed.
  # Can also be overridden locally in individual components
  ## defaultEsp: eclservices

## storage:
##
## If storage.[type].existingClaim is defined, a Persistent Volume Claim must
## exist with that name. If an existingClaim is specified, storageClass and
## storageSize are not used.
##
## If storage.[type].storageClass is defined, storageClassName: <storageClass>
## If set to "-", storageClassName: "", which disables dynamic provisioning
## If undefined (the default) or set to null, no storageClassName spec is
##   set, choosing the default provisioner.  (gp2 on AWS, standard on
##   GKE, AWS & OpenStack)
##
## storage.[type].forcePermissions=true is required by some types of provisioned
## storage, where the mounted filing system has insufficient permissions to be
## read by the hpcc pods. Examples include using hostpath storage (e.g. on
## minikube and docker for desktop), or using NFS mounted storage.

storage:
  planes:
  #   name: <required>
  #   prefix: <path>                        # Root directory for accessing the plane (if pvc defined), or url to access plane.
  #   numDevices: 1                         # number of devices that are part of the plane
  #   replication: nullptr                  # a list or single item indicating which planes the data should be replicated onto
  #   includeDeviceInPath: false            # Is the device number appended to the mount for the physical path for a file?  (Only required in unusual situations)
  #   hosts: <name>                         # Name of the host group for bare metal - must match the name of the storage plane..
  #   secret: <secret-id>                   # what secret is required to access the files.  This could optionally become a list if required (or add secrets:).
  #   options:                              # not sure if it is needed

  dllStorage:
    storageSize: 3Gi
    storageClass: ""
    # existingClaim: ""
    # forcePermissions: false

  daliStorage:
    storageSize: 1Gi
    storageClass: ""
    # existingClaim: ""
    # forcePermissions: false

  dataStorage:
    storageSize: 1Gi
    storageClass: ""
    # existingClaim: ""
    # forcePermissions: false

## The secrets section contains a set of categories, each of which contain a list of secrets.  The categories deterime which
## components have access to the secrets.
## For each secret:
##   name is the name that is is accessed by within the platform
##   secret is the name of the secret that should be published
secrets:
  #timeout: 300 # timeout period for cached secrets.  Should be similar to the k8s refresh period.

  #Secret categories follow, remove the {} if a secret is defined in a section
  storage:
    ## Secrets that are required for accessing storage.  Currently exposed in the engines, but in the future will
    ## likely be restricted to esp (when it becomes the meta-data provider)
    ## For example, to set the secret associated with the azure storage account "mystorageaccount" use
    ##azure-mystorageaccount: storage-myazuresecret

  ecl:
    ## Category for secrets published to all components that run ecl

  all:
    ## Category for secrets published to all components

bundles: []
## Specifying bundles here will cause the indicated bundles to be downloaded and installed automatically
## whenever an eclccserver pod is started
# for example
# - name: DataPatterns

dali:
- name: mydali

eclagent:
- name: hthor
  ## replicas indicates how many eclagent pods should be started
  replicas: 1
  ## maxActive controls how many workunits may be active at once (per replica)
  maxActive: 4
  ## prefix may be used to set a filename prefix applied to any relative filenames used by jobs submitted to this queue
  prefix: hthor
  ## Set to false if you want to launch each workunit in its own container, true to run as child processes in eclagent pod
  useChildProcesses: false
  ## type may be 'hthor' (the default) or 'roxie', to specify that the roxie engine rather than the hthor engine should be used for eclagent workunit processing
  type: hthor

- name: roxie-workunit
  replicas: 1
  prefix: roxie_workunit
  maxActive: 20
  useChildProcesses: true
  type: roxie
    
eclccserver:
- name: myeclccserver
  replicas: 1
  ## Set to false if you want to launch each workunit compile in its own container, true to run as child processes in eclccserver pod.
  useChildProcesses: false
  ## maxActive controls how many workunit compiles may be active at once (per replica)
  maxActive: 4
  ## Specify a list of queues to listen on if you don't want this eclccserver listening on all queues. If empty or missing, listens on all queues
  listen: []
    
esp:
- name: eclwatch
  ## Pre-configured esp applications include eclwatch, eclservices, and eclqueries
  application: eclwatch
  auth: none
  tls: off
  replicas: 1
  ## port can be used to change the local port used by the pod. If omitted, the default port (8880) is used
  port: 8888
  ## servicePort controls the port that this service will be exposed on, either internally to the cluster, or externally
  servicePort: 8010
  ## Specify public: true if you want the service available from outside the cluster. Typically, eclwatch and wsecl are published externally, while eclservices is designed for internal use.
  public: true

- name: eclservices
  application: eclservices
  auth: none
  tls: off
  replicas: 1
  servicePort: 8010
  public: false

- name: eclqueries
  application: eclqueries
  auth: none
  tls: off
  replicas: 1
  public: true
  servicePort: 8002

- name: sql2ecl
  application: sql2ecl
  auth: none
  tls: off
  replicas: 1
  public: true
  servicePort: 8510

roxie:
- name: roxie
  disabled: false
  prefix: roxie
  services:
  - name: roxie
    port: 9876
    listenQueue: 200
    numThreads: 0
    external: true
  numChannels: 2
  ## Set serverReplicas to indicate a separate replicaSet of roxie servers, with agent nodes not acting as servers
  serverReplicas: 0
  topoReplicas: 1
  ## Set localAgent to true for a scalable cluster of "single-node" roxie servers
  localAgent: false
  useAeron: false

thor:
- name: thor
  numSlaves: 2
  globalMemorySize: 4096
  prefix: thor
  eclagent:
    maxActive: 4
  thoragent:
    maxActive: 2
