# Default values for a lightweight Elastic Stack instance which can process HPCC component logs.

##The elasticsearch component can be customized by modifying helm chart values here.
elasticsearch:
  enabled: true
  description: "HPCC Managed Elasticsearch"
  ##See https://github.com/elastic/helm-charts/blob/master/elasticsearch/values.yaml for all available options
  antiAffinity: "soft"  #default is HARD, for minimal systems soft might be necessary
  replicas: 1           #default is 3, for minimal systems 1 replicas should be adequate
  minimumMasterNodes: 1 #default is 2, for minimal systems 1 master node should be adequate
  labels: {"managedby" : "HPCC"}
  clusterHealthCheckParams: "local=true" #local node health status
  #clusterHealthCheckParams: "wait_for_status=yellow&timeout=1s"
  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    resources:
      requests:
        storage: 5Gi
  #persistence:
    #enabled: true
    #labels:
      # Add default labels for the volumeClaimTemplate fo the StatefulSet
      #enabled: false
    #annotations: {}
  extraEnvs:
    # Increase WAIT_ES_READY_IN_SEC if get 'Cluster health check is unknown' message in log
    - name: WAIT_ES_READY_IN_SEC
      value: "120"
  lifecycle:
    postStart:
      exec:
        command:
          - bash
          - -c
          - |
            PIPELINE_NAME=hpccpipeline
            ES_URL=http://localhost:9200
            PIPELINE_REQ_PATH=_ingest/pipeline
            [ -z ${WAIT_ES_READY_IN_SEC} ] && WAIT_ES_READY_IN_SEC=120
            END_TIME=$(expr $(date +%s) \+ ${WAIT_ES_READY_IN_SEC})
            START_FILE=/tmp/.es_start_file
            while [[ $(date +%s) -le ${END_TIME} ]]; do
              [ -f "${START_FILE}" ] && break
              sleep 1
            done
            if [ -f "${START_FILE}" ]; then
              echo 'Cluster is ready to add hpccpipeline'
            else
              echo 'Cluster health check timeout. Will try to add hpccpipeline anyway.'
            fi
            # Need wait elasticsearch container ready to test the hostname with ordinal
            # Only add pipeline in the first elasticsearch master
            if [ ${HOSTNAME##*-} -eq 0 ]; then
              curl -s -X PUT "$ES_URL/$PIPELINE_REQ_PATH/$PIPELINE_NAME/" -H 'Content-Type: application/json' \
                -d'{ "description": "Parses and formats HPCC Systems component log entries", "processors": [{ "grok": { "field": "message", "patterns": ["%{BASE16NUM:hpcc.log.sequence}\\s+%{HPCC_LOG_AUDIENCE:hpcc.log.audience}\\s+%{HPCC_LOG_CLASS:hpcc.log.class}\\s+%{TIMESTAMP_ISO8601:hpcc.log.timestamp}\\s+%{POSINT:hpcc.log.procid}\\s+%{POSINT:hpcc.log.threadid}\\s+%{HPCC_LOG_WUID:hpcc.log.jobid}\\s+%{QUOTEDSTRING:hpcc.log.message}"], "pattern_definitions": { "HPCC_LOG_WUID": "([A-Z][0-9]{8}-[0-9]{6})|(UNK)", "HPCC_LOG_CLASS": "DIS|ERR|WRN|INF|PRO|MET|UNK", "HPCC_LOG_AUDIENCE": "OPR|USR|PRG|AUD|UNK" } } }], "on_failure": [{ "set": { "field": "error.message", "value": "{{ _ingest.on_failure_message }}" } }] }'
              rc=$?
              if [ ${rc} -eq 0 ]; then
                echo 'Successfully added hpccpipeline !'
              else
                echo 'Failed to add hpccpipeline.'
                exit $rc
              fi
            fi

##The filebeat component can be customized by modifying helm chart values here.
filebeat:
  description: "HPCC Managed filebeat"
  ##See https://github.com/elastic/helm-charts/blob/master/filebeat/values.yaml for all available options
  labels: {"managedby" : "HPCC"}
  ## Allows you to add any config files in /usr/share/filebeat
  ## such as filebeat.yml
  filebeatConfig:
    filebeat.yml: |
      filebeat.inputs:
      - type: container
        paths:
          - /var/log/containers/esdl-sandbox-*.log
          - /var/log/containers/eclwatch-*.log
          - /var/log/containers/mydali-*.log
          - /var/log/containers/eclqueries-*.log
          - /var/log/containers/sql2ecl-*.log
          - /var/log/containers/eclservices-*.log
          - /var/log/containers/dfuserver-*.log
          - /var/log/containers/eclscheduler-*.log
          - /var/log/containers/hthor-*.log
          - /var/log/containers/myeclccserver-*.log
          - /var/log/containers/roxie-*.log
          - /var/log/containers/sasha-*.log
          - /var/log/containers/thor-*.log
        #exclude_files: ['(myelk-kibana|myelk-filebeat)+(.*).log']
        processors:
        - add_kubernetes_metadata:
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
        multiline.type: pattern
        multiline.pattern: '^([A-Z-0-9]{8})\s+'
        multiline.negate: true
        multiline.match: after
      #Required if targeting non-default index (filebeat-%{[agent.version]}-%{+yyyy.MM.dd}) such as hpccsystems-%{[fields.log_type]}-%{[agent.version]}-%{+yyyy.MM.dd}, etc.
       #setup.ilm.enabled: false
       #setup.template.overwrite: true
      output.elasticsearch:
        host: '${NODE_NAME}'
        hosts: '${ELASTICSEARCH_HOSTS:elasticsearch-master:9200}'
      #NOTE: Pipeline could be used to provide Log structure and therefore enhance search capabilities of HPCC component log entries
      #      Pipeline must be manually inserted either via Elastic Search API, or Kibana Pipeline ingest UI.
      #      See https://github.com/hpcc-systems/HPCC-Platform/blob/master/helm/managed/logging/elastic/README.md
        pipeline: 'hpccpipeline'
      #  index: "hpccsystems-%{[fields.log_type]}-%{[agent.version]}-%{+yyyy.MM.dd}"
      #setup.template.name: hpccsystems
      #setup.template.pattern: hpccsystems-*
      #setup.template.enabled: true

##The kibana component can be customized by modifying helm chart values here.
kibana:
  enabled: true
  description: "HPCC Managed Kibana"
  ##See https://github.com/elastic/helm-charts/blob/master/kibana/values.yaml for all available options
  labels: {"managedby" : "HPCC"}
  ## Allows you to add any config files in /usr/share/kibana/config/
  ## such as kibana.yml
  #kibanaConfig: {}
  #   kibana.yml: |
  #     key:
  #       nestedkey: value
  service:
    type: "LoadBalancer"
    annotations:
#We want to expose Kibana's UI via link on ECLWATCH...
#Kibana's default back-end FQDN: http://<releasename>-kibana.default.svc.cluster.local:5601
#Exact back-end and/or front-end FQDN should be determined using k8s API, not static info in annotations
      #Required
      hpcc.eclwatch.io/enabled: "true"
      #Optional but informative
      hpcc.eclwatch.io/description: "Provides log management for HPCC component logs"
      hpcc.eclwatch.io/label : "HPCC Logs on Kibana"

      #Optional if targeting custom Kibana UI (different than service defined in this yaml)
      #"hpcc.eclwatch.io/protocol"
      #"hpcc.eclwatch.io/host"
      #"hpcc.eclwatch.io/port"
#Routes landing page to Kibana's discover section
      hpcc.eclwatch.io/path: "/app/discover"

      # This annotation delcares the Azure load balancer for the service as internal rather than internet-visible
      service.beta.kubernetes.io/azure-load-balancer-internal: "true"

      # Enable appropriate annotation for target cloud provider to ensure Kibana access is internal
      #
      #service.beta.kubernetes.io/cce-load-balancer-internal-vpc: "true"
      #cloud.google.com/load-balancer-type: "Internal"
      #service.beta.kubernetes.io/aws-load-balancer-internal: "true"
      #service.beta.kubernetes.io/openstack-internal-load-balancer: "true"
