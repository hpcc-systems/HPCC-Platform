<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="Containerized_Logging">
  <title>Containerized Logging</title>

  <sect1 id="HPCC_Systems_ContainerLogging" role="nobrk">
    <title>Logging Background</title>

    <para>Bare-metal HPCC Systems component logs are written to persistent
    files on local file system, In contrast, containerized HPCC logs are
    ephemeral, and their location is not always well defined. HPCC Systems
    components provide informative application level logs for the purpose of
    debugging problems, auditing actions, and progress monitoring.</para>

    <para>Following the most widely accepted containerized methodologies, HPCC
    Systems component log information is routed to the standard output streams
    rather than local files. In containerized deployments there aren't any
    component logs written to files as in previous editions.</para>

    <para>These logs are written to the standard error (stderr) stream. At the
    node level, the contents of the standard error and out streams are
    redirected to a target location by a container engine. In a Kubernetes
    environment, the Docker container engine redirects the streams to a
    logging driver, which Kubernetes configures to write to a file in JSON
    format. The logs are exposed by Kubernetes via the aptly named "logs"
    command.</para>

    <para>For example:</para>

    <programlisting>&gt;kubectl logs myesp-6476c6659b-vqckq 
&gt;0000CF0F PRG INF 2020-05-12 17:10:34.910 1 10690 "HTTP First Line: GET / HTTP/1.1" 
&gt;0000CF10 PRG INF 2020-05-12 17:10:34.911 1 10690 "GET /, from 10.240.0.4" 
&gt;0000CF11 PRG INF 2020-05-12 17:10:34.911 1 10690 “TxSummary[activeReqs=22; rcv=5ms;total=6ms;]" </programlisting>

    <para>It is important to understand that these logs are ephemeral in
    nature, and may be lost if the pod is evicted, the container crashes, the
    node dies, etc. Due to the nature of containerized systems, related logs
    are likely to originate from various locations and need to be collected
    and processed. It is highly recommended to develop a retention and
    processing strategy based on your needs.</para>

    <para>Many tools are available to help create an appropriate solution
    based on either a do-it-yourself approach, or managed features available
    from cloud providers.</para>

    <para>For the simplest of environments, it might be acceptable to rely on
    the standard Kubernetes process which forwards all contents of
    stdout/stderr to file. However, as the complexity of the cluster grows or
    the importance of retaining the logs' content grows, a cluster-level
    logging architecture should be employed.</para>

    <para>Cluster-level logging for the containerized HPCC Systems cluster can
    be accomplished by including a logging agent on each node. The task of
    each of agent is to expose the logs or push them to a log processing
    back-end. Logging agents are generally not provided out of the box, but
    there are several available such as Elasticsearch and Stackdriver Logging.
    Various cloud providers offer built-in solutions which automatically
    harvest all stdout/err streams and provide dynamic storage and powerful
    analytic tools, and the ability to create custom alerts based on log
    data.</para>

    <para>It is your responsibility to determine the appropriate solution to
    process the streaming log data.</para>

    <sect2 id="HPCC_LogProcessing_Solution">
      <title>Log Processing Solutions</title>

      <para>There are multiple log processing solutions available. You could
      choose to integrate HPCC Systems logging data with any existing logging
      solutions, or to implement another one specifically for HPCC Systems
      data. Starting with HPCC Systems version 8.4, we provide a lightweight,
      yet complete log-processing solution for your convenience. The following
      sections will look at a couple of the possible solutions.</para>
    </sect2>
  </sect1>

  <sect1 id="elastic4HPCC_HelmChart">
    <title>Managed Elastic Stack Solution</title>

    <para>HPCC Systems provides a managed Helm chart,
    <emphasis>elastic4hpcclogs</emphasis> which utilizes the Elastic Stack
    Helm charts for Elastic Search, Filebeats, and Kibana. This chart
    describes a local, minimal Elastic Stack instance for HPCC Systems
    component log processing. Once successfully deployed, HPCC component logs
    produced within the same namespace should be automatically indexed on the
    Elastic Search end-point. Users can query those logs by issuing Elastic
    Search RESTful API queries, or via the Kibana UI (after creating a simple
    index pattern).</para>

    <para>Out of the box, the Filebeat forwards the HPCC component log entries
    to a generically named index: 'hpcc-logs'- &lt;DATE_STAMP&gt; and writes
    the log data into 'hpcc.log.*' prefixed fields. It also aggregates k8s,
    Docker, and system metadata to help the user query the log entries of
    their interest.</para>

    <para>A Kibana index pattern is created automatically based on the default
    filebeat index layout.</para>

    <sect2 id="Installing_helm_logging_charts">
      <title>Installing the elastic4hpcclogs chart</title>

      <para>Installing the provided simple solution is as the name implies,
      simple and a convenient way to gather and filter log data. It is
      installed via our helm charts from the HPCC Systems repository. In the
      HPCC-platform/helm directory, the <emphasis>elastic4hpcclogs</emphasis>
      chart is delivered along with the other HPCC System platform components.
      The next sections will show you how to install and set up the Elastic
      stack logging solution for HPCC Systems.</para>

      <sect3 id="logs_Add_theHPCC_Systems_Repo">
        <title>Add the HPCC Systems Repository</title>

        <para>The delivered Elastic for HPCC Systems chart can be found in the
        HPCC Systems Helm repository. To fetch and deploy the HPCC Systems
        managed charts, add the HPCC Systems Helm repository if you haven't
        done so already:</para>

        <programlisting>helm repo add hpcc https://hpcc-systems.github.io/helm-chart/</programlisting>

        <para>Once this command has completed successfully, the
        <emphasis>elastic4hpcclogs</emphasis> chart will be accessible.</para>

        <para>Confirm the appropriate chart was pulled down.</para>

        <programlisting>helm list</programlisting>

        <para>Issuing the helm list command will display the available HPCC
        Systems charts and repositories. The
        <emphasis>elastic4hpcclogs</emphasis> chart is among them.</para>

        <para><graphic fileref="../../images/CL-Img01-1.jpg" /></para>
      </sect3>

      <sect3 id="Elastic4HPCC_Install_theChart">
        <title>Install the elastic4hpcc chart</title>

        <para>Install the <emphasis>elastic4hpcclogs</emphasis> chart using
        the following command:</para>

        <programlisting>helm install &lt;Instance_Name&gt; hpcc/elastic4hpcclogs </programlisting>

        <para>Provide the name you wish to call your Elastic Search instance
        for the &lt;Instance_Name&gt; parameter. For example, you could call
        your instance "myelk" in which case you would issue the install
        command as follows:</para>

        <programlisting>helm install myelk hpcc/elastic4hpcclogs </programlisting>

        <para>Upon successful completion, the following message is
        displayed:</para>

        <programlisting>Thank you for installing elastic4hpcclogs. 
 A lightweight Elastic Search instance for HPCC component log processing. 

This deployment varies slightly from defaults set by Elastic, please review the effective values. 

PLEASE NOTE: Elastic Search declares PVC(s) which might require explicit manual removal 
  when no longer needed.
</programlisting>

        <para><informaltable colsep="1" frame="all" rowsep="1">
            <?dbfo keep-together="always"?>

            <tgroup cols="2">
              <colspec colwidth="49.50pt" />

              <colspec />

              <tbody>
                <row>
                  <entry><inlinegraphic
                  fileref="../../images/caution.png" /></entry>

                  <entry><emphasis role="bold">IMPORTANT: </emphasis>PLEASE
                  NOTE: Elastic Search declares PVC(s) which might require
                  explicit manual removal when no longer needed. This can be
                  particularly important for some cloud providers which could
                  accrue costs even after no longer using your instance. You
                  should ensure no components (such as PVCs) persist and
                  continue to accrue costs.</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable></para>

        <para>NOTE: Depending on the version of Kubernetes, users might be
        warned about deprecated APIs in the Elastic charts (ClusterRole and
        ClusterRoleBinding are deprecated in v1.17+). Deployments based on
        Kubernetes &lt; v1.22 should not be impacted.</para>
      </sect3>

      <sect3 id="elastic4HPCC_ConfirmingThePodsReady">
        <title>Confirm Your Pods are Ready</title>

        <para>Confirm the Elastic pods are ready. Sometimes after installing,
        pods can take a few seconds to come up. Confirming the pods are in a
        ready state is a good idea before proceeding. To do this, use the
        following command:</para>

        <programlisting>kubectl get pods </programlisting>

        <para>This command returns the following information, displaying the
        status of the of the pods.</para>

        <programlisting>elasticsearch-master-0                    1/1     Running            0          
myelk-filebeat-6wd2g                      1/1     Running            0          
myelk-kibana-68688b4d4d-d489b             1/1     Running            0      </programlisting>

        <para><graphic fileref="../../images/CL-Img02-1.jpg" /></para>

        <para>Once all the pods are indicating a 'ready' state and 'Running',
        including the three components for filebeats, Elastic Search, and
        Kibana (highlighted above) you can proceed.</para>
      </sect3>

      <sect3 id="confirming_elastic_services" role="brk">
        <title>Confirming the Elastic Services</title>

        <para>To confirm the Elastic services are running, issue the following
        command:</para>

        <programlisting>$ kubectl get svc</programlisting>

        <para>This displays the following confirmation information:</para>

        <programlisting>... 
elasticsearch-master ClusterIP 10.109.50.54 &lt;none&gt; 9200/TCP,9300/TCP 68m 
elasticsearch-master-headless ClusterIP None &lt;none&gt; 9200/TCP,9300/TCP 68m 
myelk-kibana LoadBalancer 10.110.129.199 localhost 5601:31465/TCP 68m 
...</programlisting>

        <para>Note: The myelk-kibana service is declared as LoadBalancer for
        convenience.</para>
      </sect3>

      <sect3 id="Configuring_of_Elastic_Stack_Components">
        <title>Configuring of Elastic Stack Components</title>

        <para>You may need or want to customise the Elastic stack components.
        The Elastic component charts values can be overridden as part of the
        HPCC System deployment command.</para>

        <para>For example:</para>

        <programlisting>helm install myelk hpcc/elastic4hpcclogs --set elasticsearch.replicas=2 </programlisting>

        <para>Please see the Elastic Stack GitHub repository for the complete
        list of all Filebeat, Elastic Search, LogStash and Kibana options with
        descriptions.</para>
      </sect3>

      <sect3>
        <title>Use of HPCC Systems Component Logs in Kibana</title>

        <para>Once enabled and running, you can explore and query HPCC Systems
        component logs from the Kibana user interface. Kibana index patterns
        are required to explore Elastic Search data from the Kibana user
        interface. For more information about using the Elastic-Kibana
        interface please refer to the corresponding documentation:</para>

        <para><ulink url="???">https://www.elastic.co/</ulink></para>

        <para>and</para>

        <para><ulink
        url="???">https://www.elastic.co/elastic-stack/</ulink></para>
      </sect3>

      <sect3>
        <title>Configuring logAccess for Elasticstack</title>

        <para>The <emphasis>logAccess</emphasis> feature allows HPCC Systems
        to query and package relevant logs for various features such as the
        ZAP report, WorkUnit helper logs, ECLWatch log viewer, etc.</para>

        <para>Once the logs are migrated or routed to the elastic stack
        instance. The HPCC Systems platform needs to be able to access those
        logs. The way you direct HPCC Systems to do so is by providing a
        values file that includes the log mappings. We have provided a default
        values file and we provide an example command line that inserts that
        values file into your deployment.</para>

        <para>These are the Elastic4HPCCLogs values:</para>

        <programlisting># Configures HPCC logAccess to target elastic4hpcclogs deployment
global:
  logAccess:
    name: "Elastic4HPCCLogs"
    type: "elasticstack"
    connection:
        protocol: "http"
        host: "elasticsearch-master.default.svc.cluster.local"
        port: 9200
    logMaps:
      - type: "global"                             #These settings apply to all log mappings
        storeName: "hpcc-logs*"                    #Logs are expected to be housed in ES inde
        searchColumn: "message"                    #The 'message' field is to be targeted for
        timeStampColumn: "@timestamp"              #The '@timestamp' field contains time log 
      - type: "workunits"                          #Search by workunits specific log mapping
        storeName: "hpcc-logs*"                    # Only needed if differs from global.store
        searchColumn: "hpcc.log.jobid"             # Field containing WU information
      - type: "components"                         #Search by components specific log mapping
        searchColumn: "kubernetes.container.name"  # Field containing container information
      - type: "audience"                           #Search by audience specific log mapping
        searchColumn: "hpcc.log.audience"          # Field containing audience information
      - type: "class"                              #Search by log class specific log mapping
        searchColumn: "hpcc.log.class"             # Field containing log class information
      - type: "instance"                           #Search by log source instance specific ma
        searchColumn: "kubernetes.pod.name"        # Field containing source instance informa
      - type: "host"                               #Search by log source host specific mappin
        searchColumn: "kubernetes.node.hostname"   # Field containing source host information</programlisting>

        <para>You can use the delivered Elastic4HPCCLogs chart, provided on
        https://github.com/hpcc-systems/HPCC-Platform/tree/master/helm/managed/logging/elastic
        or you can add the values there to your customized configuration
        values yaml file. You can then install it using a command such
        as:</para>

        <programlisting>helm install mycluster hpcc/hpcc -f elastic4hpcclogs-hpcc-logaccess.yaml</programlisting>

        <para></para>
      </sect3>
    </sect2>
  </sect1>

  <sect1 id="Azure_AKS_LogAnalytics">
    <title>Azure Log Analytics Solution</title>

    <para>Azure Kubernetes Services (AKS) Azure Log Analytics (ALA) is an
    optional feature designed to help monitor performance and health of
    Kubernetes based clusters. Once enabled and associated a given AKS with an
    active HPCC System cluster, the HPCC component logs are automatically
    captured by Log Analytics. All STDERR/STDOUT data is captured and made
    available for monitoring and/or querying purposes. As is usually the case
    with cloud provider features, cost is a significant consideration and
    should be well understood before implementation. Log content is written to
    the logs store associated with your Log Analytics workspace.</para>

    <sect2>
      <title>Enabling Azure Log Analytics</title>

      <para>Enable Azure's Log Analytics (ALA) on the target AKS cluster using
      one of the following options: Direct command line, Scripted command
      line, or from the Azure portal.</para>

      <para>For more detailed information refer to the Azure
      documentation:</para>

      <para><ulink
      url="https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-onboard">https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-onboard</ulink></para>

      <sect3 id="Direct_CLI">
        <title>Direct Command Line</title>

        <para>To enable the Azure Log Analytics insights from the command
        line:</para>

        <para>You can create a dedicated log analytics workspace manually, or
        you can skip this step and utilize the default workspace
        instead.</para>

        <para>To create a dedicated workspace enter this command:</para>

        <programlisting>az monitor log-analytics workspace create -g myresourcegroup -n myworkspace --query-access Enabled </programlisting>

        <para>To enable Log Analytics feature on a target AKS cluster,
        reference the <emphasis>workspace resource id </emphasis>created in
        the previous step:</para>

        <programlisting>az aks enable-addons -g myresourcegroup -n myaks -a monitoring --workspace-resource-id  \
 "/subscriptions/xyz/resourcegroups/myresourcegroup/providers/ \
  microsoft.operationalinsights/workspaces/myworkspace" </programlisting>
      </sect3>

      <sect3 id="Scripted_ALA_CLI">
        <title>Scripted Command Line</title>

        <para>As a convenience, HPCC Systems provides a script to enable ALA
        (with a dedicated log analytics workspace) on the target AKS
        cluster.</para>

        <para>The <emphasis>enable-loganalytics.sh</emphasis> script is
        located at:</para>

        <para><ulink
        url="https://github.com/hpcc-systems/HPCC-Platform/tree/master/helm/examples/azure/log-analytics">https://github.com/hpcc-systems/HPCC-Platform/tree/master/helm/examples/azure/log-analytics</ulink></para>

        <para>The script requires populating the following values in the
        env-loganalytics environment file.</para>

        <para>Provide these values in the
        <emphasis>env-loganalytics</emphasis> environment file order to create
        a new Azure Log Analytics workspace, associate it with a target AKS
        cluster, and enable the processing of logs:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">LOGANALYTICS_WORKSPACE_NAME</emphasis>
            The desired name for the Azure Log Analytics workspace to be
            associated with target AKS cluster. A new workspace is created if
            this value does not exist</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">LOGANALYTICS_RESOURCE_GROUP</emphasis>
            The Azure resource group associated with the target AKS cluste. A
            new workspace will be associated with this resource group.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">AKS_CLUSTER_NAME</emphasis> The name
            of the target AKS cluster to associate the log analytics
            workspace.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">TAGS</emphasis> The tags associated
            with the new workspace.</para>

            <para>For example: "admin=MyName email=my.email@mycompany.com
            environment=myenv justification=testing"</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">AZURE_SUBSCRIPTION</emphasis>
            [Optional] Ensures this subscription is set before creating the
            new workspace</para>
          </listitem>
        </itemizedlist>

        <para>Once these values are populated, the
        <emphasis>enable-loganalyics.sh</emphasis> script can be executed and
        it will create the log analytics workspace and associate it with the
        target AKS cluster.</para>
      </sect3>

      <sect3 id="el4HPCC_EnableInsights_AzurePortal">
        <title>Azure Portal</title>

        <para>To enable the Azure Log Analytics on the Azure portal:</para>

        <para><orderedlist>
            <listitem>
              <para>Select Target AKS cluster</para>
            </listitem>

            <listitem>
              <para>Select Monitoring</para>
            </listitem>

            <listitem>
              <para>Select Insights</para>
            </listitem>

            <listitem>
              <para>Enable - choose default workspace</para>
            </listitem>
          </orderedlist></para>
      </sect3>
    </sect2>

    <sect2>
      <title>Configure HPCC logAccess for Azure</title>

      <para>The <emphasis>logAccess</emphasis> feature allows HPCC Systems to
      query and package relevant logs for various features such as the ZAP
      report, WorkUnit helper logs, ECLWatch log viewer, etc.</para>

      <sect3 id="ProcureServicePrincipal">
        <title>Procure Service Principal</title>

        <para>Azure requires an Azure Active Directory (AAD) registered
        application in order to grant Log Analytics API access. Procure an AAD
        registered application.</para>

        <para>For more information about registering an Azure Active
        Difrectory see the Azure official documentation:</para>

        <para><ulink
        url="https://docs.microsoft.com/en-us/power-apps/developer/data-platform/walkthrough-register-app-azure-active-directory">https://docs.microsoft.com/en-us/power-apps/developer/data-platform/walkthrough-register-app-azure-active-directory</ulink></para>

        <para>Depending on your Azure subscription structure, it might be
        necessary to request this from a subscription administrator.</para>
      </sect3>

      <sect3 id="ProvideAADRegisteredApplicationInformation">
        <title>Provide AAD Registered Application Information</title>

        <para>HPCC Systems logAccess requires access to the AAD tenant,
        client, token, and target workspace ID via secure secret object. The
        secret is expected to be in the 'esp' category, and named
        <emphasis>'azure-logaccess</emphasis>'.</para>

        <para>The following key value pairs are supported</para>

        <itemizedlist>
          <listitem>
            <para>aad-tenant-id</para>
          </listitem>

          <listitem>
            <para>aad-client-id</para>
          </listitem>

          <listitem>
            <para>aad-client-secret</para>
          </listitem>

          <listitem>
            <para>ala-workspace-id</para>
          </listitem>
        </itemizedlist>

        <para>The 'create-azure-logaccess-secret.sh' script provided
        at:</para>

        <para><ulink
        url="https://github.com/hpcc-systems/HPCC-Platform/tree/master/helm/examples/azure/log-analytics">https://github.com/hpcc-systems/HPCC-Platform/tree/master/helm/examples/azure/log-analytics</ulink></para>

        <para>The script can be used to create the necessary secret.</para>

        <para>Example manual secret creation command (assuming
        ./secrets-templates contains a file named exactly as the above
        keys):</para>

        <programlisting>create-azure-logaccess-secret.sh .HPCC-Platform/helm/examples/azure/log-analytics/secrets-templates/</programlisting>

        <para>Otherwise, create the secret manually.</para>

        <para>Example manual secret creation command (assuming
        ./secrets-templates contains a file named exactly as the above
        keys):</para>

        <programlisting>kubectl create secret generic azure-logaccess \
  --from-file=HPCC-Platform/helm/examples/azure/log-analytics/secrets-templates/ </programlisting>
      </sect3>

      <sect3>
        <title>Configure HPCC logAccess</title>

        <para>The target HPCC Systems deployment should be configured to
        target the above Azure Log Analytics workspace by providing
        appropriate logAccess values (such as
        ./loganalytics-hpcc-logaccess.yaml). The previously created
        azure-logaccess secret must be declared and associated with the esp
        category, this can be accomplished via secrets value yaml (such as
        ./loganalytics-logaccess-secrets.yaml).</para>

        <para>Example use:</para>

        <programlisting>helm install myhpcc hpcc/hpcc \
  -f HPCC-Platform/helm/examples/azure/log-analytics/loganalytics-hpcc-logaccess.yaml \
  -f HPCC-Platform/helm/examples/az</programlisting>
      </sect3>
    </sect2>

    <sect2>
      <title>Accessing HPCC Systems Logs</title>

      <para>The AKS Log Analytics interface on Azure provides
      Kubernetes-centric cluster/node/container-level health metrics
      visualizations, and direct links to container logs via "log analytics"
      interfaces. The logs can be queried via “Kusto” query language
      (KQL).</para>

      <para>See the Azure documentation for specifics on how to query the
      logs.</para>

      <para>Example KQL query for fetching "Transaction summary" log entries
      from an ECLWatch container:</para>

      <programlisting>let ContainerIdList = KubePodInventory 
| where ContainerName =~ 'xyz/myesp' 
| where ClusterId =~ '/subscriptions/xyz/resourceGroups/xyz/providers/Microsoft.
                      ContainerService/managedClusters/aks-clusterxyz' 
| distinct ContainerID; 
ContainerLog 
| where LogEntry contains "TxSummary[" 
| where ContainerID in (ContainerIdList) 
| project LogEntrySource, LogEntry, TimeGenerated, Computer, Image, Name, ContainerID 
| order by TimeGenerated desc 
| render table </programlisting>

      <para>Sample output</para>

      <para><graphic fileref="../../images/CL-Img03-1.jpg" /></para>

      <para>More complex queries can be formulated to fetch specific
      information provided in any of the log columns including unformatted
      data in the log message. The Log Analytics interface facilitates
      creation of alerts based on those queries, which can be used to trigger
      emails, SMS, Logic App execution, and many other actions.</para>
    </sect2>
  </sect1>

  <sect1 id="HPCC_Systems_Application-Level_logs">
    <title>Controlling HPCC Systems Logging Output</title>

    <para>The HPCC Systems logs provide a wealth of information which can be
    used for benchmarking, auditing, debugging, monitoring, etc. The type of
    information provided in the logs and its format is trivially controlled
    via standard Helm configuration. Keep in mind in container mode, every
    line of logging output is liable to incur a cost depending on the provider
    and plan you have and the verbosity should be carefully controlled using
    the following options.</para>

    <para>By default, the component logs are not filtered, and contain the
    following columns:</para>

    <programlisting>MessageID TargetAudience LogEntryClass JobID DateStamp TimeStamp ProcessId ThreadID QuotedLogMessage </programlisting>

    <para>The logs can be filtered by TargetAudience, Category, or Detail
    Level. Further, the output columns can be configured. Logging
    configuration settings can be applied at the global, or component
    level.</para>

    <sect2 id="Target_Audience_Filtering">
      <title>Target Audience Filtering</title>

      <para>The availble target audiences include operator(OPR), user(USR),
      programmer(PRO), audit(ADT), or all. The filter is controlled by the
      &lt;section&gt;.logging.audiences value. The string value is comprised
      of 3 letter codes delimited by the aggregation operator (+) or the
      removal operator (-).</para>

      <para>For example, all component log output to include Programmer and
      User messages only:</para>

      <programlisting>helm install myhpcc ./hpcc --set global.logging.audiences="PRO+USR" </programlisting>
    </sect2>

    <sect2 id="Target_Category_Filtering">
      <title>Target Category Filtering</title>

      <para>The available target categories include disaster(DIS), error(ERR),
      information(INF), warning(WRN), progress(PRO), metrics(MET). The
      category (or class) filter is controlled by the
      &lt;section&gt;.logging.classes value, comprised of 3 letter codes
      delimited by the aggregation operator (+) or the removal operator
      (-).</para>

      <para>For example, the mydali instance's log output to include all
      classes except for progress:</para>

      <programlisting>helm install myhpcc ./hpcc --set dali[0].logging.classes="ALL-PRO" --set dali[0].name="mydali" </programlisting>
    </sect2>

    <sect2 id="Log_Detail_Level_Configuration">
      <title>Log Detail Level Configuration</title>

      <para>Log output verbosity can be adjusted from "critical messages only"
      (1) up to "report all messages" (100). The default log level is rather
      high (80) and should be adjusted accordingly.</para>

      <para>For example, verbosity should be medium for all components:</para>

      <programlisting>helm install myhpcc ./hpcc --set global.logging.detail="50" </programlisting>
    </sect2>

    <sect2 id="Log_Data_Column_Configuration">
      <title>Log Data Column Configuration</title>

      <para>The available log data columns include messageid(MID),
      audience(AUD), class(CLS), date(DAT), time(TIM), node(NOD),
      millitime(MLT), microtime(MCT), nanotime(NNT), processid(PID),
      threadid(TID), job(JOB), use(USE), session(SES), code(COD),
      component(COM), quotedmessage(QUO), prefix(PFX), all(ALL), and
      standard(STD). The log data columns (or fields) configuration is
      controlled by the &lt;section&gt;.logging.fields value, comprised of 3
      letter codes delimited by the aggregation operator (+) or the removal
      operator (-).</para>

      <para>For example, all component log output should include the standard
      columns except the job ID column:</para>

      <programlisting>helm install myhpcc ./hpcc --set global.logging.fields="STD-JOB" </programlisting>

      <para>Adjustment of per-component logging values can require assertion
      of multiple component specific values, which can be inconvinient to do
      via the --set command line parameter. In these cases, a custom values
      file could be used to set all required fields.</para>

      <para>For example, the ESP component instance 'eclwatch' should output
      minimal log:</para>

      <programlisting>helm install myhpcc ./hpcc --set -f ./examples/logging/esp-eclwatch-low-logging-values.yaml</programlisting>
    </sect2>

    <sect2>
      <title>Asychronous Logging Configuration</title>

      <para>By default log entries will be created and logged asynchronously,
      so as not to block the client that is logging. Log entries will be held
      in a queue and output on a background thread. This queue has a maximum
      depth, once hit, the client will block waiting for capacity.
      Alternatively, the behaviour can be be configured such that when this
      limit is hit, logging entries are dropped and lost to avoid any
      potential blocking.</para>

      <para>NB: normally it is expected that the logging stack will keep up
      and the default queue limit will be sufficient to avoid any
      blocking.</para>

      <para>The defaults can be configured by setting
      &lt;section&gt;.logging.queueLen and/or
      &lt;section&gt;.logging.queueDrop.</para>

      <para>Setting &lt;section&gt;.logging.queueLen to 0, will disabled
      asynchronous logging, i.e. each log will block until completed.</para>

      <para>Setting &lt;section&gt;.logging.queueDrop to a non-zero (N) value
      will cause N logging entries from the queue to be discarded if the
      queueLen is reached.</para>
    </sect2>
  </sect1>
</chapter>
