<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="DPLib">
  <title>Data Patterns Library</title>

  <para>This section covers the Data Patterns methods in the ECL Standard
  Library.</para>

  <sect1 id="DPProfile">
    <title>Profile</title>

    <para><emphasis role="bold">STD.DataPatterns.Profile<indexterm>
        <primary>STD.DataPatterns.Profiles</primary>
      </indexterm><indexterm>
        <primary>DataPatterns.Profiles</primary>
      </indexterm><indexterm>
        <primary>Profile</primary>
      </indexterm></emphasis></para>

    <para>Function macro for profiling all or part of a dataset. The output is
    a dataset containing the following information for each profiled
    attribute:</para>

    <para>Most profile outputs can be disabled. See the 'features' argument,
    below.</para>

    <para>Data patterns can give you an idea of what your data looks like when
    it is expressed as a (human-readable) string. The function converts each
    character of the string into a fixed character palette to producing a
    "data pattern" and then counts the number of unique patterns for that
    attribute.</para>

    <para>The most- and least-popular patterns from the data will be shown in
    the output, along with the number of times that pattern appears and an
    example (randomly chosen from the actual data). The character palette used
    is:</para>

    <para>A Any uppercase letter</para>

    <para>a Any lowercase letter</para>

    <para>9 Any numeric digit</para>

    <para>B A boolean value (true or false)</para>

    <para>All other characters are left as-is in the pattern.</para>

    <para><emphasis role="bold">Parameters</emphasis></para>

    <informaltable colsep="1" frame="all" rowsep="1">
      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis>inFile</emphasis></entry>

            <entry>The dataset to process; this could be a child dataset (e.g.
            inFile.childDS); REQUIRED</entry>
          </row>

          <row>
            <entry><emphasis>fieldListStr</emphasis></entry>

            <entry>A string containing a comma-delimited list of attribute
            names to process; use an empty string to process all attributes in
            inFile; OPTIONAL, defaults to an empty string</entry>
          </row>

          <row>
            <entry><emphasis>maxPatterns</emphasis></entry>

            <entry>The maximum number of patterns (both popular and rare) to
            return for each attribute; OPTIONAL, defaults to 100</entry>
          </row>

          <row>
            <entry><emphasis>maxPatternLen</emphasis></entry>

            <entry>The maximum length of a pattern; longer patterns are
            truncated in the output; this value is also used to set the
            maximum length of the data to consider when finding cardinality
            and mode values; must be 33 or larger; OPTIONAL, defaults to
            100</entry>
          </row>

          <row>
            <entry><emphasis>features</emphasis></entry>

            <entry>A comma-delimited string listing the profiling elements to
            be included in the output; OPTIONAL, defaults to a comma-delimited
            string containing all of the available keywords mentioned in the
            table "keywords". To omit the output associated with a single
            keyword, set this argument to a comma-delimited string containing
            all other keywords; note that the is_numeric output will appear
            only if min_max, mean, std_dev, quartiles, or correlations
            features are active; also note that enabling the
            cardinality_breakdown feature will also enable the cardinality
            feature, even if it is not explicitly enabled</entry>
          </row>

          <row>
            <entry><emphasis>sampleSize</emphasis></entry>

            <entry>A positive integer representing a percentage of inFile to
            examine, which is useful when analyzing a very large dataset and
            only an estimated data profile is sufficient; valid range for this
            argument is 1-100; values outside of this range will be clamped;
            OPTIONAL, defaults to 100 (which indicates that the entire dataset
            will be analyzed)</entry>
          </row>

          <row>
            <entry><emphasis>lcbLimit</emphasis></entry>

            <entry>A positive integer (&lt;= 1000) indicating the maximum
            cardinality allowed for an attribute in order to emit a breakdown
            of the attribute's values; this parameter will be ignored if
            cardinality_breakdown is not included in the features argument;
            OPTIONAL, defaults to 64</entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>

    <para><emphasis role="bold">Attributes Returned</emphasis></para>

    <informaltable colsep="1" frame="all" rowsep="1">
      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis>attribute</emphasis></entry>

            <entry>The name of the attribute</entry>
          </row>

          <row>
            <entry><emphasis>given_attribute_type</emphasis></entry>

            <entry>The ECL type of the attribute as it was defined in the
            input dataset</entry>
          </row>

          <row>
            <entry><emphasis>best_attribute_type</emphasis></entry>

            <entry>An ECL data type that both allows all values in the input
            dataset and consumes the least amount of memory</entry>
          </row>

          <row>
            <entry><emphasis>rec_count</emphasis></entry>

            <entry>The number of records analyzed in the dataset; this may be
            fewer than the total number of records, if the optional sampleSize
            argument was provided with a value less than 100</entry>
          </row>

          <row>
            <entry><emphasis>fill_count</emphasis></entry>

            <entry>he number of rec_count records containing non-nil values; a
            'nil value' is an empty string, a numeric zero, or an empty SET;
            note that BOOLEAN attributes are always counted as filled,
            regardless of their value; also, fixed-length DATA attributes
            (e.g. DATA10) are also counted as filled, given their typical
            function of holding data blobs</entry>
          </row>

          <row>
            <entry><emphasis>cardinality</emphasis></entry>

            <entry>The number of unique, non-nil values within the
            attribute</entry>
          </row>

          <row>
            <entry><emphasis>cardinality_breakdown</emphasis></entry>

            <entry>For those attributes with a low number of unique, non-nil
            values, show each value and the number of records containing that
            value; the lcbLimit parameter governs what "low number"
            means</entry>
          </row>

          <row>
            <entry><emphasis>modes</emphasis></entry>

            <entry>The most common values in the attribute, after coercing all
            values to STRING, along with the number of records in which the
            values were found; if no value is repeated more than once then no
            mode will be shown; up to five (5) modes will be shown; note that
            string values longer than the maxPatternLen argument will be
            truncated</entry>
          </row>

          <row>
            <entry><emphasis>min_length</emphasis></entry>

            <entry>For SET datatypes, the fewest number of elements found in
            the set; for other data types, the shortest length of a value when
            expressed as a string; null values are ignored</entry>
          </row>

          <row>
            <entry><emphasis>max_length</emphasis></entry>

            <entry>For SET datatypes, the largest number of elements found in
            the set; for other data types, the longest length of a value when
            expressed as a string; null values are ignored</entry>
          </row>

          <row>
            <entry><emphasis>ave_length</emphasis></entry>

            <entry>For SET datatypes, the average number of elements found in
            the set; for other data types, the average length of a value when
            expressed as a string; null values are ignored</entry>
          </row>

          <row>
            <entry><emphasis>popular_patterns</emphasis></entry>

            <entry>The most common patterns of values; see below</entry>
          </row>

          <row>
            <entry><emphasis>rare_patterns</emphasis></entry>

            <entry>The least common patterns of values; see below</entry>
          </row>

          <row>
            <entry><emphasis>is_numeric</emphasis></entry>

            <entry>Boolean indicating if the original attribute was a numeric
            scalar or if the best_attribute_type value was a numeric scaler;
            if TRUE then the numeric_xxxx output fields will be populated with
            actual values; if this value is FALSE then all numeric_xxxx output
            values should be ignored</entry>
          </row>

          <row>
            <entry><emphasis>numeric_min</emphasis></entry>

            <entry>The smallest non-nil value found within the attribute as a
            DECIMAL; this value is valid only if is_numeric is TRUE; if
            is_numeric is FALSE then zero will show here</entry>
          </row>

          <row>
            <entry><emphasis>numeric_max </emphasis></entry>

            <entry>The largest non-nil value found within the attribute as a
            DECIMAL; this value is valid only if is_numeric is TRUE; if
            is_numeric is FALSE then zero will show here</entry>
          </row>

          <row>
            <entry><emphasis>numeric_mean</emphasis></entry>

            <entry>The mean (average) non-nil value found within the attribute
            as a DECIMAL; this value is valid only if is_numeric is TRUE; if
            is_numeric is FALSE then zero will show here</entry>
          </row>

          <row>
            <entry><emphasis>numeric_std_dev</emphasis></entry>

            <entry>The standard deviation of the non-nil values in the
            attribute as a DECIMAL; this value is valid only if is_numeric is
            TRUE; if is_numeric is FALSE then zero will show here</entry>
          </row>

          <row>
            <entry><emphasis>numeric_lower_quartile</emphasis></entry>

            <entry>The value separating the first (bottom) and second quarters
            of non-nil values within the attribute as a DECIMAL; this value is
            valid only if is_numeric is TRUE; if is_numeric is FALSE then zero
            will show here</entry>
          </row>

          <row>
            <entry><emphasis>numeric_median</emphasis></entry>

            <entry>The median non-nil value within the attribute as a DECIMAL;
            this value is valid only if is_numeric is TRUE; if is_numeric is
            FALSE then zero will show here</entry>
          </row>

          <row>
            <entry><emphasis>numeric_upper_quartile</emphasis></entry>

            <entry>The value separating the third and fourth (top) quarters of
            non-nil values within the attribute as a DECIMAL; this value is
            valid only if is_numeric is TRUE; if is_numeric is FALSE then zero
            will show here</entry>
          </row>

          <row>
            <entry><emphasis>correlations</emphasis></entry>

            <entry>A child dataset containing correlation values comparing the
            current numeric attribute with all other numeric attributes,
            listed in descending correlation value order; the attribute must
            be a numeric ECL datatype; non-numeric attributes will return an
            empty child dataset; note that this can be a time-consuming
            operation, depending on the number of numeric attributes in your
            dataset and the number of rows (if you have N numeric attributes,
            then N (N - 1) / 2 calculations are performed, each scanning all
            data rows)</entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>

    <para><informaltable colsep="1" frame="all" rowsep="1">
        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis>KEYWORD</emphasis></entry>

              <entry>AFFECTED OUTPUT</entry>
            </row>

            <row>
              <entry><emphasis>fill_rate</emphasis></entry>

              <entry>fill_rate, fill_count</entry>
            </row>

            <row>
              <entry><emphasis>cardinality</emphasis></entry>

              <entry>cardinality</entry>
            </row>

            <row>
              <entry><emphasis>cardinality_breakdown</emphasis></entry>

              <entry>cardinality_breakdown</entry>
            </row>

            <row>
              <entry><emphasis>best_ecl_types</emphasis></entry>

              <entry>best_attribute_type</entry>
            </row>

            <row>
              <entry><emphasis>modes</emphasis></entry>

              <entry>modes</entry>
            </row>

            <row>
              <entry><emphasis>lengths</emphasis></entry>

              <entry>min_length, max_length, ave_length</entry>
            </row>

            <row>
              <entry><emphasis>patterns</emphasis></entry>

              <entry>popular_patterns, rare_patterns</entry>
            </row>

            <row>
              <entry><emphasis>min_max</emphasis></entry>

              <entry>numeric_min, numeric_max</entry>
            </row>

            <row>
              <entry><emphasis>mean</emphasis></entry>

              <entry>numeric_mean</entry>
            </row>

            <row>
              <entry><emphasis>std_dev</emphasis></entry>

              <entry>numeric_std_dev</entry>
            </row>

            <row>
              <entry><emphasis>quartiles</emphasis></entry>

              <entry>numeric_lower_quartile, numeric_median,
              numeric_upper_quartile</entry>
            </row>

            <row>
              <entry><emphasis>correlations</emphasis></entry>

              <entry>correlations</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable></para>

    <para><emphasis role="bold">Example:</emphasis></para>

    <programlisting>IMPORT STD.DataPatterns AS DP;
filePath := '~pg::exampledata::people';
ds := DATASET(filePath, RECORDOF(filePath, LOOKUP), FLAT);
profileResults := DP.Profile(ds);
OUTPUT(profileResults, ALL, NAMED('profileResults'));</programlisting>
  </sect1>

  <sect1 id="DPBestRecord">
    <title>Best Record Structure</title>

    <para><emphasis role="bold">STD.DataPatterns.BestRecordStructure<indexterm>
        <primary>STD.DataPatterns.BestRecordStructure</primary>
      </indexterm><indexterm>
        <primary>DataPatterns.BestRecordStructure</primary>
      </indexterm><indexterm>
        <primary>BestRecordStructure</primary>
      </indexterm></emphasis></para>

    <para>Function macro that leverages DataPatterns to return a string
    defining the best ECL record structure for the input data.</para>

    <informaltable colsep="1" frame="all" rowsep="1">
      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis>inFile</emphasis></entry>

            <entry>The dataset to process; REQUIRED</entry>
          </row>

          <row>
            <entry><emphasis>sampling</emphasis></entry>

            <entry>A positive integer representing a percentage of inFile to
            examine, which is useful when analyzing a very large dataset and
            only an estimatation is sufficient; valid range for this argument
            is 1-100; values outside of this range will be clamped; OPTIONAL,
            defaults to 100 (which indicates that the entire dataset will be
            analyzed)</entry>
          </row>

          <row>
            <entry><emphasis>emitTransform</emphasis></entry>

            <entry>Boolean governing whether the function emits a TRANSFORM
            function that could be used to rewrite the dataset into the 'best'
            record definition; OPTIONAL, defaults to FALSE.</entry>
          </row>

          <row>
            <entry><emphasis>textOutput</emphasis></entry>

            <entry>Boolean governing the type of result that is delivered by
            this function; if FALSE then a recordset of STRINGs will be
            returned; if TRUE then a dataset with a single STRING field, with
            the contents formatted for HTML, will be returned (this is the
            ideal output if the intention is to copy the output from ECL
            Watch); OPTIONAL, defaults to FALSE</entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>

    <para>The best record structure returns a recordset defining the best ECL
    record structure for the data. If textOutput is FALSE (the default) then
    each record will contain one field declaration, and the list of
    declarations will be wrapped with RECORD and END strings; if the
    emitTransform argument was TRUE, there will also be a set of records that
    that comprise a stand-alone TRANSFORM function. If textOutput is TRUE then
    only one record will be returned, containing an HTML-formatted string
    containing the new field declarations (and optionally the TRANSFORM); this
    is the ideal format if the intention is to copy the result from ECL
    Watch.</para>

    <para><emphasis role="bold">Example:</emphasis></para>

    <programlisting>IMPORT STD.DataPatterns AS DP;
filePath := '~pg::exampledata::people';
ds := DATASET(filePath, RECORDOF(filePath, LOOKUP), FLAT);
bestRecordStructureResults := DP.BestRecordStructure(ds);
OUTPUT(bestRecordStructureResults, ALL, NAMED('bestRecordStructure'));</programlisting>
  </sect1>

  <sect1 id="DPBenford">
    <title>Benford</title>

    <para><emphasis role="bold">STD.DataPatterns.Benford<indexterm>
        <primary>STD.DataPatterns.Benford</primary>
      </indexterm><indexterm>
        <primary>DataPatterns.Benford</primary>
      </indexterm><indexterm>
        <primary>Benford</primary>
      </indexterm></emphasis></para>

    <para>Benford's law, also called the Newcomb–Benford law, or the law of
    anomalous numbers, is an observation about the frequency distribution of
    leading digits in many real-life sets of numerical data.</para>

    <para>Benford's law doesn't apply to every set of numbers, but it usually
    applies to large sets of naturally occurring numbers with some connection
    like:</para>

    <para>* Companies' stock market values</para>

    <para>* Data found in texts — like the Reader's Digest, or a copy of
    Newsweek</para>

    <para>* Demographic data, including state and city populations</para>

    <para>* Income tax data</para>

    <para>* Mathematical tables, like logarithms</para>

    <para>* River drainage rates</para>

    <para>* Scientific data</para>

    <para>The law usually doesn't apply to data sets that have a stated
    minimum and maximum, like interest rates or hourly wages. If numbers are
    assigned, rather than naturally occurring, they will also not follow the
    law. Examples of assigned numbers include: zip codes, telephone numbers
    and Social Security numbers.</para>

    <para>For more information:
    https://en.wikipedia.org/wiki/Benford%27s_law</para>

    <para>This function computes the distribution of digits within one or more
    attributes in a dataset and displays the result, one attribute per row,
    with an "expected" row showing the expected distributions. Included in
    each data row is a chi-squared computation for that row indicating how
    well the computed result matches the expected result (if the chi-squared
    value exceeds the one shown in the --EXPECTED-- row then the data row DOES
    NOT follow Benford's Law).</para>

    <para>Note that when computing the distribution of the most significant
    digit, the digit zero is ignored. So for instance, the values 0100, 100,
    1.0, 0.10, and 0.00001 all have a most-significant digit of '1'. The digit
    zero is considered for all other positions.</para>

    <informaltable colsep="1" frame="all" rowsep="1">
      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis>inFile</emphasis></entry>

            <entry>The dataset to process; REQUIRED</entry>
          </row>

          <row>
            <entry><emphasis>fieldListStr</emphasis></entry>

            <entry>A string containing a comma-delimited list of attribute
            names to process; note that attributes listed here must be
            top-level attributes (not child records or child datasets); use an
            empty string to process all top-level attributes in inFile;
            OPTIONAL, defaults to an empty string</entry>
          </row>

          <row>
            <entry><emphasis>digit</emphasis></entry>

            <entry>The 1-based digit within the number to examine; the first
            significant digit is '1' and it only increases; OPTIONAL, defaults
            to 1, meaning the most-significant non-zero digit</entry>
          </row>

          <row>
            <entry><emphasis>sampleSize</emphasis></entry>

            <entry>A positive integer representing a percentage of inFile to
            examine, which is useful when analyzing a very large dataset and
            only an estimated data analysis is sufficient; valid range for
            this argument is 1-100; values outside of this range will be
            clamped; OPTIONAL, defaults to 100 (which indicates that all rows
            in the dataset will be used)</entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>

    <para>Benford returns a new dataset with the following record
    structure:</para>

    <para>RECORD</para>

    <para>STRING attribute; // Name of data attribute examined</para>

    <para>DECIMAL4_1 zero; // Percentage of rows with digit of '0'</para>

    <para>DECIMAL4_1 one; // Percentage of rows with digit of '1'</para>

    <para>DECIMAL4_1 two; // Percentage of rows with digit of '2'</para>

    <para>DECIMAL4_1 three; // Percentage of rows with digit of '3'</para>

    <para>DECIMAL4_1 four; // Percentage of rows with digit of '4'</para>

    <para>DECIMAL4_1 five; // Percentage of rows with digit of '5'</para>

    <para>DECIMAL4_1 six; // Percentage of rows with digit of '6'</para>

    <para>DECIMAL4_1 seven; // Percentage of rows with digit of '7'</para>

    <para>DECIMAL4_1 eight; // Percentage of rows with digit of '8'</para>

    <para>DECIMAL4_1 nine; // Percentage of rows with digit of '9'</para>

    <para>DECIMAL7_3 chi_squared; // Chi-squared "fitness test" result</para>

    <para>UNSIGNED8 num_values; // Number of rows with non-zero values for
    this attribute</para>

    <para>END;</para>

    <para>The named digit fields (e.g. "zero" and "one" and so on) represent
    the * digit found in the 'digit' position of the associated attribute. The
    values * that appear there are percentages. num_values shows the number of
    * non-zero values processed, and chi_squared shows the result of applying
    * that test using the observed vs expected distribution values.</para>

    <para>The first row of the results will show the expected values for the
    named * digits, with "-- EXPECTED DIGIT n --" showing as the attribute
    name.'n' will * be replaced with the value of 'digit' which indicates
    which digit position * was examined.</para>

    <para>Note that when viewing the results for the mosts significant digit
    (digit = 1), * the 'zero' field will show a -1 value, indicating that it
    was ignored.</para>

    <para><emphasis role="bold">Example:</emphasis></para>

    <programlisting>Benford(inFile, fieldListStr = '\'\'', digit = 1, sampleSize = 100) := FUNCTIONMACRO

    #UNIQUENAME(minDigit);
    LOCAL %minDigit% := MAX((INTEGER)digit, 1);

    #UNIQUENAME(clampedDigit);  
    LOCAL %clampedDigit% := MIN(%minDigit%, 4);

    // Chi-squared critical value table:
    // https://www.itl.nist.gov/div898/handbook/eda/section3/eda3674.htm

    // Chi-squared critical values for 8 degrees of freedom at various probabilities
    // Probability:     0.90    0.95    0.975   0.99    0.999
    // Critical value:  13.362  15.507  17.535  20.090  26.125
    #UNIQUENAME(CHI_SQUARED_CRITICAL_VALUE_1);
    #SET(CHI_SQUARED_CRITICAL_VALUE_1, 20.090); // 99% probability

    // Chi-squared critical values for 9 degrees of freedom at various probabilities
    // Probability:     0.90    0.95    0.975   0.99    0.999
    // Critical value:  14.684  16.919  19.023  21.666  27.877
    #UNIQUENAME(CHI_SQUARED_CRITICAL_VALUE_2);
    #SET(CHI_SQUARED_CRITICAL_VALUE_2, 21.666); // 99% probability

    #UNIQUENAME(CHI_SQUARED_CRITICAL_VALUE);
    LOCAL %CHI_SQUARED_CRITICAL_VALUE% := IF(%clampedDigit% = 1, %CHI_SQUARED_CRITICAL_VALUE_1%, %CHI_SQUARED_CRITICAL_VALUE_2%);

    #UNIQUENAME(expectedDistribution);
    LOCAL %expectedDistribution% := DATASET
        (
            [
                {1, -1, 30.1, 17.6, 12.5, 9.7, 7.9, 6.7, 5.8, 5.1, 4.6},
                {2, 12.0, 11.4, 10.9, 10.4, 10.0, 9.7, 9.3, 9.0, 8.8, 8.5},
                {3, 10.2, 10.1, 10.1, 10.1, 10.0, 10.0, 9.9, 9.9, 9.9, 9.8},
                {4, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0}
            ],
            {
                UNSIGNED1   pos,
                DECIMAL4_1  zero,
                DECIMAL4_1  one,
                DECIMAL4_1  two,
                DECIMAL4_1  three,
                DECIMAL4_1  four,
                DECIMAL4_1  five,
                DECIMAL4_1  six,
                DECIMAL4_1  seven,
                DECIMAL4_1  eight,
                DECIMAL4_1  nine
            }
        );

    // Remove all spaces from field list so we can parse it more easily
    #UNIQUENAME(trimmedFieldList);
    LOCAL %trimmedFieldList% := TRIM((STRING)fieldListStr, ALL);

    // Ungroup the given dataset, in case it was grouped
    #UNIQUENAME(ungroupedInFile);
    LOCAL %ungroupedInFile% := UNGROUP(inFile);

    // Clamp the sample size to something reasonable
    #UNIQUENAME(clampedSampleSize);
    LOCAL %clampedSampleSize% := MAX(1, MIN(100, (INTEGER)sampleSize));

    // Create a sample dataset if needed
    #UNIQUENAME(sampledData);
    LOCAL %sampledData% := IF
        (
            %clampedSampleSize% &lt; 100,
            ENTH(%ungroupedInFile%, %clampedSampleSize%, 100, 1, LOCAL),
            %ungroupedInFile%
        );
IMPORT Std;</programlisting>

    <para/>

    <para><emphasis role="bold"/></para>
  </sect1>
</chapter>
