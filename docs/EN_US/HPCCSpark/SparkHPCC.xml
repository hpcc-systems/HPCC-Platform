<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<book xml:base="../">
  <bookinfo>
    <title>HPCC / Spark Connector</title>

    <mediaobject>
      <imageobject>
        <imagedata fileref="images/redswooshWithLogo3.jpg" />
      </imageobject>
    </mediaobject>

    <author>
      <surname>Boca Raton Documentation Team</surname>
    </author>

    <legalnotice>
      <para>We welcome your comments and feedback about this document via
      email to <email>docfeedback@hpccsystems.com</email></para>

      <para>Please include <emphasis role="bold">Documentation
      Feedback</emphasis> in the subject line and reference the document name,
      page numbers, and current Version Number in the text of the
      message.</para>

      <para>LexisNexis and the Knowledge Burst logo are registered trademarks
      of Reed Elsevier Properties Inc., used under license.</para>

      <para>HPCC Systems<superscript>®</superscript> is a registered trademark
      of LexisNexis Risk Data Management Inc.</para>

      <para>Other products, logos, and services may be trademarks or
      registered trademarks of their respective companies.</para>

      <para>All names and example data used in this manual are fictitious. Any
      similarity to actual persons, living or dead, is purely
      coincidental.</para>

      <para></para>
    </legalnotice>

    <xi:include href="common/Version.xml" xpointer="FooterInfo"
                xmlns:xi="http://www.w3.org/2001/XInclude" />

    <xi:include href="common/Version.xml" xpointer="DateVer"
                xmlns:xi="http://www.w3.org/2001/XInclude" />

    <corpname>HPCC Systems<superscript>®</superscript></corpname>

    <xi:include href="common/Version.xml" xpointer="Copyright"
                xmlns:xi="http://www.w3.org/2001/XInclude" />

    <mediaobject role="logo">
      <imageobject>
        <imagedata fileref="images/LN_Rightjustified.jpg" />
      </imageobject>
    </mediaobject>
  </bookinfo>

  <chapter>
    <title>The Spark HPCC Systems Connector</title>

    <sect1 id="overview" role="nobrk">
      <title>Overview</title>

      <para>The Spark-HPCCSystems Distributed Connector is a Java library that
      facilitates access from a Spark cluster to data stored on an HPCC
      Systems cluster. The connector library employs the standard HPCC Systems
      remote file read facility to read data from either sequential or indexed
      HPCC datasets.</para>

      <para>The data on an HPCC cluster is partitioned horizontally, with data
      on each cluster node. Once configured, the HPCC data is available for
      reading in parallel by the Spark cluster.</para>

      <para>In the GitHub repository (<ulink
      url="https://github.com/hpcc-systems/Spark-HPCC">https://github.com/hpcc-systems/Spark-HPCC</ulink>)
      you can find the source code and examples. There are several artifacts
      in the DataAccess/src/main/java folder of primary interest. The
      <emphasis>org.hpccsystems.spark.HpccFile</emphasis> class is the façade
      of a file on an HPCC Cluster. The
      <emphasis>org.hpccsystems.spark.HpccRDD</emphasis> is a resilient
      distributed dataset derived from the data on the HPCC Cluster and is
      created by the
      <emphasis>org.hpccsystems.spark.HpccFile.getRDD</emphasis>(…) method.
      The <emphasis>HpccFile</emphasis> class supports loading data to
      construct a <emphasis>Dataset&lt;Row&gt;</emphasis> object for the Spark
      interface. This will first load the data into an RDD&lt;Row&gt; and then
      convert this RDD to a Dataset&lt;Row&gt; through internal Spark
      mechanisms.</para>

      <para>There are several additional artifacts of some interest. The
      <emphasis>org.hpccsystems.spark.ColumnPruner</emphasis> class is
      provided to enable retrieving only the columns of interest from the HPCC
      Cluster. The <emphasis>targetCluster</emphasis> artifact allows you to
      specify the HPCC cluster on which the target file exists. The
      <emphasis>org.hpccsystems.spark.thor.FileFilter</emphasis> class is
      provided to facilitate filtering records of interest from the HPCC
      Cluster.</para>

      <para>The git repository includes two examples under the
      Examples/src/main/scala folder. The examples
      (<emphasis>org.hpccsystems.spark_examples.Dataframe_Iris_LR</emphasis>
      and <emphasis>org.hpccsystems.spark_examples.Iris_LR</emphasis>) are
      Scala Objects with a main() function. Both examples use the classic Iris
      dataset. The dataset can be obtained from a variety of sources,
      including the HPCC-Systems/ecl-ml repository. IrisDs.ecl (can be found
      under the ML/Tests/Explanatory folder: <ulink
      url="https://github.com/hpcc-systems/Spark-HPCC/blob/master/Examples/src/main/ecl/IrisDS.ecl">https://github.com/hpcc-systems/Spark-HPCC/blob/master/Examples/src/main/ecl/IrisDS.ecl</ulink>)
      can be executed to generate the Iris dataset in HPCC. A walk-through of
      the examples is provided in the Examples section.</para>

      <para>The Spark-HPCCSystems Distributed Connector also supports PySpark.
      It uses the same classes/API as Java does.</para>

      <para><informaltable colsep="1" frame="all" rowsep="1">
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec colwidth="49.50pt" />

            <colspec />

            <tbody>
              <row>
                <entry><graphic fileref="images/tip.jpg" /></entry>

                <entry><para>As is common in Java client communication over
                TLS, Spark-HPCC connectors targeting an HPCC cluster over TLS
                will need to import the appropriate certificates to local Java
                keystore.</para><para> </para><para>*One way to accomplish
                this is to use the keytool packaged with Java installations.
                Refer to the keytool documentation for usage. </para></entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <sect2>
        <title>Spark Integration</title>

        <para>The HPCC integrated Spark plugin is no longer supported as of
        version 9.0.0 in favor of stand-alone user-managed Spark clusters
        linked to the HPCC platform using the Spark-HPCC connector.</para>
      </sect2>

      <sect2 role="brk">
        <title>Special considerations</title>

        <sect3>
          <title>Unsigned Value Overflow</title>

          <para>Java does not support an unsigned integer type so reading
          UNSIGNED8 values from HPCC data can cause an integer overflow in
          Java. UNSIGNED8 values are often used as unique identifiers in
          datasets, in which case overflowing would be acceptable as the
          overflowed value will still be unique.</para>

          <para>The Spark-HPCC connector allows unsigned values to overflow in
          Java and will not report a exception. The caller is responsible for
          interpreting the value based on the recdef <emphasis
          role="bold">isunsigned</emphasis> flag.</para>
        </sect3>
      </sect2>
    </sect1>

    <sect1 id="primary-classes">
      <title>Primary Classes</title>

      <para>The <emphasis>HpccFile</emphasis> class and the
      <emphasis>HpccRDD</emphasis> classes are discussed in more detail below.
      These are the primary classes used to access data from an HPCC Cluster.
      The <emphasis>HpccFile</emphasis> class supports loading data to
      construct a <emphasis>Dataset&lt;Row&gt;</emphasis> object for the Spark
      interface. This will first load the data into an RDD&lt;Row&gt; and then
      convert this RDD to a Dataset&lt;Row&gt; through internal Spark
      mechanisms.</para>

      <para>The <emphasis>org.hpccsystems.spark.HpccFile</emphasis> class has
      several constructors. All of the constructors take information about the
      Cluster and the name of the dataset of interest. The JAPI WS-Client
      classes are used to access file detail information. A definition used to
      select the columns to be returned and a definition to select the rows to
      be returned could also be supplied. These are discussed in the
      <emphasis>Additional Classes of Interest</emphasis> section below. The
      class has two methods of primary interest: the
      <emphasis>getRDD(…)</emphasis> method and the
      <emphasis>getDataframe(…)</emphasis> method, which are illustrated in
      the <emphasis>Example</emphasis> section.</para>

      <para>The <emphasis>HpccFile</emphasis> class
      <emphasis>getRecordDefinition()</emphasis> method can be used to
      retrieve a definition of the file. The
      <emphasis>getFileParts()</emphasis> method can be used to see how the
      file is partitioned on the HPCC Cluster. These methods return the same
      information as can be found on the ECL Watch dataset details page DEF
      tab and the PARTS tab respectively.</para>

      <para>The <emphasis>org.hpccsystems.spark.HpccRDD</emphasis> class
      extends the <emphasis>RDD&lt;Record&gt;</emphasis> templated class. The
      class employs the <emphasis>org.hpccsystems.spark.HpccPart</emphasis>
      class for the Spark partitions. The
      <emphasis>org.hpccsystems.spark.Record</emphasis> class is used as the
      container for the fields from the HPCC Cluster. The
      <emphasis>Record</emphasis> class can create a <emphasis>Row</emphasis>
      instance with a schema.</para>

      <para>The <emphasis>HpccRDD</emphasis> HpccPart partition objects each
      read blocks of data from the HPCC Cluster independently from each other.
      The initial read fetches the first block of data, requests the second
      block of data, and returns the first record. When the block is
      exhausted, the next block should be available on the socket and new read
      request is issued.</para>

      <para>The <emphasis>HpccFileWriter</emphasis> is another primary class
      used for writing data to an HPCC Cluster. It has a single constructor
      with the following signature:</para>

      <programlisting>public HpccFileWriter(String connectionString, String user, String pass) throws Exception { </programlisting>

      <para>The first parameter <emphasis>connectionString</emphasis> contains
      the same information as <emphasis>HpccFile</emphasis>. It should be in
      the following format:
      {http|https}://{ECLWATCHHOST}:{ECLWATCHPORT}</para>

      <para>The constructor will attempt to connect to HPCC. This connection
      will then be used for any subsequent calls to
      <emphasis>saveToHPCC</emphasis>.</para>

      <programlisting>public long saveToHPCC(SparkContext sc, RDD&lt;Row&gt; scalaRDD, String clusterName, 
                        String fileName) throws Exception {</programlisting>

      <para>The <emphasis>saveToHPCC</emphasis> method only supports
      RDD&lt;row&gt; types. You may need to modify your data representation to
      use this functionality. However, this data representation is what is
      used by Spark SQL and by HPCC. This is only supported by writing in a
      co-located setup. Thus Spark and HPCC must be installed on the same
      nodes. Reading only supports reading data in from a remote HPCC
      cluster.</para>

      <para>The <emphasis>clusterName</emphasis> as used in the above case is
      the desired cluster to write data to, for example, the "mythor" Thor
      cluster. Currently there is only support for writing to Thor clusters.
      Writing to a Roxie cluster is not supported and will return an
      exception. The filename as used in the above example is in the HPCC
      format, for example: "~example::text".</para>

      <para>Internally the saveToHPCC method will Spawn multiple Spark jobs.
      Currently, this spawns two jobs. The first job maps the location of
      partitions in the Spark cluster so it can provide this information to
      HPCC. The second job does the actual writing of files. There are also
      some calls internally to ESP to handle things like starting the writing
      process by calling <emphasis>DFUCreateFile</emphasis> and publishing the
      file once it has been written by calling
      <emphasis>DFUPublishFile</emphasis>.</para>

      <sect2 role="brk">
        <title>Using the Spark Datasource API to Read and Write</title>

        <para>Example Python code:</para>

        <para><programlisting># Connect to HPCC and read a file
df = spark.read.load(format="hpcc",
                     host="127.0.0.1:8010",
                     password="",
                     username="",
                     limitPerFilePart=100,        
                                      # Limit the number of rows to read from each file part
                     projectList="field1, field2, field3.childField1",     
                                      # Comma separated list of columns to read
                     fileAccessTimeout=240,
                     path="example::file")
# Write the file back to HPCC
df.write.save(format="hpcc",
              mode="overwrite",        
                   # Left blank or not specified results in an error if the file exists
              host="127.0.0.1:8010",
              password="",
              username="",
              cluster="mythor",
              path="example::file")</programlisting></para>

        <para>Example Scala code:</para>

        <para><programlisting>// Read a file from HPCC
val dataframe = spark.read.format("hpcc")
                .option("host","127.0.0.1:8010")
                .option("password", "")
                .option("username", "")
                .option("limitPerFilePart",100)
                .option("fileAccessTimeout",240)
                .option("projectList","field1, field2, field3.childField")
                .load("example::file")
// Write the dataset back
   dataframe.write.mode("overwrite")
                   .format("hpcc")
                   .option("host","127.0.0.1:8010")
                   .option("password", "")
                   .option("username", "")
                   .option("cluster","mythor")
                   .save("example::file")</programlisting></para>

        <para>Example R code:</para>

        <para><programlisting>df &lt;- read.df(source = "hpcc",
              host = "127.0.0.1:8010",
              path = "example::file",
              password = "",
              username = "",
              limitPerFilePart = 100,
              fileAccessTimeout = 240,
              projectList = "field1, field2, field3.childField")
write.df(df, source = "hpcc",
             host = "127.0.0.1:8010",
             cluster = "mythor",
             path = "example::file",
             mode = "overwrite",
             password = "",
             username = "",
             fileAccessTimeout = 240)</programlisting></para>
      </sect2>
    </sect1>

    <sect1 id="additional-classes-of-interest">
      <title>Additional Classes of interest</title>

      <para>The main classes of interest for this section are column pruning
      and file filtering. In addition there is a helper class to remap IP
      information when required, and this is also discussed below.</para>

      <para>The column selection information is provided as a string to the
      <emphasis>org.hpccsystems.spark.ColumnPruner</emphasis> object. The
      string is a list of comma separated field names. A field of interest
      could contain a row or child dataset, and the dotted name notation is
      used to support the selection of individual child fields. The
      <emphasis>ColumnPruner</emphasis> parses the string into a root
      <emphasis>TargetColumn</emphasis> class instance which holds the top
      level target columns. A <emphasis>TargetColumn</emphasis> can be a
      simple field or can be a child dataset and so be a root object for the
      child record layout.</para>

      <para>The row filter is implemented in the
      <emphasis>org.hpccsystems.spark.thor.FileFilter</emphasis> class. A
      <emphasis>FileFilter</emphasis> instance is constricted from an array of
      <emphasis>org.hpccsystems.spark.thor.FieldFilter</emphasis> objects.
      Each <emphasis>FieldFilter</emphasis> instance is composed of a field
      name (in doted notation for compound names) and an array of
      <emphasis>org.hpccsystems.spark.thor.FieldFilterRange</emphasis>
      objects. Each <emphasis>FieldFilterRange</emphasis> instance can be an
      open or closed interval or a single value. The record is selected when
      at least one <emphasis>FieldFilterRange</emphasis> matches for each of
      the <emphasis>FieldFilter</emphasis> instances in the array.</para>

      <para>The <emphasis>FieldFilterRange</emphasis> values may be either
      strings or numbers. There are methods provided to construct the
      following range tests: equals, not equals, less than, less than or
      equals, greater than, and a greater than or equals. In addition, a set
      inclusion test is supported for strings. If the file is an index, the
      filter fields that are key fields are used for an index lookup. Any
      filter field unmentioned is treated as wild.</para>

      <para>The usual deployment architecture for HPCC Clusters consists of a
      collection of nodes on a network. The file management information
      includes the IP addresses of the nodes that hold the partitions of the
      file. The Spark-HPCC connector classes use these IP addresses to
      establish socket connections for the remote read. An HPCC Cluster may be
      deployed as a virtual cluster with private IP addresses. This works for
      the cluster components because they are all on the same private LAN.
      However, the Spark cluster nodes might not be on that same LAN. In this
      case, the <emphasis>org.hpccsystems.spark.RemapInfo</emphasis> class is
      used to define the information needed to change the addressing. There
      are two options that can be used. The first option is that each Thor
      worker node can be assigned an IP that is visible to the Spark cluster.
      These addresses must be a contiguous range. The second option is to
      assign an IP and a contiguous range of port numbers. The
      <emphasis>RemapInfo</emphasis> object is supplied as a parameter.</para>
    </sect1>

    <sect1 id="examples">
      <title>Examples</title>

      <para>We have provided some examples of utilizing a Spark environment.
      The examples provided are dependent on the Spark shell.</para>

      <para>You can refer to the examples in the Github repository: </para>

      <para><ulink
      url="https://github.com/hpcc-systems/Spark-HPCC/tree/master/Examples">https://github.com/hpcc-systems/Spark-HPCC/tree/master/Examples</ulink>
      </para>
    </sect1>
  </chapter>
</book>
