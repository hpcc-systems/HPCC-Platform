<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<book xml:base="../">
  <bookinfo>
    <title>HPCC / Spark Integration</title>

    <mediaobject>
      <imageobject>
        <imagedata fileref="images/redswooshWithLogo3.jpg" />
      </imageobject>
    </mediaobject>

    <author>
      <surname>Boca Raton Documentation Team</surname>
    </author>

    <legalnotice>
      <para>We welcome your comments and feedback about this document via
      email to <email>docfeedback@hpccsystems.com</email></para>

      <para>Please include <emphasis role="bold">Documentation
      Feedback</emphasis> in the subject line and reference the document name,
      page numbers, and current Version Number in the text of the
      message.</para>

      <para>LexisNexis and the Knowledge Burst logo are registered trademarks
      of Reed Elsevier Properties Inc., used under license.</para>

      <para>HPCC Systems<superscript>®</superscript> is a registered trademark
      of LexisNexis Risk Data Management Inc.</para>

      <para>Other products, logos, and services may be trademarks or
      registered trademarks of their respective companies.</para>

      <para>All names and example data used in this manual are fictitious. Any
      similarity to actual persons, living or dead, is purely
      coincidental.</para>

      <para />
    </legalnotice>

    <xi:include href="common/Version.xml" xpointer="FooterInfo"
                xmlns:xi="http://www.w3.org/2001/XInclude" />

    <xi:include href="common/Version.xml" xpointer="DateVer"
                xmlns:xi="http://www.w3.org/2001/XInclude" />

    <corpname>HPCC Systems<superscript>®</superscript></corpname>

    <xi:include href="common/Version.xml" xpointer="Copyright"
                xmlns:xi="http://www.w3.org/2001/XInclude" />

    <mediaobject role="logo">
      <imageobject>
        <imagedata fileref="images/LN_Rightjustified.jpg" />
      </imageobject>
    </mediaobject>
  </bookinfo>

  <chapter>
    <title>HPCC / Spark Installation and Configuration</title>

    <para>The HPCC Systems Spark plug-in, hpccsystems-plugin-spark, integrates
    Spark into your HPCC System platform. Once installed and configured, the
    Sparkthor component manages the Integrated Spark cluster. It dynamically
    configures, starts, and stops your Integrated Spark cluster when you start
    or stop your HPCC Systems platform.</para>

    <sect1 role="nobrk">
      <title>Spark Installation</title>

      <para>To add Spark integration to your HPCC Systems cluster, you must
      have an HPCC Cluster running version 7.0.0 or later. Java 8 is also
      required. You will need to configure the Sparkthor component. The
      Sparkthor component needs to be associated with a valid existing Thor
      cluster. The Spark worker nodes will be created alongside each Thor
      worker. The Integrated Spark Manager node will be designated during
      configuration, along with any other Spark node resources. Then the
      Sparkthor component will spawn an Integrated Spark cluster at start up.
      You will also have a SPARK-HPCC jar connector available.</para>

      <para>To get the Integrated Spark component, packages and plug-ins are
      available from the HPCC Systems<superscript>®</superscript> web portal:
      <ulink
      url="https://hpccsystems.com/download/free-community-edition">https://hpccsystems.com/download/</ulink></para>

      <para>Download the hpccsystems-plugin-spark package from the HPCC
      Systems Portal.</para>

      <sect2 id="installing-SparkPlugin">
        <title>Installing the Spark Plug-in</title>

        <para>The installation process and package that you download vary
        depending on the operating system you plan to use. The installation
        packages may fail to install if their dependencies are missing from
        the target system. To install the package, follow the appropriate
        installation instructions for your operating system:</para>

        <sect3 id="Spark_CentOS">
          <title>CentOS/Red Hat</title>

          <para>For RPM based systems, you can install using yum.</para>

          <para><programlisting>sudo yum install &lt;hpccsystems-plugin-spark&gt;  </programlisting></para>
        </sect3>

        <sect3>
          <title>Ubuntu/Debian</title>

          <para>To install a Ubuntu/Debian package, use:</para>

          <programlisting>sudo dpkg -i &lt;hpccsystems-plugin-spark&gt;  </programlisting>

          <para>After installing the package, you should run the following to
          update any dependencies.</para>

          <para><programlisting>sudo apt-get install -f </programlisting></para>

          <itemizedlist>
            <listitem>
              <para>You need to copy and install the plug-in onto all nodes.
              This can be done using the install-cluster.sh script which is
              provided with HPCC. Use the following command:</para>

              <programlisting>sudo /opt/HPCCSystems/sbin/install-cluster.sh &lt;hpccsystems-plugin-spark&gt;</programlisting>

              <para>More details including other options that may be used with
              this command are included in the appendix of Installing and
              Running the HPCC Platform, also available on the HPCC
              Systems<superscript>®</superscript> web portal.</para>
            </listitem>
          </itemizedlist>
        </sect3>
      </sect2>
    </sect1>

    <sect1 id="Spark_Configuration">
      <title>Spark Configuration</title>

      <para>To configure your existing HPCC System to integrate Spark, install
      the hpccsystems-plugin-spark package and modify your existing
      environment (file) to add the Sparkthor component.</para>

      <orderedlist>
        <listitem>
          <para>If it is running, stop the HPCC system, using this
          command:</para>

          <xi:include href="Installing_and_RunningTheHPCCPlatform/Inst-Mods/SysDStop.xml"
                      xpointer="element(/1)"
                      xmlns:xi="http://www.w3.org/2001/XInclude" />

          <para><informaltable colsep="1" frame="all" rowsep="1">
              <?dbfo keep-together="always"?>

              <tgroup cols="2">
                <colspec colwidth="49.50pt" />

                <colspec />

                <tbody>
                  <row>
                    <entry><inlinegraphic
                    fileref="images/OSSgr3.png" /></entry>

                    <entry>You can use this command to confirm HPCC processes
                    are stopped:<para><programlisting>sudo systemctl status hpccsystems-platform.target</programlisting></para></entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable></para>
        </listitem>

        <listitem>
          <para>Start the Configuration Manager service.<programlisting>sudo /opt/HPCCSystems/sbin/configmgr
</programlisting></para>

          <para><graphic fileref="images/gs_img_configmgrStart.jpg" /></para>
        </listitem>

        <listitem>
          <para>Leave this window open. You can minimize it, if
          desired.</para>
        </listitem>

        <listitem>
          <para>Using a Web browser, go to the Configuration Manager's
          interface:</para>

          <programlisting>http://&lt;<emphasis>node ip </emphasis>&gt;:8015</programlisting>
        </listitem>

        <listitem>
          <para>Check the box by the Advanced View and select the environment
          file to edit.</para>
        </listitem>

        <listitem>
          <para>Enable write access (checkbox top-right of the page)</para>
        </listitem>

        <listitem>
          <para>Right-click on the Navigator panel on the left side</para>

          <para>Choose <emphasis role="bold">New Components</emphasis> then
          choose <emphasis role="bold">Sparkthor</emphasis></para>
        </listitem>

        <listitem>
          <?dbfo keep-together="always"?>

          <para>Configure the attributes of your Spark Instance:</para>

          <para><informaltable colsep="1" id="Th.t1" rowsep="1">
              <tgroup align="left" cols="4">
                <colspec colwidth="155pt" />

                <colspec colwidth="2*" />

                <colspec colwidth="1*" />

                <colspec colwidth="0.5*" />

                <thead>
                  <row>
                    <entry>attribute</entry>

                    <entry>description</entry>

                    <entry>default</entry>

                    <entry>required</entry>
                  </row>
                </thead>

                <tbody>
                  <row>
                    <entry>name</entry>

                    <entry>Name for this process</entry>

                    <entry>mysparkthor</entry>

                    <entry>required</entry>
                  </row>

                  <row>
                    <entry>ThorClusterName</entry>

                    <entry>Thor cluster for workers to attach to*</entry>

                    <entry>mythor*</entry>

                    <entry>required</entry>
                  </row>

                  <row>
                    <entry>SPARK_EXECUTOR_CORES</entry>

                    <entry>Number of cores for executors</entry>

                    <entry>1</entry>

                    <entry>optional</entry>
                  </row>

                  <row>
                    <entry>SPARK_EXECUTOR_MEMORY</entry>

                    <entry>Memory per executor</entry>

                    <entry>1G</entry>

                    <entry>optional</entry>
                  </row>

                  <row>
                    <entry>SPARK_MASTER_WEBUI_PORT</entry>

                    <entry>Base port to use for manager web interface</entry>

                    <entry>8080</entry>

                    <entry>optional</entry>
                  </row>

                  <row>
                    <entry>SPARK_MASTER_PORT</entry>

                    <entry>Base port to use for manager</entry>

                    <entry>7077</entry>

                    <entry>optional</entry>
                  </row>

                  <row>
                    <entry>SPARK_WORKER_CORES</entry>

                    <entry>Number of cores for workers</entry>

                    <entry>1</entry>

                    <entry>optional</entry>
                  </row>

                  <row>
                    <entry>SPARK_WORKER_MEMORY</entry>

                    <entry>Memory per worker</entry>

                    <entry>1G</entry>

                    <entry>optional</entry>
                  </row>

                  <row>
                    <entry>SPARK_WORKER_PORT</entry>

                    <entry>Base port to use for workers</entry>

                    <entry>7071</entry>

                    <entry>optional</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable></para>

          <para>*ThorClusterName targets an existing Thor cluster. When
          configuring you must choose a valid existing Thor cluster for the
          Integrated Spark cluster to mirror.</para>

          <para><informaltable colsep="1" frame="all" rowsep="1">
              <?dbfo keep-together="always"?>

              <tgroup cols="2">
                <colspec colwidth="49.50pt" />

                <colspec />

                <tbody>
                  <row>
                    <entry><inlinegraphic
                    fileref="images/caution.png" /></entry>

                    <entry>NOTE: You should leave a least 2 cores open for
                    HPCC to use to provide Spark with Data. The number of
                    cores and memory you allocate to Spark would depend on the
                    workload. Do not try and allocate too many resources to
                    Spark where you could run into an issue of HPCC and Spark
                    conflicting for resources.</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable></para>
        </listitem>

        <listitem>
          <para>Specify a Spark Manager Node; Select the Instances tab.
          Right-click on the Instances table and choose <emphasis
          role="bold">Add Instances </emphasis></para>

          <para>Add the instance of your Spark manager node.</para>

          <para><informaltable colsep="1" frame="all" rowsep="1">
              <?dbfo keep-together="always"?>

              <tgroup cols="2">
                <colspec colwidth="49.50pt" />

                <colspec />

                <tbody>
                  <row>
                    <entry><inlinegraphic
                    fileref="images/caution.png" /></entry>

                    <entry>NOTE: You can only have one Spark Manager
                    Instance</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable></para>
        </listitem>

        <listitem>
          <para>Save the environment file. Exit configmgr (Ctrl+C). Copy the
          environment file from the source directory to the /etc/HPCCSystems
          directory.</para>

          <para><informaltable colsep="1" frame="all" rowsep="1">
              <?dbfo keep-together="always"?>

              <tgroup cols="2">
                <colspec colwidth="49.50pt" />

                <colspec />

                <tbody>
                  <row>
                    <entry><inlinegraphic
                    fileref="images/caution.png" /></entry>

                    <entry>Be sure system is stopped before attempting to move
                    the environment.xml file.</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable></para>

          <para><programlisting>sudo cp /etc/HPCCSystems/source/&lt;new environment file.xml&gt; /etc/HPCCSystems/environment.xml</programlisting>and
          distribute the new environment file to all the nodes in your
          cluster.</para>

          <para>You could choose to use the delivered hpcc-push.sh script to
          deploy the new environment file. For example:</para>

          <programlisting>sudo /opt/HPCCSystems/sbin/hpcc-push.sh -s &lt;sourcefile&gt; -t &lt;destinationfile&gt; </programlisting>
        </listitem>
      </orderedlist>

      <para>Now you can start your HPCC System cluster and verify that
      Sparkthor is alive.</para>

      <?hard-pagebreak ?>

      <para>To start your HPCC System.</para>

      <xi:include href="Installing_and_RunningTheHPCCPlatform/Inst-Mods/SysDStart.xml"
                  xpointer="element(/1)"
                  xmlns:xi="http://www.w3.org/2001/XInclude" />

      <para>Using a browser, navigate to your Integrated Spark's Manager
      instance (the instance you added above) running on port 8080 of your
      HPCC System.</para>

      <para>For example, http://nnn.nnn.nnn.nnn:8080, where nnn.nnn.nnn.nnn is
      your Integrated Spark Manager node's IP address.</para>

      <programlisting>https://192.168.56.101:8080</programlisting>

      <sect2 id="Addl_Spark_Config">
        <title>Integrated Spark Cluster Configuration Options</title>

        <para>In addition to the configuration options available through the
        HPCC Systems configuration manager, there are configuration options
        meant for edge cases and more advanced setups. To customize your
        Integrated Spark cluster environment to utilize these additional
        options use the provided <emphasis role="bold">spark-env.sh</emphasis>
        script.</para>

        <programlisting>/etc/HPCCSystems/externals/spark-hadoop/spark-env.sh</programlisting>

        <para>For more information about Spark Cluster options, see the
        following pages.</para>

        <itemizedlist>
          <listitem>
            <para><ulink
            url="https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts ">https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts</ulink></para>
          </listitem>

          <listitem>
            <para><ulink
            url="https://spark.apache.org/docs/latest/configuration.html#environment-variables ">https://spark.apache.org/docs/latest/configuration.html#environment-variables</ulink></para>
          </listitem>
        </itemizedlist>

        <sect3 id="ExampleUseCases">
          <title>Example Uses Cases</title>

          <itemizedlist>
            <listitem>
              <para>Spark currently requires Java 8 to run. On a system where
              the default Java installation is not Java 8. The JAVA_HOME
              environment variable can be used to set the Spark Java version
              to Java 8.</para>
            </listitem>

            <listitem>
              <para>Typically when a job is run on a Spark cluster, it will
              take ownership of all worker nodes. In a shared cluster
              environment this may not be ideal. Using the SPARK_MASTER_OPTS
              attribute it is possible to set a limit to the number of worker
              nodes one job can utilize.</para>
            </listitem>
          </itemizedlist>
        </sect3>
      </sect2>
    </sect1>
  </chapter>

  <chapter>
    <title>The Spark HPCC Systems Connector</title>

    <sect1 id="overview" role="nobrk">
      <title>Overview</title>

      <para>The Spark-HPCCSystems Distributed Connector is a Java library that
      facilitates access from a Spark cluster to data stored on an HPCC
      Systems cluster. The connector library employs the standard HPCC Systems
      remote file read facility to read data from either sequential or indexed
      HPCC datasets.</para>

      <para>The data on an HPCC cluster is partitioned horizontally, with data
      on each cluster node. Once configured, the HPCC data is available for
      reading in parallel by the Spark cluster.</para>

      <para>In the GitHub repository (<ulink
      url="https://github.com/hpcc-systems/Spark-HPCC">https://github.com/hpcc-systems/Spark-HPCC</ulink>)
      you can find the source code and examples. There are several artifacts
      in the DataAccess/src/main/java folder of primary interest. The
      <emphasis>org.hpccsystems.spark.HpccFile</emphasis> class is the façade
      of a file on an HPCC Cluster. The
      <emphasis>org.hpccsystems.spark.HpccRDD</emphasis> is a resilient
      distributed dataset derived from the data on the HPCC Cluster and is
      created by the
      <emphasis>org.hpccsystems.spark.HpccFile.getRDD</emphasis>(…) method.
      The <emphasis>HpccFile</emphasis> class supports loading data to
      construct a <emphasis>Dataset&lt;Row&gt;</emphasis> object for the Spark
      interface. This will first load the data into an RDD&lt;Row&gt; and then
      convert this RDD to a Dataset&lt;Row&gt; through internal Spark
      mechanisms.</para>

      <para>There are several additional artifacts of some interest. The
      <emphasis>org.hpccsystems.spark.ColumnPruner</emphasis> class is
      provided to enable retrieving only the columns of interest from the HPCC
      Cluster. The <emphasis>targetCluster</emphasis> artifact allows you to
      specify the HPCC cluster on which the target file exists. The
      <emphasis>org.hpccsystems.spark.thor.FileFilter</emphasis> class is
      provided to facilitate filtering records of interest from the HPCC
      Cluster.</para>

      <para>The git repository includes two examples under the
      Examples/src/main/scala folder. The examples
      (<emphasis>org.hpccsystems.spark_examples.Dataframe_Iris_LR</emphasis>
      and <emphasis>org.hpccsystems.spark_examples.Iris_LR</emphasis>) are
      Scala Objects with a main() function. Both examples use the classic Iris
      dataset. The dataset can be obtained from a variety of sources,
      including the HPCC-Systems/ecl-ml repository. IrisDs.ecl (can be found
      under the ML/Tests/Explanatory folder: <ulink
      url="https://github.com/hpcc-systems/ecl-ml/blob/master/ML/Tests/Explanatory/IrisDS.ecl">https://github.com/hpcc-systems/ecl-ml/blob/master/ML/Tests/Explanatory/IrisDS.ecl</ulink>)
      can be executed to generate the Iris dataset in HPCC. A walk-through of
      the examples is provided in the Examples section.</para>

      <para>The Spark-HPCCSystems Distributed Connector also supports PySpark.
      It uses the same classes/API as Java does.</para>

      <sect2 role="brk">
        <title>Special considerations</title>

        <sect3>
          <title>Unsigned Value Overflow</title>

          <para>Java does not support an unsigned integer type so reading
          UNSIGNED8 values from HPCC data can cause an integer overflow in
          Java. UNSIGNED8 values are often used as unique identifiers in
          datasets, in which case overflowing would be acceptable as the
          overflowed value will still be unique.</para>

          <para>The Spark-HPCC connector allows unsigned values to overflow in
          Java and will not report a exception. The caller is responsible for
          interpreting the value based on the recdef <emphasis
          role="bold">isunsigned</emphasis> flag.</para>
        </sect3>
      </sect2>

      <sect2 id="SparkSupportHPCCECLWatch" role="brk">
        <title>Spark support in ECL Watch</title>

        <para>As a part of HPCC Systems, the Spark connector can be monitored
        from the ECL Watch interface. See the <emphasis>ECL Watch</emphasis>
        manual for more information about using ECL Watch.</para>

        <para>The <emphasis role="bold">SparkThor</emphasis> cluster is listed
        on the ECL Watch System Servers page. To access the Systems Servers
        page:</para>

        <itemizedlist>
          <listitem>
            <para>In ECL Watch, click on the operations icon/link</para>
          </listitem>

          <listitem>
            <para>Click on the System Servers tab</para>
          </listitem>
        </itemizedlist>

        <para><graphic fileref="images/ECLWA-SprkThor.jpg" /></para>

        <para>This reports information for the manager node of the integrated
        Spark cluster. Here you can verify that the cluster is up and running,
        and you can run preflight by selecting the SparkThor cluster and
        pressing the submit button at the bottom of that page.</para>

        <para>Click on the disk icon next to your SparkThor cluster to access
        its logs.</para>

        <para>Click on the blue information icon to view more integrated Spark
        cluster information.</para>
      </sect2>
    </sect1>

    <sect1 id="primary-classes">
      <title>Primary Classes</title>

      <para>The <emphasis>HpccFile</emphasis> class and the
      <emphasis>HpccRDD</emphasis> classes are discussed in more detail below.
      These are the primary classes used to access data from an HPCC Cluster.
      The <emphasis>HpccFile</emphasis> class supports loading data to
      construct a <emphasis>Dataset&lt;Row&gt;</emphasis> object for the Spark
      interface. This will first load the data into an RDD&lt;Row&gt; and then
      convert this RDD to a Dataset&lt;Row&gt; through internal Spark
      mechanisms.</para>

      <para>The <emphasis>org.hpccsystems.spark.HpccFile</emphasis> class has
      several constructors. All of the constructors take information about the
      Cluster and the name of the dataset of interest. The JAPI WS-Client
      classes are used to access file detail information. A definition used to
      select the columns to be returned and a definition to select the rows to
      be returned could also be supplied. These are discussed in the
      <emphasis>Additional Classes of Interest</emphasis> section below. The
      class has two methods of primary interest: the
      <emphasis>getRDD(…)</emphasis> method and the
      <emphasis>getDataframe(…)</emphasis> method, which are illustrated in
      the <emphasis>Example</emphasis> section.</para>

      <para>The <emphasis>HpccFile</emphasis> class
      <emphasis>getRecordDefinition()</emphasis> method can be used to
      retrieve a definition of the file. The
      <emphasis>getFileParts()</emphasis> method can be used to see how the
      file is partitioned on the HPCC Cluster. These methods return the same
      information as can be found on the ECL Watch dataset details page DEF
      tab and the PARTS tab respectively.</para>

      <para>The <emphasis>org.hpccsystems.spark.HpccRDD</emphasis> class
      extends the <emphasis>RDD&lt;Record&gt;</emphasis> templated class. The
      class employs the <emphasis>org.hpccsystems.spark.HpccPart</emphasis>
      class for the Spark partitions. The
      <emphasis>org.hpccsystems.spark.Record</emphasis> class is used as the
      container for the fields from the HPCC Cluster. The
      <emphasis>Record</emphasis> class can create a <emphasis>Row</emphasis>
      instance with a schema.</para>

      <para>The <emphasis>HpccRDD</emphasis> HpccPart partition objects each
      read blocks of data from the HPCC Cluster independently from each other.
      The initial read fetches the first block of data, requests the second
      block of data, and returns the first record. When the block is
      exhausted, the next block should be available on the socket and new read
      request is issued.</para>

      <para>The <emphasis>HpccFileWriter</emphasis> is another primary class
      used for writing data to an HPCC Cluster. It has a single constructor
      with the following signature:</para>

      <programlisting>public HpccFileWriter(String connectionString, String user, String pass) throws Exception { </programlisting>

      <para>The first parameter <emphasis>connectionString</emphasis> contains
      the same information as <emphasis>HpccFile</emphasis>. It should be in
      the following format:
      {http|https}://{ECLWATCHHOST}:{ECLWATCHPORT}</para>

      <para>The constructor will attempt to connect to HPCC. This connection
      will then be used for any subsequent calls to
      <emphasis>saveToHPCC</emphasis>.</para>

      <programlisting>public long saveToHPCC(SparkContext sc, RDD&lt;Row&gt; scalaRDD, String clusterName, 
                        String fileName) throws Exception {</programlisting>

      <para>The <emphasis>saveToHPCC</emphasis> method only supports
      RDD&lt;row&gt; types. You may need to modify your data representation to
      use this functionality. However, this data representation is what is
      used by Spark SQL and by HPCC. This is only supported by writing in a
      co-located setup. Thus Spark and HPCC must be installed on the same
      nodes. Reading only supports reading data in from a remote HPCC
      cluster.</para>

      <para>The <emphasis>clusterName</emphasis> as used in the above case is
      the desired cluster to write data to, for example, the "mythor" Thor
      cluster. Currently there is only support for writing to Thor clusters.
      Writing to a Roxie cluster is not supported and will return an
      exception. The filename as used in the above example is in the HPCC
      format, for example: "~example::text".</para>

      <para>Internally the saveToHPCC method will Spawn multiple Spark jobs.
      Currently, this spawns two jobs. The first job maps the location of
      partitions in the Spark cluster so it can provide this information to
      HPCC. The second job does the actual writing of files. There are also
      some calls internally to ESP to handle things like starting the writing
      process by calling <emphasis>DFUCreateFile</emphasis> and publishing the
      file once it has been written by calling
      <emphasis>DFUPublishFile</emphasis>.</para>
    </sect1>

    <sect1 id="additional-classes-of-interest">
      <title>Additional Classes of interest</title>

      <para>The main classes of interest for this section are column pruning
      and file filtering. In addition there is a helper class to remap IP
      information when required, and this is also discussed below.</para>

      <para>The column selection information is provided as a string to the
      <emphasis>org.hpccsystems.spark.ColumnPruner</emphasis> object. The
      string is a list of comma separated field names. A field of interest
      could contain a row or child dataset, and the dotted name notation is
      used to support the selection of individual child fields. The
      <emphasis>ColumnPruner</emphasis> parses the string into a root
      <emphasis>TargetColumn</emphasis> class instance which holds the top
      level target columns. A <emphasis>TargetColumn</emphasis> can be a
      simple field or can be a child dataset and so be a root object for the
      child record layout.</para>

      <para>The row filter is implemented in the
      <emphasis>org.hpccsystems.spark.thor.FileFilter</emphasis> class. A
      <emphasis>FileFilter</emphasis> instance is constricted from an array of
      <emphasis>org.hpccsystems.spark.thor.FieldFilter</emphasis> objects.
      Each <emphasis>FieldFilter</emphasis> instance is composed of a field
      name (in doted notation for compound names) and an array of
      <emphasis>org.hpccsystems.spark.thor.FieldFilterRange</emphasis>
      objects. Each <emphasis>FieldFilterRange</emphasis> instance can be an
      open or closed interval or a single value. The record is selected when
      at least one <emphasis>FieldFilterRange</emphasis> matches for each of
      the <emphasis>FieldFilter</emphasis> instances in the array.</para>

      <para>The <emphasis>FieldFilterRange</emphasis> values may be either
      strings or numbers. There are methods provided to construct the
      following range tests: equals, not equals, less than, less than or
      equals, greater than, and a greater than or equals. In addition, a set
      inclusion test is supported for strings. If the file is an index, the
      filter fields that are key fields are used for an index lookup. Any
      filter field unmentioned is treated as wild.</para>

      <para>The usual deployment architecture for HPCC Clusters consists of a
      collection of nodes on a network. The file management information
      includes the IP addresses of the nodes that hold the partitions of the
      file. The Spark-HPCC connector classes use these IP addresses to
      establish socket connections for the remote read. An HPCC Cluster may be
      deployed as a virtual cluster with private IP addresses. This works for
      the cluster components because they are all on the same private LAN.
      However, the Spark cluster nodes might not be on that same LAN. In this
      case, the <emphasis>org.hpccsystems.spark.RemapInfo</emphasis> class is
      used to define the information needed to change the addressing. There
      are two options that can be used. The first option is that each Thor
      worker node can be assigned an IP that is visible to the Spark cluster.
      These addresses must be a contiguous range. The second option is to
      assign an IP and a contiguous range of port numbers. The
      <emphasis>RemapInfo</emphasis> object is supplied as a parameter.</para>
    </sect1>

    <sect1 id="examples">
      <title>Examples</title>

      <para>We will walk through the two examples below utilizing a Spark
      environment. Additionally, the repository provides testing programs (in
      the DataAccess/src/test folder) that can be executed as stand-alone
      examples.  </para>

      <para>These test programs are intended to be run from a development IDE
      such as Eclipse via the Spark-submit application whereas the examples
      below are dependent on the Spark shell.</para>

      <para>The following examples assume a Spark Shell. You can use the
      spark-submit command if you intend to compile and package these
      examples. To properly connect your spark shell to the Integrated Spark
      cluster, provide the following parameters when starting the
      shell:</para>

      <programlisting>bin/spark-shell  \
 --master=&lt;spark://{remotesparkhost-IP}:{sparkport}&gt; --conf="spark.driver.host={localhost-ip}" </programlisting>

      <sect2 id="iris_lr">
        <title>Iris_LR</title>

        <para>This example assumes that you have Spark Shell running. The next
        step is to establish your HpccFile and your RDD for that file. You
        need the name of the file, the protocol (http or https), the name or
        IP of the ESP, the port for the ESP (usually 8010), and your user
        account and password. The <emphasis>sc</emphasis> value is the
        <emphasis>SparkContext</emphasis> object provided by the shell.</para>

        <programlisting> val espcon = new Connection("http", "myeclwatchhost", "8010");
 espcon.setUserName("myuser");
 espcon.setPassword("mypass");
 val file = new HpccFile("myfile",espcon);
</programlisting>

        <para>Now we have an RDD of the data. Nothing has actually happened at
        this point because Spark performs lazy evaluation and there is nothing
        yet to trigger an evaluation.</para>

        <para>The Spark MLLib has a Logistic Regression package. The MLLib
        Logistic Regression expects that the data will be provided as Labeled
        Point formatted records. This is common for supervised training
        implementations in MLLib. We need column labels, so we make an array
        of names. We then make a labeled point RDD from our RDD. This is also
        just a definition. Finally, we define the Logistic Regression that we
        want to run. The column names are the field names in the ECL record
        definition for the file, including the name “class” which is the name
        of the field holding the classification code.</para>

        <programlisting> val names = Array("petal_length","petal_width", "sepal_length", 
                   "sepal_width")
 var lpRDD = myRDD.makeMLLibLabeledPoint("class", names)
 val lr = new LogisticRegressionWithLBFGS().setNumClasses(3)</programlisting>

        <para>The next step is to define the model, which is an action and
        will cause Spark to evaluate the definitions.</para>

        <programlisting> val iris_model = lr.run(lpRDD)</programlisting>

        <para>Now we have a model. We will utilize this model to take our
        original dataset and use the model to produce new labels. The correct
        way to do this is to have randomly sampled some hold out data. We are
        just going to use the original dataset because it is easier to show
        how to use the connector. We then take our original data and use a map
        function defined inline to create a new record with our prediction
        value and the original classification.</para>

        <programlisting> val predictionAndLabel = lpRDD.map {
   case LabeledPoint(label, features) =&gt;
     val prediction = iris_model.predict(features)
   (prediction, label)
 }</programlisting>

        <para>The <emphasis>MulticlassMetrics</emphasis> class can now be used
        to produce a confusion matrix.</para>

        <programlisting> val metrics = new MulticlassMetrics(predictionAndLabel)
 metrics.confusionMatrix</programlisting>
      </sect2>

      <sect2 id="dataframe_iris_lr">
        <title>Dataframe_Iris_LR</title>

        <para>The Dataframe_Iris_LR is similar to the Iris_LR, except that a
        Dataframe is used and the new ML Spark classes are used instead of the
        old MLLib classes. Since ML is not completely done, we do fall back to
        an MLLib class to create our confusion matrix.</para>

        <para>Once the Spark shell is brought up, we need our import
        classes.</para>

        <para><programlisting> import org.hpccsystems.spark.HpccFile
 import org.apache.spark.sql.Dataset
 import org.apache.spark.ml.feature.VectorAssembler
 import org.apache.spark.ml.classification.LogisticRegression
 import org.apache.spark.mllib.evaluation.MulticlassMetrics</programlisting></para>

        <para>The next step is to establish the <emphasis>HpccFile</emphasis>
        object and create the Dataframe. The <emphasis>spark</emphasis> value
        is a <emphasis>SparkSession</emphasis> object supplied by the shell
        and is used instead of the <emphasis>SparkContext</emphasis>
        object.</para>

        <programlisting> val espcon = new Connection("http", "myeclwatchhost", "8010");
 espcon.setUserName("myuser");
 espcon.setPassword("mypass");
 val file = new HpccFile("myfile",espcon);
</programlisting>

        <para>The Spark <emphasis>ml</emphasis> Machine Learning classes use
        different data container classes. In the case of Logistic Regression,
        we need to transform our data rows into a row with a column named
        “features” holding the features and a column named “label” holding the
        classification label. Recall that our row has “class”, “sepal_width”,
        “sepal_length”, “petal_width”, and “petal_length” as the column names.
        This kind of transformation can be accomplished with a
        <emphasis>VectorAssembler</emphasis> class.</para>

        <programlisting> val assembler = new VectorAssembler()
 assembler.setInputCols(Array("petal_length","petal_width",
                              "sepal_length", "sepal_width"))
 assembler.setOutputCol("features")
 val iris_fv = assembler.transform(my_df)
                              .withColumnRenamed("class", "label")</programlisting>

        <para>Now that the data (<emphasis>iris_fv</emphasis>) is ready, we
        define our model and fit the data.</para>

        <programlisting> val lr = new LogisticRegression()
 val iris_model = lr.fit(iris_fv)</programlisting>

        <para>We now want to apply our prediction and evaluate the results. As
        noted before, we would use a holdout dataset to perform the
        evaluation. We are going to be lazy and just use the original data to
        avoid the sampling task. We use the <emphasis>transform(…)</emphasis>
        function for the model to add the prediction. The function adds a
        column named “prediction” and defines a new dataset. The new Machine
        Learning implementation lacks a metrics capability to produce a
        confusion matrix, so we will then take our dataset with the
        <emphasis>prediction</emphasis> column and create a new RDD with a
        dataset for a <emphasis>MulticlassMetrics</emphasis> class.</para>

        <programlisting> val with_preds = iris_model.transform(iris_fv)
 val predictionAndLabel = with_preds.rdd.map(
 r =&gt; (r.getDouble(r.fieldIndex("prediction")),
 r.getDouble(r.fieldIndex("label"))))
 val metrics = new MulticlassMetrics(predictionAndLabel)
 metrics.confusionMatrix</programlisting>
      </sect2>
    </sect1>
  </chapter>
</book>
