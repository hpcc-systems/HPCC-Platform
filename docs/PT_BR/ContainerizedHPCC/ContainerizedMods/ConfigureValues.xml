<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ContainerConfigurationValuesCHAPTER">
  <title>Configuration Values</title>

  <para>Este capítulo descreve a configuração do HPCC Systems para uma
  implantação Kubernetes em contêineres. As seções a seguir detalham como as
  configurações são fornecidas aos charts do helm, como descobrir quais opções
  estão disponíveis e alguns detalhes da estrutura do arquivo de configuração.
  As seções subsequentes também fornecerão uma breve explicação de alguns dos
  conteúdos do arquivo padrão <emphasis>values.yaml</emphasis>, usado na
  configuração do HPCC Systems para uma implantação em contêiner.</para>

  <sect1 id="Intro_Containerized_Environments" role="nobrk">
    <title>O Ambiente do Contêiner</title>

    <para>Uma das ideias por trás de nossa mudança para a nuvem foi tentar
    simplificar a configuração do sistema e, ao mesmo tempo, fornecer uma
    solução flexível o suficiente para atender às demandas de nossa
    comunidade, aproveitando os recursos do contêiner sem sacrificar o
    desempenho.</para>

    <para>Toda a configuração do HPCC Systems no contêiner é gerida por um
    único arquivo, um arquivo <emphasis>values.yaml</emphasis> e associado ao
    schema (<emphasis>values-schema.json</emphasis>) file.</para>

    <sect2 id="WhatIsValues.Yaml">
      <title>O <emphasis>values.yaml</emphasis> e como é utilizado</title>

      <para>O arquivo de estoque <emphasis>values.yaml</emphasis>, fornecido
      no repositório HPCC Systems, são os valores de configuração fornecidos
      para o Helm chart "hpcc". O arquivo <emphasis>values.yaml</emphasis> é
      usado pelo Helm chart para controlar como HPCC systems é implantado na
      nuvem. Este arquivo <emphasis>values.yaml</emphasis> é um único arquivo
      usado para configurar e obter uma instância do HPCC Systems em execução
      no Kubernetes. O arquivo <emphasis>values.yaml</emphasis> define tudo o
      que acontece para configurar e/ou definir seu sistema para implantação
      em contêiner. Você deve usar o arquivo de valores fornecido como base
      para personalizações da modelagem do seus requisitos para sua
      implantação específica.</para>

      <para>O arquivo <emphasis>values.yaml</emphasis> do HPCC Systems pode
      ser encontrado no repositório github do HPCC Systems. Para usar o chart
      Helm do HPCC Systems, primeiro adicione o repositório de charts hpcc
      usando o Helm e, em seguida, acesse os valores do chart Helm dos charts
      nesse repositório.</para>

      <para>Por exemplo, ao adicionar o repositório "hpcc", conforme
      recomendado antes de instalar o chart do Helm com o seguinte
      comando:</para>

      <programlisting>helm repo add hpcc https://hpcc-systems.github.io/helm-chart
</programlisting>

      <para>Agora você pode visualizar os charts entregues do HPCC Systems e
      ver os valores lá emitindo:</para>

      <programlisting>helm show values hpcc/hpcc</programlisting>

      <para>Você pode capturar a saída deste comando, ver como os padrões são
      configurados e usá-lo como base para sua customização.</para>
    </sect2>

    <sect2 id="Values-SchemaJSONFile" role="brk">
      <title>O values-schema.json</title>

      <para>O <emphasis>values-schema.json</emphasis> é um arquivo JSON que
      declara o que é válido e o que não está dentro da soma total dos valores
      mesclados que são passados para o Helm no momento da instalação. Ele
      define quais valores são permitidos e valida o arquivo de valores em
      relação a eles. Todos os itens principais são declarados no arquivo de
      esquema, enquanto o arquivo default <emphasis>values.yaml</emphasis>
      também contém comentários sobre os elementos mais importantes.</para>

      <para>Se você quiser saber quais opções estão disponíveis para qualquer
      componente específico, o esquema é um bom lugar para começar.</para>

      <para>O arquivo de esquema normalmente contém (para uma propriedade) um
      nome e uma descrição. Muitas vezes, incluirá detalhes do tipo e os itens
      que pode conter se for uma lista ou dicionário. Por exemplo:</para>

      <programlisting>    "roxie": { 
      "description": "roxie process",
      "type": "array"
      "items": { "$ref": "#/definitions/roxie" }
    },</programlisting>

      <para>Cada plano, no arquivo de esquema, tem uma lista de propriedades
      geralmente contendo um prefixo (caminho), um subcaminho (subcaminho) e
      propriedades adicionais. Por exemplo, para um plano de armazenamento, o
      arquivo de esquema possui uma lista de propriedades, incluindo o
      prefixo. Os "planos" neste caso são uma referência ($ref) para outra
      seção do esquema. O arquivo de esquema deve ser completo e conter tudo o
      que é necessário, incluindo descrições que devem ser relativamente
      autoexplicativas.</para>

      <programlisting>    "storage": {
      "type": "object",
      "properties": {
        "hostGroups": {
          "$ref": "#/definitions/hostGroups"
        },
        "planes": {
          "$ref": "#/definitions/storagePlanes"
        }
      },
      "additionalProperties": false
</programlisting>

      <para>Observe o valor de <emphasis>additionalProperties</emphasis>
      normalmente no final de cada seção no esquema. Ele especifica se os
      valores permitem propriedades adicionais ou não. Se esse valor
      <emphasis>additionalProperties</emphasis> estiver presente e definido
      como false, nenhuma outra propriedade será permitida e a lista de
      propriedades estará completa.</para>

      <para>Ao trabalhar com o HPCC Systems <emphasis>values.yam</emphasis>, o
      arquivo de valores deve ser validado em relação a esse esquema. Se
      houver um valor que não seja permitido conforme definido no arquivo de
      esquema, ele não será iniciado e, em vez disso, gerará um ERRO.</para>
    </sect2>
  </sect1>

  <sect1 id="TheValuesYaml_FileAndRelated" role="nobrk">
    <title>Componentes HPCC Systems e o Arquivo
    <emphasis>values.yaml</emphasis></title>

    <para>Os gráficos de Helm do HPCC Systems são enviados com valores de
    estoque/padrão. Esses charts do Helm têm um conjunto de valores padrão
    idealmente para serem usados como guia na configuração de sua implantação.
    Geralmente, cada componente do HPCC Systems é uma lista. Essa lista define
    as propriedades para cada instância do componente.</para>

    <para>Esta seção fornecerá detalhes adicionais e qualquer percepção digna
    de nota para os componentes do HPCC Systems definidos no arquivo
    <emphasis>values.yaml</emphasis>.</para>

    <sect2 id="YAMLHPCC_Components">
      <title>Os Componentes do HPCC Systems</title>

      <para>Uma das principais diferenças entre o bare metal e o
      contêiner/nuvem é que o armazenamento bare metal está diretamente
      vinculado aos nós do job trabalho Thor ou Thor e aos nós de trabalho
      Roxie, ou mesmo no caso do servidor ECLCC as DLLs. Nos contêineres, eles
      são completamente separados e qualquer coisa relacionada a arquivos é
      definida no arquivo <emphasis>values.yaml</emphasis></para>

      <para>Em contêineres, as instâncias de componentes são executadas
      dinamicamente. Por exemplo, se você configurou seu sistema para usar um
      Thor de 50 vias, então um Thor de 50 vias será gerado quando um trabalho
      for enfileirado para ele. Quando esse trabalho for concluído, a
      instância Thor desaparecerá. Este é o mesmo padrão para os outros
      componentes também.</para>

      <para>Cada componente deve ter uma entrada de recursos, no arquivo
      <emphasis>values.yaml</emphasis> entregues os recursos estão presentes,
      mas comentados conforme indicado aqui.</para>

      <para><programlisting>  #resources:
  #  cpu: "1"
  #  memory: "4G"
</programlisting>The stock values file will work and allow you to stand up a
      functional system, however you should define the component resources in
      a manner that corresponds best to your operational strategy.</para>

      <sect3 id="YML_HPCCSystemsServices">
        <title>Os Serviços do Sistema</title>

        <para>A maioria dos componentes do HPCC Systems tem uma entrada de
        definição de serviço, semelhante à entrada de recursos. Todos os
        componentes que possuem definições de serviço seguem esse mesmo
        padrão.</para>

        <para>Qualquer informação relacionada ao serviço precisa estar em um
        objeto de serviço, por exemplo:</para>

        <para><programlisting>  service:
    servicePort: 7200
    visibility: local
</programlisting></para>

        <para>Isso se aplica à maioria dos componentes do HPCC Systems, ESP,
        Dali, dafilesrv e Sasha. A especificação do Roxie é um pouco
        diferente, pois tem seu serviço definido em "roxieservice". Cada Roxie
        pode ter várias definições de "roxieservice". (ver esquema).</para>
      </sect3>

      <sect3 id="DALI_ValueYAML" role="brk">
        <title>Dali</title>

        <para>Ao configurar o Dali, que também possui uma seção de recursos,
        ele também precisará de muita memória e uma boa quantidade de CPU. É
        muito importante defini-los com cuidado. Caso contrário, o Kubernetes
        pode atribuir todos os pods à mesma máquina virtual e os componentes
        que lutam pela memória os esmagarão. Portanto, mais memória atribuída
        melhor. Se você definir isso errado e um processo usar mais memória do
        que o configurado, o Kubernetes matará o pod.</para>
      </sect3>

      <sect3 id="DAFLESRV_DFURVR_YMLSECT">
        <title>Componentes: dafilesvrs, dfuserver</title>

        <para>Os componentes do HPCC Systems de dafilesvrs, eclccservers,
        dfuserver, são declarados como listas no arquivo YAML, assim como o
        ECL Agent.</para>

        <para>Considere o dfuserver que está nos
        <emphasis>values.yaml</emphasis> entregues do HPCC Systems
        como:</para>

        <programlisting>dfuserver:
- name: dfuserver
  maxJobs: 1</programlisting>

        <para>If you were to add a mydfuserver as follows</para>

        <para><programlisting>dfuserver:
- name: dfuserver
  maxJobs: 1
- name: mydfuserver
  maxJobs: 1
</programlisting>Nesse cenário, você teria outro item aqui chamado
        mydfuserver, ele apareceria no ECLWatch e você poderia enviar itens
        para ele.</para>

        <para>Se você quiser adicionar outro dfuserver, poderá adicioná-lo à
        lista da mesma forma. Você também pode instanciar outros componentes
        adicionando-os às suas respectivas listas.</para>
      </sect3>

      <sect3 id="VALYml_ECLCCServer">
        <title>ECL Agent e ECLCC Server</title>

        <para>Values of note for the ECL Agent and ECLCC Server.</para>

        <para><emphasis role="bold">useChildProcess</emphasis> -- Conforme
        definido no esquema, iniciada cada compilação da workunit como um
        processo secundário em vez de em seu próprio contêiner. Quando você
        envia um job ou consulta para compilar, ele é enfileirado e
        processado, com essa opção definida como true, ele gerará um processo
        secundário utilizando quase nenhuma sobrecarga adicional na
        inicialização. Ideal para enviar muitos jobs pequenos para compilar.
        No entanto, como cada job de compilação não é mais executado como um
        pod independente com suas próprias especificações de recursos, mas é
        executado como um processo secundário no próprio pod do servidor
        ECLCC, o pod do servidor ECLCC deve ser definido com recursos
        adequados para si mesmo (mínimo para ouvir para a fila etc.) e todos
        os jobs que ele possa ter que executar em paralelo.</para>

        <para>Por exemplo, imagine que <emphasis>maxJobs</emphasis> está
        definido como 4 e 4 consultas grandes são enfileiradas rapidamente, o
        que significa que 4 processos secundário são iniciados, cada cpu
        consumindo e memória dentro do pod do servidor ECLCC. Com o componente
        configurado com <emphasis>useChildProcesses</emphasis> definido como
        true, cada trabalho será executado no mesmo pod (até o valor de
        <emphasis>maxJobs</emphasis> em paralelo). Portanto, com
        <emphasis>useChildProcesses</emphasis> habilitado, os recursos do
        componente devem ser definidos de forma que o pod tenha recursos
        suficientes para lidar com as demandas de recursos de todos esses
        trabalhos para poder ser executado em paralelo.</para>

        <para>Com useChildProcess ativado, pode ser bastante caro na maioria
        dos modelos de preços de nuvem e bastante dispendioso se não houver
        nenhum job em execução. Em vez disso, você pode definir esse
        <emphasis>useChildprocess</emphasis> como false (o padrão) para
        iniciar um pod para compilar cada consulta apenas com a memória
        necessária para o trabalho que será descartado quando concluído.
        Agora, esse modelo também ouviu, talvez 20 segundos a um minuto para
        gerar o cluster Kubernetes para processar o trabalho. O que pode não
        ser ideal para um ambiente que está enviando vários trabalhos
        pequenos, mas sim jobs maiores que minimizariam o efeito da sobrecarga
        ao iniciar o cluster Kubernetes.</para>

        <para>Definir <emphasis>useChildProcess</emphasis> como false permite
        melhor a possibilidade de dimensionamento dinâmico. Para jobs que
        levariam muito tempo para compilar, a sobrecarga extra (inicialização)
        é mínima, e esse seria o caso ideal para ter o
        <emphasis>useChildProcess</emphasis> como falso. Definir
        <emphasis>useChildProcess</emphasis> como false permite apenas 1 pod
        por compilação, embora haja um atributo para colocar um limite de
        tempo nessa compilação.</para>

        <para><emphasis role="bold">ChildProcessTimeLimit</emphasis> é o tempo
        limite (em segundos) para compilação de processos secundários antes de
        abortarem e usarem um contêiner separado, quando o
        <emphasis>useChildProcesses</emphasis> é false.</para>

        <para><emphasis role="bold">maxActive</emphasis> -- O número máximo de
        jobs que podem ser executadas em paralelo. Novamente, tome cuidado
        porque cada job precisará de memória suficiente para ser executado.
        Por exemplo, se <emphasis>maxActive</emphasis> estiver definido como
        2000, você poderá enviar um trabalho muito grande e, nesse caso, gerar
        cerca de 2.000 trabalhos usando uma quantidade considerável de
        recursos, o que poderia gerar uma conta de compilação bastante cara,
        novamente dependendo do seu provedor de nuvem e seu plano de
        faturamento.</para>
      </sect3>

      <sect3 id="ValYML_Sasha">
        <title>Sasha</title>

        <para>A configuração para Sasha é uma exceção, pois é uma estrutura do
        tipo dicionário e não uma lista. Você não pode ter mais de um
        arquivador ou dfuwu-archiver, pois isso é uma limitação de valor, você
        pode optar por ter o serviço ou não (defina o valor 'disabled' como
        true).</para>
      </sect3>

      <sect3 id="ValYML_Thor">
        <title>Thor</title>

        <para>As instâncias Thor são executadas dinamicamente, assim como os
        outros componentes em contêineres. A configuração do Thor também
        consiste em uma lista de instâncias do Thor. Cada instância gera
        dinamicamente uma coleção de pods (manager + N workers) quando os
        workers são enfileirados para ela. Quando ocioso, não há pods de
        worker (ou manager) em execução.</para>

        <para>Se você quisesse um Thor de 50 vias, você definiria o número de
        workers, o valor <emphasis role="bold">numWorkers</emphasis> para 50 e
        você teria um Thor de 50 vias. Conforme indicado no exemplo a
        seguir:</para>

        <para><programlisting>thor:
- name: thor
  prefix: thor
  numWorkers: 50</programlisting></para>

        <para>Ao fazer isso, o ideal é renomear o recurso para algo que o
        descreva claramente, como <emphasis>thor_50</emphasis> como no exemplo
        a seguir.</para>

        <para><programlisting>- name: thor_50</programlisting></para>

        <para>A atualização do valor <emphasis>numWorkers</emphasis>
        reiniciará o agente Thor ouvindo a fila, fazendo com que todos os
        novos jobs usem a nova configuração.</para>

        <para><emphasis role="bold">maxJobs</emphasis> -- Controla o número de
        jobs, especificamente <emphasis>maxJobs</emphasis> define o número
        máximo de jobs.</para>

        <para><emphasis role="bold">maxGraphs</emphasis> -- Limita a
        quantidade máxima de charts. Geralmente faz sentido manter esse valor
        abaixo ou no mesmo número de <emphasis>maxJobs</emphasis>, pois nem
        todos os jobs enviam charts e quando fazem os jobs Thor não estão
        executando charts o tempo todo. Se houver mais de 2 charts enviados
        (Thor), o segundo será bloqueado até que a próxima instância Thor
        fique disponível.</para>

        <para>A ideia aqui é que os jobs podem passar uma quantidade
        significativa de tempo fora dos charts, como aguardar um estado de
        fluxo de trabalho (fora do próprio mecanismo Thor), bloqueado em uma
        persistência ou atualizando super arquivos etc. ter um limite maior de
        trabalhos simultâneos (<emphasis>maxJobs</emphasis>) do que gráficos
        (instâncias <emphasis>maxGraphs</emphasis> / Thor). Como as instâncias
        Thor (charts) são relativamente caras (muitos pods/maior uso de
        recursos), enquanto os pods de fluxo de trabalho (jobs) são
        comparativamente baratos.</para>

        <para>Assim, os valores de charts entregues (exemplo) definem
        <emphasis>maxJobs</emphasis> como maior que
        <emphasis>maxGraphs</emphasis>. Os jobs enfileirados para um Thor nem
        sempre estão executando charts. Portanto, pode fazer sentido ter mais
        desses trabalhos, que não consomem um Thor grande e todos os seus
        recursos, mas restringem o número máximo de instâncias do Thor em
        execução.</para>

        <para>Thor têm 3 componentes (o que corresponde as seções de
        recurso).</para>

        <orderedlist>
          <listitem>
            <para>Workflow</para>
          </listitem>

          <listitem>
            <para>Manager</para>
          </listitem>

          <listitem>
            <para>Workers</para>
          </listitem>
        </orderedlist>

        <para>O Manager e os Workers são lançados juntos e normalmente
        consomem bastante recursos (e nós). Enquanto o Workflow é barato e
        geralmente não requer tantos recursos. Você pode esperar em um mundo
        Kubernetes, muitos deles coexistiriam no mesmo nó (e, portanto, seriam
        baratos). Portanto, faz sentido que <emphasis>maxJobs</emphasis> seja
        maior e <emphasis>maxGraphs</emphasis> seja menor</para>

        <para>No Kubernetes, os jobs são executados de forma independente em
        seus próprios pods. Enquanto no bare metal, podemos ter jobs que podem
        afetar outros trabalhos porque estão sendo executados no mesmo espaço
        de processo.</para>
      </sect3>

      <sect3 id="YAML_Thor_and_hThor_Memory">
        <title>Thor e Memória hThor</title>

        <para>As seções de <emphasis>memory</emphasis> Thor e hThor permitem
        que a memória de recursos do componente seja refinada em diferentes
        áreas.</para>

        <para>Por exemplo, o "workerMemory" para um Thor é definido
        como:</para>

        <programlisting>thor:
- name: thor
  prefix: thor
  numWorkers: 2
  maxJobs: 4
  maxGraphs: 2
  managerResources:
    cpu: "1"
    memory: "2G"
  workerResources:
    cpu: "4"
    memory: "4G"
  workerMemory:
    query: "3G"
    thirdParty: "500M"
  eclAgentResources:
    cpu: "1"
    memory: "2G"</programlisting>

        <para>A seção "<emphasis>workerResources</emphasis>" informará ao
        Kubernetes para recursos 4G por pod de worker. Por padrão, o Thor
        reservará 90% dessa memória para usar na memória de consulta HPCC
        (roxiemem). Os 10% restantes são deixados para todos os outros usos
        não baseados em linha (roxiemem), como heap geral, sobrecarga do
        sistema operacional etc. Não há permissão para qualquer biblioteca de
        terceiros, plug-ins ou uso de linguagem incorporada dentro desse
        padrão. Em outras palavras, se, por exemplo, o python incorporado
        alocar 4G, o processo logo falhará com um erro de falta de memória,
        quando começar a usar qualquer memória, pois esperava que 90% desse 4G
        estivesse disponível gratuitamente para uso próprio.</para>

        <para>Esses padrões podem ser substituídos pelas seções de memória.
        Neste exemplo, <emphasis>workerMemory.query</emphasis> define que 3G
        da memória com recursos disponíveis deve ser atribuído à memória de
        consulta e 500M para usos de "thirdParty".</para>

        <para>Isso limita o uso de memória <emphasis>roxiemem</emphasis> do
        HPCC Systems a exatamente 3G, deixando 1G livre para outros
        propósitos. O "thirdParty" não é realmente alocado, mas é usado
        exclusivamente como parte do total em execução, para garantir que a
        configuração não especifique um total nesta seção maior do que a seção
        de recursos, por exemplo, se "thirdParty" fosse definido como "2G" na
        seção acima, haveria uma reclamação de tempo de execução quando o Thor
        fosse executado de que a definição excedeu o limite de recurso.</para>

        <para>Também é possível substituir a porcentagem recomendada padrão
        (90% por padrão), definindo <emphasis>maxMemPercentage</emphasis>. Se
        "query" não estiver definida, ela será calculada como a memória máxima
        recomendada menos a memória definida (por exemplo,
        "thirdParty).</para>

        <para>No Thor existem 3 áreas de recursos, <emphasis>eclAgent,
        ThorManager </emphasis>e<emphasis> ThorWorker(s)</emphasis>. Cada um
        tem uma área *Resource que define suas necessidades de recursos do
        Kubernetes e uma seção *Memory correspondente que pode ser usada para
        substituir os requisitos de alocação de memória padrão.</para>

        <para>Essas configurações também podem ser substituídas por consulta,
        por meio de opções de workunits seguindo o padrão:
        &lt;memory-section-name&gt;.&lt;property&gt;. Por exemplo:
        #option('workerMemory.thirdParty', "1G");</para>

        <para><emphasis role="bold">Nota</emphasis>: Atualmente, há apenas
        "consulta" (uso de HPCC roxiemem) e "thirdParty" para todos/qualquer
        uso de terceiros. É possível que outras categorias sejam adicionadas
        no futuro, como "python" ou "java" - que definem especificamente os
        usos de memória para esses destinos..</para>
      </sect3>
    </sect2>
  </sect1>

  <sect1 id="Delivered_HPCC_ValuesYaml">
    <title>O arquivo HPCC Systems <emphasis>values.yaml</emphasis></title>

    <para>O arquivo HPCC systems <emphasis>values.yaml</emphasis> entregue é
    mais um exemplo que fornece uma configuração de tipo básico que deve ser
    personalizada para suas necessidades específicas. Uma das principais
    ideias por trás do arquivo de valores é poder personalizá-lo com relativa
    facilidade para seu cenário específico. O chart entregue é configurado
    para ser sensato o suficiente para ser entendido, ao mesmo tempo em que
    permite uma personalização relativamente fácil para configurar um sistema
    de acordo com seus requisitos específicos. Esta seção examinará mais de
    perto alguns aspectos do arquivo <emphasis>values.yaml</emphasis>.</para>

    <para>O arquivo HPCC Systems Values entregue consiste principalmente nas
    seguintes áreas:</para>

    <para><informaltable>
        <tgroup cols="3">
          <tbody>
            <row>
              <entry>global</entry>

              <entry>storage</entry>

              <entry>visibilities</entry>
            </row>

            <row>
              <entry>data planes</entry>

              <entry>certificates</entry>

              <entry>security</entry>
            </row>

            <row>
              <entry>secrets</entry>

              <entry>components</entry>

              <entry/>
            </row>
          </tbody>
        </tgroup>
      </informaltable></para>

    <para>As seções subsequentes examinarão alguns deles mais de perto e por
    que cada um deles está lá.</para>

    <sect2 id="ValYAML_STorage">
      <title>Armazenamento</title>

      <para>O armazenamento em contêiner é outro conceito-chave que difere do
      bare metal. Existem algumas diferenças entre contêiner e armazenamento
      em metal. A seção Storage é bastante bem definida entre o arquivo de
      esquema e o <emphasis>values.yaml</emphasis>. Uma boa abordagem para
      armazenamento é entender claramente suas necessidades de armazenamento e
      descrevê-las, e uma vez que você tenha essa estrutura básica em mente, o
      esquema pode ajudar a preencher os detalhes. O esquema deve ter uma
      descrição decente para cada atributo. Todo o armazenamento deve ser
      definido por meio de planos. Há um comentário relevante no arquivo
      <emphasis>values.yaml</emphasis> descrevendo melhor o
      armazenamento.</para>

      <programlisting>## storage:
##
## 1. If an engine component has the dataPlane property set, 
#       then that plane will be the default data location for that component.
## 2. If there is a plane definition with a category of "data" 
#       then the first matching plane will be the default data location
##
## If a data plane contains the storageClass property then an implicit pvc 
#       will be created for that data plane.
##
## If plane.pvc is defined, a Persistent Volume Claim must exist with that name, 
#       storageClass and storageSize are not used.
##
## If plane.storageClass is defined, storageClassName: &lt;storageClass&gt;
## If set to "-", storageClassName: "", which disables dynamic provisioning
## If set to "", choosing the default provisioner.  
#       (gp2 on AWS, standard on GKE, AWS &amp; OpenStack)
##
## plane.forcePermissions=true is required by some types of provisioned
## storage, where the mounted filing system has insufficient permissions to be
## read by the hpcc pods. Examples include using hostpath storage (e.g. on
## minikube and docker for desktop), or using NFS mounted storage.
</programlisting>

      <para>Existem diferentes categorias de armazenamento, para uma
      implantação de HPCC Systems você deve ter no mínimo uma categoria dali,
      uma categoria dll e pelo menos 1 categoria de dados. Esses tipos
      geralmente são aplicáveis a todas as configurações, além de outras
      categorias opcionais de dados.</para>

      <para>Todo o armazenamento deve estar em uma definição de plano de
      armazenamento. Isso é melhor descrito no comentário na definição de
      armazenamento no arquivo de valores.</para>

      <programlisting>planes:
  #   name: &lt;required&gt;
  #   prefix: &lt;path&gt;                        # Root directory for accessing the plane 
                                            # (if pvc defined), 
  #                                         # or url to access plane.
  #   category: data|dali|lz|dll|spill|temp # What category of data is stored on this plane?
  #
  # For dynamic pvc creation:
  #   storageClass: ''
  #   storageSize: 1Gi
  #
  # For persistent storage:
  #   pvc: &lt;name&gt;                           # The name of the persistant volume claim
  #   forcePermissions: false
  #   hosts: [ &lt;host-list&gt; ]                 # Inline list of hosts
  #   hostGroup: &lt;name&gt;                     # Name of the host group for bare-metal 
  #                                         # must match the name of the storage plane..
  #
  # Other options:
  #   subPath: &lt;relative-path&gt;              # Optional sub directory within &lt;prefix&gt; 
  #                                         # to use as the root directory
  #   numDevices: 1                         # number of devices that are part of the plane
  #   secret: &lt;secret-id&gt;                   # what secret is required to access the files.  
  #                                         # This could optionally become a list if required 
                                            # (or add secrets:).

  #   defaultSprayParts: 4                  # The number of partitions created when spraying 
                                            # (default: 1)

  #   cost:                                 # The storage cost
  #     storageAtRest: 0.0135               # Storage at rest cost: cost per GiB/month</programlisting>

      <para>Cada plano tem 3 campos obrigatórios: O nome, a categoria e o
      prefixo.</para>

      <para>Quando o sistema estiver instalado, usando os valores fornecidos
      em estoque, ele criará um volume de armazenamento com capacidade de 1 GB
      através das seguintes propriedades.</para>

      <para>Por exemplo:</para>

      <programlisting>- name: dali
  storageClass: ""
  storageSize: 1Gi
  prefix: "/var/lib/HPCCSystems/dalistorage"
  category: dali
</programlisting>

      <para>Mais comumente o prefixo: define o caminho dentro do contêiner
      onde o armazenamento está montado. O prefixo pode ser uma URL para
      armazenamento de blobs. Todos os pods usarão o caminho (prefixo: ) para
      acessar o armazenamento.</para>

      <para>Para o exemplo acima, quando você observar a lista de
      armazenamento, o <emphasis>storageSize</emphasis> criará um volume com 1
      GB de capacidade. O prefixo será o caminho, a categoria é usada para
      limitar o acesso aos dados e minimizar o número de volumes acessíveis de
      cada componente.</para>

      <para>As listas de armazenamento dinâmico no arquivo
      <emphasis>values.yaml</emphasis> são caracterizadas pelos valores
      storageClass: e storageSize:.</para>

      <para><emphasis role="bold">storageClass</emphasis>: define qual storage
      deve ser usado para alocar o armazenamento. Uma classe de armazenamento
      em branco indica que deve usar a classe de armazenamento de provedores
      de nuvem padrão.</para>

      <para><emphasis role="bold">storageSize</emphasis>: Conforme indicado no
      exemplo, define a capacidade do volume.</para>

      <sect3 id="YAML_StorageCategory">
        <title>Categoria de Armazenamento</title>

        <para>A categoria de armazenamento (Storage Category) é usada para
        indicar o tipo de dados que está sendo armazenado nesse local.
        Diferentes planos são usados para as diferentes categorias para isolar
        os diferentes tipos de dados uns dos outros, mas também porque eles
        geralmente exigem características de desempenho diferentes. Um plano
        nomeado pode armazenar apenas uma categoria de dados. As seções a
        seguir examinam as categorias de dados com suporte atualmente usadas
        em nossa implantação em contêiner.</para>

        <para><programlisting> category: data|dali|lz|dll|spill|temp  # What category of data is stored on this plane?</programlisting></para>

        <para>O próprio sistema pode gravar em qualquer plano de dados. É
        assim que a categoria de dados pode ajudar a melhorar o desempenho.
        Por exemplo, se você tiver um índice, o Roxie desejará acesso rápido
        aos dados, em vez de outros componentes.</para>

        <para>Alguns componentes podem usar apenas 1 categoria, alguns podem
        usar várias. O arquivo de valores pode conter mais de uma definição de
        plano de armazenamento para cada categoria. O primeiro plano de
        armazenamento na lista para cada categoria é usado como local padrão
        para armazenar essa categoria de dados. Essas categorias minimizam a
        exposição dos dados do avião a componentes que não precisam deles. Por
        exemplo, o componente ECLCC Server não precisa saber sobre as landing
        zones ou onde Dali armazena seus dados, portanto, ele monta apenas as
        categorias de avião necessárias.</para>
      </sect3>

      <sect3 id="YML_EphemeralStorage">
        <title>Armazenamento Temporário</title>

        <para>O armazenamento temporário (Ephemeral storage) é alocado quando
        o cluster HPCC Systems é instalado e excluído quando o chart é
        desinstalado. Isso é útil para manter os custos de nuvem baixos, mas
        pode não ser apropriado para seus dados.</para>

        <para>Em seu sistema, você deseja substituir o(s) valor(es) de estoque
        fornecido(s) pelo armazenamento apropriado para suas necessidades
        específicas. Os valores fornecidos criam volumes persistentes efêmeros
        ou temporários que são excluídos automaticamente quando o chart é
        desinstalado. Você provavelmente quer que o armazenamento seja
        persistente. Você deve personalizar o armazenamento para uma
        configuração mais adequada às suas necessidades.</para>
      </sect3>

      <sect3 id="YAML_Persist_storage">
        <title>Armazenamento Persistente</title>

        <para>O Kubernetes usa declarações de volume persistentes (pvcs) para
        fornecer acesso ao armazenamento de dados. O HPCC Systems oferece
        suporte ao armazenamento em nuvem por meio do provedor de nuvem que
        pode ser exposto por meio dessas declarações de volume
        persistentes.</para>

        <para>As Declarações de Volume Persistentes podem ser criadas
        substituindo os valores de armazenamento no chart do Helm entregue. Os
        valores no arquivo example/local/values-localfile.yaml fornecidos
        substituem as entradas correspondentes no chart de comando original da
        pilha entregue do HPCC Systems. O chart localfile cria volumes de
        armazenamento persistentes. Você pode usar o values-localfile.yaml
        diretamente (como demonstrado em documentos/tutoriais separados) ou
        pode usá-lo como base para criar seu próprio chart de
        substituição.</para>

        <para>Para definir um plano de armazenamento que utiliza um PVC, você
        deve decidir onde esses dados residirão. Você cria os diretórios de
        armazenamento, com os nomes apropriados e, em seguida, pode instalar o
        chart do Helm de arquivos locais para criar os volumes para usar a
        opção de armazenamento local, como no exemplo a seguir:</para>

        <programlisting>helm install mycluster hpcc/hpcc -f examples/local/values-localfile.yaml</programlisting>

        <para><emphasis role="bold">Nota: </emphasis>As configurações para os
        PVCs devem ser ReadWriteMany, exceto para Dali que pode ser
        ReadWriteOnce.</para>

        <para>Há vários recursos, blogs, tutoriais e até mesmo vídeos de
        desenvolvedores que fornecem detalhes passo a passo para a criação de
        volumes de armazenamento persistentes.</para>
      </sect3>

      <sect3 id="CYML_BareMEtalStorage">
        <title>Armazenamento Bare Metal</title>

        <para>Há dois aspectos no uso do armazenamento bare metal no sistema
        Kubernetes. A primeira é a entrada <emphasis>hostGroups</emphasis> na
        seção de armazenamento que fornece listas nomeadas de hosts. As
        entradas <emphasis>hostGroups</emphasis> podem assumir uma das duas
        formas. Essa é a forma mais comum e associa diretamente uma lista de
        nomes de host a um nome:</para>

        <programlisting>storage: 
  hostGroups: 
  - name:  &lt;name&gt; "The name of the host group" 
    hosts: [ "a list of host names" ] 
</programlisting>

        <para>A segunda forma permite que um grupo de hosts seja derivado de
        outro:</para>

        <programlisting>storage: 
  hostGroups: 
  - name: "The name of the host group process" 
    hostGroup: "Name of the hostgroup to create a subset of" 
    count: &lt;Number of hosts in the subset&gt; 
    offset: &lt;the first host to include in the subset&gt;  
    delta: &lt;Cycle offset to apply to the hosts&gt; 
</programlisting>

        <para>Alguns exemplos típicos com clusters bare-metal são subconjuntos
        menores do host ou os mesmos hosts, mas armazenando partes diferentes
        em nós diferentes, por exemplo:</para>

        <programlisting>storage: 
  hostGroups: 
  - name: groupABCDE # Explicit list of hosts 
    hosts: [A, B, C, D, E] 
  - name groupCDE # Subset of the group last 3 hosts 
    hostGroup: groupABCDE 
    count: 3 
    offset: 2  
  - name groupDEC # Same set of hosts, but different part-&gt;host mapping 
    hostGroup: groupCDE 
    delta: 1</programlisting>

        <para>O segundo aspecto é adicionar uma propriedade à definição do
        plano de armazenamento para indicar quais hosts estão associados a
        ela. Existem duas opções:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">hostGroup: &lt;name&gt;</emphasis> O
            nome do grupo de hosts para bare metal. O nome do hostGroup deve
            corresponder ao nome do plano de armazenamento.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">hosts:
            &lt;list-of-namesname&gt;</emphasis> Uma lista embutida de hosts.
            Principalmente útil para landing zones.</para>
          </listitem>
        </itemizedlist>

        <para>Por Exemplo:</para>

        <programlisting>storage: 
  planes: 
  - name: demoOne 
    category: data 
    prefix: "/home/demo/temp" 
    hostGroup: groupABCD # The name of the hostGroup 
  - name: myDropZone 
    category: lz 
    prefix: "/home/demo/mydropzone" 
    hosts: [ 'mylandingzone.com' ] # Inline reference to an external host. </programlisting>
      </sect3>

      <sect3 id="CV_RemoteStorage" role="brk">
        <title>Armazenamento Remoto</title>

        <para>Você pode configurar sua implantação em nuvem do HPCC Systems
        para acessar arquivos lógicos de outros ambientes remotos. Você
        configura esse armazenamento remoto adicionando uma seção "remota" ao
        seu gráfico de helm.</para>

        <para>A seção <emphasis>storage.remote</emphasis> é uma lista de
        ambientes remotos nomeados que definem a URL do serviço remoto e uma
        seção que mapeia os nomes dos planos remotos para os nomes dos planos
        locais. Os planos locais referenciados são planos especiais com a
        categoria 'remoto'. Eles são somente leitura e só estão disponíveis
        para os motores que podem lê-los.</para>

        <para>Por exemplo:</para>

        <programlisting>storage:
  planes:
...
  - name: hpcc2-stddata
    pvc: hpcc2-stddata-pv
    prefix: "/var/lib/HPCCSystems/hpcc2/hpcc-stddata"
    category: remote
  - name: hpcc2-fastdata
    pvc: hpcc2-fastdata-pv
    prefix: "/var/lib/HPCCSystems/hpcc2/hpcc-fastdata"
    category: remote
  remote:
  - name: hpcc2
    service: http://20.102.22.31:8010
    planes:
    - remote: data
      local: hpcc2-stddata
    - remote: fastdata
      local: hpcc2-fastdata</programlisting>

        <para>Este exemplo define um alvo remoto chamado "hpcc2" cuja URL do
        serviço DFS é http://20.102.22.31:8010 e cujo plano local é
        "hpcc2data". O plano local deve ser definido de modo que compartilhe o
        mesmo armazenamento que o ambiente remoto. Espera-se que isso seja
        feito através de um PVC que foi pré-configurado para usar o mesmo
        armazenamento.</para>

        <para>Para acessar o arquivo lógico em ECL use o seguinte
        formato:</para>

        <programlisting>ds := DATASET('~remote::hpcc2::somescope1::somelfn1', rec);</programlisting>
      </sect3>

      <sect3>
        <title>Armazenamento Preferêncial</title>

        <para>A opção <emphasis>preferredReadPlanes</emphasis> está disponível
        para cada tipo de cluster - hThor, Thor e Roxie.</para>

        <para>Esta opção é significativa apenas para arquivos lógicos que
        residem em múltiplos planos de armazenamento. Quando especificado, a
        plataforma HPCC Systems buscará ler arquivos do(s) plano(s)
        preferido(s) se um arquivo residir neles. Estes planos preferenciais
        devem existir e ser definidos em <emphasis>
        storage.planes</emphasis>.</para>

        <para>O próximo trecho é um exemplo de um cluster Thor configurado com
        a opção preferredDataReadPlanes.</para>

        <programlisting>thor: 
- name: thor 
  prefix: thor 
  numWorkers: 2
  maxJobs: 4
  maxGraphs: 3 
  preferredDataReadPlanes: 
  - data-copy
  - indexdata-copy
</programlisting>

        <para>No exemplo acima, ao executar uma consulta que lê um arquivo que
        reside tanto em "data" quanto em "data-copy" (nessa ordem),
        normalmente leria a primeira cópia em "data". Com
        <emphasis>preferredDataReadPlanes</emphasis> especificado, se esse
        arquivo também residir em "data-copy", Thor lerá essa cópia.</para>

        <para>Isso pode ser útil quando existem múltiplas cópias de arquivos
        em diferentes planos com características distintas que podem impactar
        o desempenho.</para>
      </sect3>
    </sect2>

    <sect2 id="StorageItems_HPCC_Systems_Coomponents" role="brk">
      <title>Itens de armazenamento para componentes de HPCC Systems</title>

      <sect3 id="YML-DOC_GenData-Storage">
        <title>Armazenamento geral de dados</title>

        <para>Os arquivos de dados gerais gerados pelo HPCC são armazenados em
        dados. Para Thor, os custos de armazenamento de dados provavelmente
        podem ser significativos. A velocidade de acesso sequencial é
        importante, mas o acesso aleatório é muito menos importante. Para
        ROXIE, a velocidade de acesso aleatório provavelmente será mais
        importante.</para>
      </sect3>

      <sect3>
        <title>LZ</title>

        <para>LZ ou lz, utilizados para dados da landing zone. É aqui que
        colocamos os dados brutos que chegam ao sistema. Uma landing zone onde
        usuários externos podem ler e gravar arquivos. O HPCC Systems podem
        importar ou exportar arquivos para uma landing zone. Normalmente, o
        desempenho é um problema menor, pode ser armazenamento de bucket
        blob/s3, acessado diretamente ou por meio de uma montagem NFS.</para>
      </sect3>

      <sect3>
        <title>dali</title>

        <para>A localização do repositório de metadados dali, que precisa dar
        suporte ao acesso aleatório rápido.</para>
      </sect3>

      <sect3>
        <title>dll</title>

        <para>Onde as consultas ECL compiladas são armazenadas. O
        armazenamento precisa permitir que objetos compartilhados sejam
        carregados diretamente a partir dele de forma eficiente.</para>

        <para>Se você quiser dados Dali e dll no mesmo plano, é possível usar
        o mesmo prefixo para ambas as propriedades do subcaminho. Ambos
        usariam o mesmo prefixo, mas deveriam ter subcaminhos
        diferentes.</para>
      </sect3>

      <sect3>
        <title>sasha</title>

        <para>Este é o local onde as workunitss são arquivadas, etc., são
        armazenadas e normalmente é menos crítico de velocidade, exigindo
        menores custos de armazenamento.</para>
      </sect3>

      <sect3>
        <title>spill</title>

        <para>Uma categoria opcional na qual os arquivos spill são gravados.
        Os discos NVMe locais são potencialmente uma boa opção para
        isso.</para>
      </sect3>

      <sect3>
        <title>temp</title>

        <para>Uma categoria opcional onde os arquivos temporários podem ser
        gravados.</para>
      </sect3>
    </sect2>

    <sect2 id="Egress-HYAML" role="brk">
      <title>Egresso</title>

      <para>A fim de permitir que os clusters sejam trancados de forma segura,
      mas ainda permitir o acesso aos serviços de que precisam, existe um
      mecanismo de saída. O egresso fornece um mecanismo similar ao ingresso,
      sendo capaz de definir quais endpoints e portas os componentes têm
      permissão para se conectar.</para>

      <para>A maioria dos componentes do HPCC Systems possui suas próprias
      políticas de rede geradas automaticamente. As políticas de rede geradas
      tipicamente trabalham para limitar o ingresso e egresso à comunicação
      inter-componente, ou apenas ao ingresso de serviço externo
      esperado.</para>

      <para>Por exemplo, em um sistema implantado por padrão com políticas de
      rede em vigor, uma consulta em execução (em hThor, Thor ou Roxie) não
      será capaz de se conectar com um serviço de terceiros, como um serviço
      LDAP ou pilha de log.</para>

      <para>Na configuração padrão, qualquer pod com acesso à API do Kube
      também terá acesso a qualquer endpoint externo. Isso ocorre porque uma
      <emphasis>NetworkPolicy</emphasis> é gerada para acesso de egresso para
      componentes que precisam de acesso à API do Kube. No entanto,
      <emphasis>global.egress.kubeApiCidr</emphasis> e
      <emphasis>global.egress.kubeApiPort</emphasis> no values.yaml devem ser
      configurados em um sistema seguro para restringir esse acesso de
      egresso, de modo que ele apenas exponha acesso de egresso para o
      endpoint da API do Kube.</para>

      <para>Adicionamos um mecanismo semelhante às definições de visibilidade,
      que permite seções de saída nomeadas, que então podem ser referenciadas
      por componente.</para>

      <para>Por exemplo:</para>

      <programlisting>global:
  egress:
    engineEgress:
    - to:
        - ipBlock:
            cidr: 201.13.21.0/24
        - ipBlock:
            cidr: 142.250.187.0/24
      ports:
        - protocol: TCP
          port: 6789
        - protocol: TCP
          port: 7890
...

thor:
...
  egress: engineEgress</programlisting>

      <para>Note que o nome 'engineEgress' é um nome arbitrário, qualquer nome
      pode ser escolhido, e qualquer número dessas seções de egresso nomeadas
      pode ser definido.</para>

      <para>Para mais informações, por favor, veja a seção
      <emphasis>egress:</emphasis> no arquivo YAML padrão fornecido pelo HPCC
      Systems. O arquivo <emphasis>values.yaml</emphasis> pode ser encontrado
      no diretório helm/hpcc/ no repositório github do HPCC Systems:</para>

      <para><ulink
      url="https://github.com/hpcc-systems/HPCC-Platform">https://github.com/hpcc-systems/HPCC-Platform</ulink></para>
    </sect2>

    <sect2>
      <title>Valores de segurança</title>

      <para>Esta seção examinará as seções de <emphasis>values.yaml</emphasis>
      que tratam dos componentes de segurança do sistema.</para>

      <sect3>
        <title>Certificados</title>

        <para>A seção de certificados pode ser usada para permitir que o
        cert-manager gere certificados TLS para cada componente na implantação
        do HPCC Systems.</para>

        <programlisting>certificates:
  enabled: false
  issuers:
    local:
      name: hpcc-local-issuer</programlisting>

        <para>No arquivo yaml entregue, os certificados não estão habilitados,
        conforme ilustrado acima. Você deve primeiro instalar o cert-manager
        para usar esse recurso.</para>
      </sect3>

      <sect3 id="ValYAML_Secrets">
        <title>Secrets</title>

        <para>A seção Secrets contém um conjunto de categorias, cada uma
        contendo uma lista de secrets. A seção Secrects é o local onde é
        possível obter informações no sistema, caso não as queira na fonte.
        Tal como código embarcado, você pode definir isso nas seções de sign
        do código. Se você tiver informações que não deseja publicar, mas
        precisa executá-las, poderá usar secrets. Existe uma categoria chamada
        "eclUser", onde você colocaria o conteúdo que deseja ler diretamente
        do código ECL. Outras categorias de secrects, incluindo a categoria
        "ecl", são lidas internamente pelos componentes do sistema e não
        expostas diretamente ao código ECL.</para>
      </sect3>

      <sect3>
        <title>Vaults</title>

        <para>Vaults é outra maneira de esconder informações. A seção de
        Vaults espelha a seção Secrects, mas aproveita
        <emphasis>HashiCorpVault</emphasis> para o armazenamento dos secrects.
        Existe uma categoria para vaults chamada "eclUser". A intenção da
        categoria de vault eclUser é ser legível diretamente do código ECL.
        Adicione apenas configurações de vault à categoria "eclUser" que você
        deseja que os usuários ECL possam acessar. Outras categorias de vault,
        incluindo a categoria "ecl", são lidas internamente pelos componentes
        do sistema e não expostas diretamente ao código ECL.</para>
      </sect3>

      <sect3 id="CV_CrossOriginRes">
        <title>Manipulação de Recursos de Origem Cruzada</title>

        <para>O compartilhamento de recursos de origem cruzada (CORS) é um
        mecanismo para integrar aplicativos em diferentes domínios. CORS
        define como as aplicações web do cliente em um domínio podem interagir
        com recursos em outro domínio. Você pode configurar as configurações
        de suporte ao CORS na seção ESP do arquivo values.yaml, conforme
        ilustrado abaixo:</para>

        <programlisting>esp:
- name: eclwatch
  application: eclwatch
  auth: ldap
  replicas: 1
  # The following 'corsAllowed' section is used to configure CORS support
  #   origin - the origin to support CORS requests from
  #   headers - the headers to allow for the given origin via CORS
  #   methods - the HTTP methods to allow for the given origin via CORS
  #
  corsAllowed:
  # origin starting with https will only allow https CORS
  - origin: https://*.example2.com
    headers:
    - "X-Custom-Header"
    methods:
    - "GET"
  # origin starting with http will allow http or https CORS
  - origin: http://www.example.com
    headers:
    - "*"
    methods:
    - "GET"
    - "POST" </programlisting>
      </sect3>
    </sect2>

    <sect2>
      <title>Visibilidades</title>

      <para>A seção Visibilidades pode ser usada para definir rótulos,
      anotações e tipos de serviço para qualquer serviço com a visibilidade
      especificada.</para>
    </sect2>

    <sect2 id="Replicas-Resources-YAML">
      <title>Réplicas e Recursos</title>

      <para>Outros valores dignos de nota nos charts que têm relação com a
      instalação e configuração do HPCC Systems.</para>

      <sect3 id="REPLICAS_">
        <title>Réplicas</title>

        <para>replicas: define quantos nós de réplica surgem, quantos pods são
        executados para equilibrar uma carga. Para ilustrar, se você tiver um
        Roxie de 1 via e definir réplicas como 2, você terá 2 Roxies de 1
        via.</para>
      </sect3>

      <sect3 id="RESOURCES_ValuesYAML">
        <title>Recursos</title>

        <para>A maioria dos componentes tem uma seção de recursos que define
        quantos recursos são atribuídos a esse componente. Nos arquivos de
        valores entregues em estoque, as seções recursos: existem apenas para
        fins ilustrativos e são comentadas. Qualquer implantação em nuvem que
        venha a desempenhar qualquer função não trivial, esses valores devem
        ser definidos adequadamente com recursos adequados para cada
        componente, da mesma forma que você alocaria recursos físicos
        adequados em um data center. Os recursos devem ser configurados de
        acordo com os requisitos específicos do sistema e o ambiente em que
        você os executaria. A definição inadequada de recursos pode resultar
        em falta de memória e/ou remoção do Kubernetes, pois o sistema pode
        usar quantidades ilimitadas de recursos, como memória e os nós ficarão
        sobrecarregados, momento em que o Kubernetes começará a despejar os
        pods. Portanto, se sua implantação estiver vendo despejos frequentes,
        convém ajustar sua alocação de recursos.</para>

        <para><programlisting>  #resources:
  #  cpu: "1"
  #  memory: "4G"
</programlisting>Cada componente deve ter uma entrada de recursos, mas alguns
        componentes, como Thor, possuem vários recursos. Os componentes
        manager, worker e eclagent têm requisitos de recursos
        diferentes.</para>
      </sect3>
    </sect2>

    <sect2 id="ENV_values_yaml">
      <title>Valores do Ambiente</title>

      <para>Você pode definir variáveis de ambiente em um arquivo YAML. Os
      valores do ambiente são definidos na seção
      <emphasis>global.env</emphasis> do arquivo values.yaml fornecido
      peloHPCC Systems. Esses valores são especificados como uma lista de
      pares de nome-valor, conforme ilustrado abaixo.</para>

      <programlisting>global:
  env:
  - name: SMTPserver 
    value: mysmtpserver</programlisting>

      <para>A seção global.env do arquivo values.yaml fornecido adiciona
      variáveis de ambiente padrão para todos os componentes. Você também pode
      especificar variáveis de ambiente para os componentes individuais.
      Consulte o esquema para definir esse valor para componentes
      individuais.</para>

      <para>Para adicionar valores de ambiente, você pode inseri-los no seu
      arquivo YAML de configuração de personalização quando fizer o deploy do
      seu HPCC Systems contêinerizado.</para>

      <sect3>
        <title>Variáveis de Ambiente para Sistemas Conteinerizados</title>

        <para>Existem várias configurações em environment.conf para sistemas
        bare-metal. Embora muitas configurações de environment.conf não sejam
        válidas para contêineres, algumas podem ser úteis. Em uma implantação
        na nuvem, essas configurações são herdadas de variáveis de ambiente.
        Essas variáveis de ambiente são configuráveis usando o values yaml
        globalmente ou no nível do componente.</para>

        <para>Algumas dessas variáveis estão disponíveis para implantações em
        contêineres e na nuvem e podem ser definidas usando o gráfico do Helm.
        Os seguintes valores de environment.conf para sistemas bare-metal têm
        valores equivalentes que podem ser definidos para instâncias
        conteinerizadas.</para>

        <para><informaltable>
            <tgroup cols="2">
              <tbody>
                <row>
                  <entry><emphasis role="bold">Environment.conf
                  Value</emphasis></entry>

                  <entry><emphasis role="bold">Helm Environment
                  Variable</emphasis></entry>
                </row>

                <row>
                  <entry>skipPythonCleanup</entry>

                  <entry>SKIP_PYTHON_CLEANUP</entry>
                </row>

                <row>
                  <entry>jvmlibpath</entry>

                  <entry>JAVA_LIBRARY_PATH</entry>
                </row>

                <row>
                  <entry>jvmoptions</entry>

                  <entry>JVM_OPTIONS</entry>
                </row>

                <row>
                  <entry>classpath</entry>

                  <entry>CLASSPATH</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable></para>

        <para>O seguinte exemplo define a variável de ambiente para pular a
        limpeza do Python no componente Thor:</para>

        <para><programlisting>thor: 
  env: 
  - name: SKIP_PYTHON_CLEANUP 
    value: true</programlisting></para>
      </sect3>
    </sect2>

    <sect2 id="CNT_IndexBuildPlane">
      <title>Index Build Plane</title>

      <para>Defina o valor <emphasis>indexBuildPlane</emphasis> como uma opção
      de gráfico de helm para permitir que arquivos de índice sejam escritos
      por padrão em um plano de dados diferente. Ao contrário de arquivos
      planos, os arquivos de índices têm requisitos diferentes. Os arquivos de
      índice se beneficiam de armazenamento de acesso aleatório rápido.
      Normalmente, arquivos planos e arquivos de índices são gravados nos
      planos de dados padrão definidos. Usando esta opção, você pode definir
      que os arquivos de índice são construídos em um plano de dados separado
      de outros arquivos comuns. Este valor de gráfico pode ser fornecido em
      um componente ou nível global.</para>

      <para>Por exemplo, adicionando o valor a um nível global em
      globlal.storage:</para>

      <programlisting>global:
  storage:
    indexBuildPlane: myindexplane</programlisting>

      <para>Optionally, you could add it at the component level, as
      follows:</para>

      <programlisting>thor:
- name: thor
  prefix: thor
  numWorkers: 2
  maxJobs: 4
  maxGraphs: 2
  indexBuildPlane: myindexplane</programlisting>

      <para>Opcionalmente, você poderia adicioná-lo no nível do componente,
      conforme segue:</para>
    </sect2>
  </sect1>

  <sect1 id="CV_Pods-Nodes">
    <title>Pods e Nós</title>

    <para>Uma das principais características do Kubernetes é sua capacidade de
    agendar pods nos nós do cluster. Um pod é a unidade mais pequena e simples
    no ambiente Kubernetes que você pode criar ou implantar. Um nó é uma
    máquina "trabalhadora", física ou virtual, no Kubernetes.</para>

    <para>A tarefa de agendar pods para nós específicos no cluster é
    gerenciada pelo kube-scheduler. O comportamento padrão deste componente é
    filtrar os nós com base nas solicitações e limites de recursos de cada
    recipiente no pod criado. Os nós viáveis são então pontuados para
    encontrar o melhor candidato para a localização do pod. O agendador também
    leva em conta outros fatores, como afinidade e anti-afinidade de pods,
    marcas e tolerâncias, restrições de dispersão de topologia de pod, e os
    rótulos de seleção de nó. O agendador pode ser configurado para usar esses
    diferentes algoritmos e políticas para otimizar a colocação do pod de
    acordo com as necessidades do seu cluster.</para>

    <para>Você pode implantar esses valores usando o arquivo values.yaml ou
    pode colocá-los em um arquivo e fazer com que o Kubernetes leia os valores
    do arquivo fornecido. Veja a seção acima <emphasis>Técnicas de
    personalização</emphasis> para mais informações sobre como personalizar
    sua implantação.</para>

    <sect2 id="YAML_FileStruct_Placement">
      <title>Placements</title>

      <para>O termo "Placements" é usado pelo HPCC Systems para descrever o
      que o Kubernetes se refere como agendador ou agendamento/atribuição.
      Para evitar confusão com a terminologia específica do agendador do HPCC
      Systems e ECL, usamos o termo "Placements" para nos referir ao
      agendamento do Kubernetes. Os Placements são um valor na configuração do
      HPCC Systems que reside em um nível acima de entidades como
      nodeSelector, Tolerância, Afinidade e Anti-Afinidade, e
      TopologySpreadConstraints.</para>

      <para>O Placement é responsável por encontrar o melhor nó para um pod.
      Na maioria das vezes, a colocação é gerenciada automaticamente pelo
      Kubernetes. Você pode restringir um Pod para que ele possa ser executado
      apenas em um conjunto específico de Nós.</para>

      <para>Placements, então, seriam usadas para garantir que pods ou jobs
      que requerem nós com características específicas sejam alocados nestes
      nós.</para>

      <para>Por exemplo, um cluster Thor poderia ser direcionado para machine
      learning usando nós com GPU. Outro job pode precisar de nós com uma
      quantidade maior de memória ou outro para mais CPU.</para>

      <para>Usando placements, você pode configurar o agendador do Kubernetes
      para usar uma lista de "pods" para aplicar configurações aos
      pods.</para>

      <para>Por exemplo:</para>

      <programlisting> placements:
   - pods: [list]
     placement:
       &lt;supported configurations&gt;</programlisting>

      <sect3 id="PlacementScope">
        <title>Escope do Placement</title>

        <para>Use padrões de pods para aplicar o escopo para os
        placements.</para>

        <para>Os pods: [list] podem conter uma variedade de itens.</para>

        <informaltable colsep="1" frame="all" rowsep="1">
          <tgroup cols="2">
            <colspec colwidth="125.55pt"/>

            <colspec/>

            <tbody>
              <row>
                <entry>Tipo: &lt;component&gt;</entry>

                <entry>Cobre todos os pods/trabalhos sob este tipo de
                componente. Isso é comumente utilizado para os componentes do
                HPCC Systems. Por exemplo, o <emphasis>type:thor
                </emphasis>que se aplicará a qualquer componente do tipo Thor;
                thoragent, thormanager, thoragent e thorworker, etc.</entry>
              </row>

              <row>
                <entry>Destino: &lt;name&gt;</entry>

                <entry>O campo "name" de cada componente, uso típico para
                componentes do HPCC Systems refere-se ao nome do cluster. Por
                exemplo,<emphasis> Roxie: -name: roxie</emphasis> que será o
                destinoalvo "Roxie" (cluster). Você também pode definir vários
                alvos, cada um com um nome único, como "roxie", "roxie2",
                "roxie-web", etc.</entry>
              </row>

              <row>
                <entry>Pod: &lt;name&gt;</entry>

                <entry>Este é o nome dos metadados de "Implantação" a partir
                do nome do item de array de um tipo. Por exemplo, "eclwatch-",
                "mydali-", "thor-thoragent", o uso de uma expressão regular é
                preferido, pois o Kubernetes usará o nome dos metadados como
                prefixo e gerará dinamicamente o nome do pod, como,
                eclwatch-7f4dd4dd44cb-c0w3x.</entry>
              </row>

              <row>
                <entry>Nome do Job:</entry>

                <entry>O nome do job é geralmente também uma expressão
                regular, já que o nome do job é gerado dinamicamente. Por
                exemplo, um job de compilação compile-54eB67e567e, pode usar
                "compile-" ou "compile-.*" ou "^compile-.*$"</entry>
              </row>

              <row>
                <entry>All:</entry>

                <entry>Aplica para todos os componentes do HPCC Systems. O
                padrão de implantação dos placements para pods é [all]</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable>

        <para>Independentemente da ordem em que os placements aparecem na
        configuração, eles serão processadas na seguinte ordem: "all", "type",
        "target", e então "pod"/"job".</para>

        <sect4>
          <title>Combinações mistas</title>

          <para>NodeSelector, taints e tolerations, e outros valores pode ser
          colocado nos mesmos pods: [list] ambos por zona e por nós no Azure
          <programlisting>placements:
- pods: ["eclwatch","roxie-workunit","^compile-.*$","mydali"]
  placement:
    nodeSelector:
      name: npone</programlisting></para>
        </sect4>
      </sect3>
    </sect2>

    <sect2 id="S2NodeSelection">
      <title>Seleção de Nós</title>

      <para>Em um ambiente de contêineres Kubernetes, existem várias maneiras
      de agendar seus nós. As abordagens recomendadas utilizam seletores de
      rótulos para facilitar a seleção. Geralmente, você pode não precisar
      definir tais restrições, pois o agendador normalmente faz um
      posicionamento razoavelmente aceitável automaticamente. No entanto, em
      algumas implantações, você pode querer ter mais controle sobre pods
      específicos.</para>

      <para>O Kubernetes utiliza os seguintes métodos para escolher onde
      agendar os pods:</para>

      <para><itemizedlist>
          <listitem>
            <para>campo nodeSelector correspondendo aos rótulos dos nós</para>
          </listitem>

          <listitem>
            <para>Afinidade e anti-afinidade</para>
          </listitem>

          <listitem>
            <para>Taints and Tolerâncias</para>
          </listitem>

          <listitem>
            <para>campo nodeName</para>
          </listitem>

          <listitem>
            <para>restrições de dispersão topológica de pods</para>
          </listitem>
        </itemizedlist></para>

      <sect3 id="CV_NodeLabels">
        <title>Rótulo dos Nós</title>

        <para>Os nós do Kubernetes possuem rótulos. Kubernetes possui um
        conjunto padrão de rótulos utilizados para nós em um cluster. Você
        também pode anexar rótulos manualmente, o que é recomendado, já que o
        valor desses rótulos é específico do provedor de nuvem e não é
        garantido ser confiável.</para>

        <para>Adicionar rótulos aos nós permite agendar pods para nós ou
        grupos de nós específicos. Você pode então usar essa funcionalidade
        para garantir que pods específicos sejam executados apenas em nós com
        determinadas propriedades.</para>
      </sect3>

      <sect3 id="CV_nodeSelector">
        <title>O nodeSelector</title>

        <para>O nodeSelector é um campo na especificação do Pod que permite
        especificar um conjunto de rótulos de nó que devem estar presentes no
        nó de destino para que o Pod seja agendado lá. É a forma mais simples
        de restrição de seleção de nó. Ele seleciona nós com base nos rótulos,
        mas possui algumas limitações. Suporta apenas uma chave de rótulo e um
        valor de rótulo. Se você desejar corresponder a vários rótulos ou usar
        expressões mais complexas, precisa usar a Afinidade de Nó (node
        Affinity)</para>

        <para>Adicione o campo nodeSelector à especificação do seu pod e
        especifique os rótulos de nó que você deseja que o nó de destino
        tenha. Você deve ter os rótulos de nó definidos no trabalho (job) e no
        pod. Em seguida, você precisa especificar cada grupo de nós que o
        rótulo de nó deve usar. O Kubernetes agendará o pod apenas nos nós que
        possuem os rótulos que você especificar.</para>

        <para>O exemplo a seguir mostra o nodeSelector colocado na lista de
        pods. Este exemplo agenda todos os componentes do HPCC para usar o
        grupo de nós com o rótulo "group: hpcc".</para>

        <para><programlisting> placements:
   - pods: ["all"]
     placement:
       nodeSelector:
         group: "hpcc"</programlisting></para>

        <para><emphasis role="bold">Nota:</emphasis> O rótulo group:hpcc
        corresponde ao rótulo do grupo de nós hpcc.</para>

        <para>Este próximo exemplo mostra como configurar um grupo de nós para
        evitar o agendamento de um componente Dali neste grupo de nós rotulado
        com a chave spot: com o valor false. Como esse tipo de nó nem sempre
        está disponível e pode ser revogado, você não deseja usar o grupo de
        nós spot para componentes Dali. Este é um exemplo de como configurar
        um tipo específico (Dali) de componente do HPCC Systems para não usar
        um determinado grupo de nós.</para>

        <para><programlisting> placements:
   - pods: ["type:dali"]
     placement:
       nodeSelector:
         spot: "false"</programlisting></para>

        <para>Ao usar nodeSelector, múltiplos nodeSelectors podem ser
        aplicados. Se chaves duplicadas forem definidas, apenas a última
        prevalecerá.</para>
      </sect3>

      <sect3 id="CV-TAINTS_TOLERATIONS">
        <title>Taints e Tolerâncias</title>

        <para>Taints e Tolerâncias são tipos de restrições de nó do
        Kubernetes, também referidas como Afinidade de Nó. Apenas uma
        afinidade pode ser aplicada a um pod. Se um pod corresponder a várias
        listas de posicionamento ('pods'), então apenas a última definição de
        afinidade será aplicada.</para>

        <para>Taints e tolerâncias trabalham juntos para garantir que os pods
        não sejam agendados em nós inadequados. Tolerâncias são aplicadas aos
        pods e permitem (mas não exigem) que os pods sejam agendados em nós
        com taints correspondentes. Taints funcionam de forma oposta - elas
        permitem que um nó repila um conjunto de pods. Uma forma de implantar
        usando taints é configurar para repelir todos os nós exceto um
        específico. Então, esse pod pode ser agendado em outro nó que seja
        tolerante.</para>

        <para>Por exemplo, os jobs do Thor devem estar todos no tipo
        apropriado de VM. Se um trabalho grande do Thor aparecer, então o
        nível de taints repele qualquer pod que tente ser agendado em um nó
        que não atenda aos requisitos.</para>

        <para>Para mais informações e exemplos de nossos Taints, Tolerâncias,
        e Placements, por favor verifique nossa documentação de
        desenvolvimento:</para>

        <para><ulink
        url="https://github.com/hpcc-systems/HPCC-Platform/blob/master/helm/hpcc/docs/placements.md">https://github.com/hpcc-systems/HPCC-Platform/blob/master/helm/hpcc/docs/placements.md</ulink></para>

        <sect4 id="CSP_TaintsandTolerationsExamples">
          <title>Exemplos de Taints e tolerâncias</title>

          <para>Os exemplos a seguir ilustram como alguns taints e tolerâncias
          podem ser aplicados.</para>

          <para>O Kubernetes pode agendar um pod para qualquer grupo de nós
          sem um taint. Nos exemplos seguintes, o Kubernetes só pode agendar
          os dois componentes para os grupos de nós com esses rótulos exatos,
          group e gpu.</para>

          <para><programlisting> placements:
   - pods: ["all"]
     tolerations:
       key: "group"
         operator: "Equal" 
           value: "hpcc"
             effect: "NoSchedule"         </programlisting></para>

          <programlisting>placements:
   - pods: ["type:thor"] 
     tolerations: 
        key: "gpu" 
        operator: "Equal" 
        value: "true" 
        effect: "NoSchedule" </programlisting>

          <para>Também é possível usar múltiplas tolerations. O exemplo a
          seguir possui duas tolerations, group e gpu.</para>

          <programlisting>#The settings will be applied to all thor pods/jobs and myeclccserver pod and job
- pods: ["thorworker-", "thor-thoragent", "thormanager-","thor-eclagent","hthor-"]
  placement:
    nodeSelector:
      app: tf-gpu
    tolerations:
    - key: "group"
      operator: "Equal"
      value: "hpcc"
      effect: "NoSchedule" 
    - key: "gpu"
      operator: "Equal" 
      value: "true"
      effect: "NoSchedule"
</programlisting>

          <para>Neste exemplo, o nodeSelector está impedindo o agendador do
          Kubernetes de implantar qualquer pod neste grupo de nós. Sem taints,
          o agendador poderia implantar qualquer pod no grupo de nós. Ao
          utilizar o nodeSelector, o taint forçará o pod a ser implantado
          apenas nos nós que correspondem a esse rótulo de nó. Existem, então,
          duas restrições neste exemplo: uma do grupo de nós e outra do
          pod.</para>
        </sect4>
      </sect3>

      <sect3 id="CV_TopologySpreadConstraints">
        <title>Restrições de Dispersão Topológica</title>

        <para>Você pode usar restrições de dispersão topológica para controlar
        como os pods são distribuídos em seu cluster entre domínios de falha,
        como regiões, zonas, nós e outros domínios de topologia definidos pelo
        usuário. Isso pode ajudar a alcançar alta disponibilidade e utilização
        eficiente de recursos. Você pode definir restrições de dispersão
        topológica ao nível do cluster como padrão, ou configurá-las para
        cargas de trabalho individuais. As restrições de dispersão topológica
        <emphasis role="bold">topologySpreadConstraints</emphasis> requerem
        Kubernetes v1.19 ou superior.</para>

        <para>Para mais informações:</para>

        <para><ulink
        url="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/">https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/</ulink>
        and</para>

        <para><ulink
        url="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/">https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</ulink></para>

        <para>Usando o exemplo de "topologySpreadConstraints", há dois grupos
        de nós que têm "hpcc=nodepool1" e "hpcc=nodepool2", respectivamente.
        Os pods do Roxie serão agendados de forma equitativa nos dois grupos
        de nós.</para>

        <para>Após a implantação, você pode verificar emitindo o seguinte
        comando:</para>

        <programlisting>kubectl get pod -o wide | grep roxie</programlisting>

        <para>o código dos placements:</para>

        <programlisting>- pods: ["type:roxie"]
  placement:
    topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: hpcc
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          roxie-cluster: "roxie"</programlisting>
      </sect3>

      <sect3 id="CV_Affinity-AntiAffinity">
        <title>Afinidade e Anti-Afinidade</title>

        <para>A afinidade e anti-afinidade ampliam os tipos de restrições que
        você pode definir. As regras de afinidade e anti-afinidade ainda são
        baseadas nos rótulos. Além dos rótulos, elas fornecem regras que
        orientam o agendador do Kubernetes sobre onde colocar pods com base em
        critérios específicos. A linguagem de afinidade/anti-afinidade é mais
        expressiva do que simples rótulos e oferece mais controle sobre a
        lógica de seleção.</para>

        <para>Existem dois tipos principais de afinidade: Afinidade de Nó
        (Node Affinity) e Afinidade de Pod (Pod Affinity).</para>

        <sect4 id="CS-Node-Affiniy">
          <title>Afinidade de Nó</title>

          <para>A afinidade de nó (Node Affinity) é semelhante ao conceito de
          nodeSelector, que permite restringir em quais nós seu pod pode ser
          agendado com base nos rótulos dos nós. Ela é usada para restringir
          os nós que podem receber um pod, combinando os rótulos desses nós. A
          afinidade de nó e anti-afinidade só podem ser usadas para definir
          afinidades positivas que atraem pods para o nó.</para>

          <para>Não há verificação de esquema para o conteúdo da afinidade.
          Apenas uma afinidade pode ser aplicada a um pod ou job. Se um
          pod/job corresponder a várias listas de posicionamento de pods,
          então apenas a última definição de afinidade será aplicada.</para>

          <para>Para maiores informações, veja <ulink
          url="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/</ulink></para>

          <para>Existem dois tipos de afinidade de nós:</para>

          <para><emphasis>requiredDuringSchedulingIgnoredDuringExecution:</emphasis>
          O agendador não pode agendar o pod a menos que esta regra seja
          cumprida. Essa função é semelhante ao nodeSelector, mas com uma
          sintaxe mais expressiva.</para>

          <para><emphasis>preferredDuringSchedulingIgnoredDuringExecution</emphasis>:
          O agendador tenta encontrar um nó que atenda à regra. Se um nó
          correspondente não estiver disponível, o agendador ainda assim
          agenda o pod.</para>

          <para>Você pode especificar afinidades de nó usando o campo
          <emphasis>spec.affinity.nodeAffinity</emphasis> na especificação do
          seu pod.</para>
        </sect4>

        <sect4>
          <title>Afinidade de Pod</title>

          <para>A afinidade de pod ou Inter-Pod Affinity é usada para
          restringir os nós que podem receber um pod combinando os rótulos dos
          pods existentes já em execução nesses nós. A afinidade de pod e
          anti-afinidade pode ser tanto uma afinidade atrativa quanto uma
          anti-afinidade repelente.</para>

          <para>A afinidade inter-pod funciona de maneira muito semelhante à
          afinidade de nó, mas apresenta algumas diferenças importantes. Os
          modos "hard" e "soft" são indicados usando os mesmos campos
          <emphasis>requiredDuringSchedulingIgnoredDuringExecution</emphasis>
          e
          <emphasis>preferredDuringSchedulingIgnoredDuringExecution</emphasis>.
          No entanto, esses devem estar aninhados sob os campos
          <emphasis>spec.affinity.podAffinity</emphasis> ou
          <emphasis>preferredDuringSchedulingIgnoredDuringExecution</emphasis>,
          dependendo se você deseja aumentar ou reduzir a afinidade do
          pod.</para>
        </sect4>

        <sect4>
          <title>Exemplo de Afinidade</title>

          <para>O código a seguir ilustra um exemplo de afinidade:</para>

          <programlisting>- pods: ["thorworker-.*"]
  placement:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/e2e-az-name
              operator: In
              values:
              - e2e-az1
              - e2e-az2</programlisting>

          <para>Na seção `schedulerName` a seguir, as configurações de
          "afinidade" também podem ser incluídas com esse exemplo.</para>

          <para><emphasis role="bold">Nota:</emphasis> O valor "afinidade" no
          campo "schedulerName" é suportado apenas nas versões beta do
          Kubernetes 1.20.0 e posteriores.</para>
        </sect4>
      </sect3>

      <sect3 id="CV_schedulerName">
        <title>schedulerName</title>

        <para>O campo <emphasis role="bold">schedulerName</emphasis>
        especifica o nome do agendador responsável por agendar um pod ou uma
        tarefa. No Kubernetes, você pode configurar vários agendadores com
        nomes e perfis diferentes para serem executados simultaneamente no
        cluster.</para>

        <para>Apenas um "schedulerName" pode ser aplicado a qualquer pod ou
        job.</para>

        <para>Exemplo de schedulerName:</para>

        <programlisting>- pods: ["target:roxie"]
  placement:
    schedulerName: "my-scheduler"
#The settings will be applied to all thor pods/jobs and myeclccserver pod and job
- pods: ["target:myeclccserver", "type:thor"]
  placement:
    nodeSelector:
      app: "tf-gpu"
    tolerations:
    - key: "gpu"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
</programlisting>
      </sect3>
    </sect2>
  </sect1>

  <sect1 id="MoreHelmANDYAML">
    <title>Fundamentos de Helm e YAML</title>

    <para>Esta seção destina-se a fornecer informações básicas para ajudá-lo a
    iniciar sua implantação containerizada do HPCC Systems. Existem inúmeros
    recursos disponíveis para aprender sobre Kubernetes, Helm e arquivos YAML.
    Para mais informações sobre o uso dessas ferramentas, ou para implantações
    em nuvem ou container, consulte a documentação respectiva.</para>

    <para>Na seção anterior, mencionamos os arquivos
    <emphasis>values.yaml</emphasis> e
    <emphasis>values-schema.json</emphasis>. Esta seção expande alguns desses
    conceitos e como eles podem ser aplicados ao usar a versão containerizada
    da plataforma HPCC Systems.</para>

    <sect2 id="TheValuesYamlFileStruct">
      <title>A estrutura de arquivos <emphasis>values.yaml</emphasis></title>

      <para>O arquivo <emphasis>values.yaml</emphasis> é um arquivo YAML, que
      é um formato frequentemente usado para arquivos de configuração. A
      estrutura que compõe a maior parte de um arquivo YAML é o par
      chave-valor, às vezes referido como um dicionário. A construção de par
      chave-valor consiste em uma chave que aponta para algum valor ou
      valores. Esses valores são definidos pelo esquema.</para>

      <para>Nos arquivos de configuração, a indentação usada para representar
      a estrutura do documento é bastante importante. Espaços à frente são
      significativos e o uso de tabulações não é permitido.</para>

      <para>Os arquivos YAML são compostos principalmente por dois tipos de
      elementos: dicionários e listas.</para>

      <sect3 id="YAML_Dictionary">
        <title>Dicionário</title>

        <para>Dicionários são coleções de mapeamentos chave-valor. Todas as
        chaves são sensíveis a maiúsculas e minúsculas, e a indentação também
        é crucial. Essas chaves devem ser seguidas por dois pontos (:) e um
        espaço.</para>

        <para>Por exemplo:</para>

        <programlisting>    logging: 
       detail: 80 
</programlisting>

        <para>Este é um exemplo de um dicionário para logging.</para>

        <para>Os dicionários nos arquivos de valores passados, como no arquivo
        <emphasis>myoverrides.yaml</emphasis> no exemplo abaixo, serão
        mesclados nos dicionários correspondentes nos valores existentes,
        começando pelos valores padrão do gráfico do Helm do HPCC
        entregue.</para>

        <para><programlisting>helm install myhpcc hpcc/hpcc -f myoverrides.yaml</programlisting></para>

        <para>Quaisquer valores pré-existentes em um dicionário que não sejam
        substituídos continuarão presentes no resultado mesclado. No entanto,
        você pode substituir o conteúdo de um dicionário definindo-o como
        nulo.</para>
      </sect3>

      <sect3 id="YAML_Lists">
        <title>Listas</title>

        <para>Listas são grupos de elementos que começam no mesmo nível de
        indentação, iniciando com um hífen e um espaço (-). Cada elemento da
        lista é indentado no mesmo nível e começa com um hífen e um espaço.
        Listas também podem ser aninhadas, e até mesmo ser listas de
        dicionários.</para>

        <para>Um exemplo de uma lista de dicionários, com
        placement.tolerations como uma lista aninhada.:</para>

        <para><programlisting>placements:
- pods: ["all"]
  placement:
    tolerations:
    - key: "kubernetes.azure.com/scalesetpriority"
</programlisting></para>

        <para>A entrada da lista aqui é denotada usando o hífen, que é um item
        de entrada na lista, que por sua vez é um dicionário com atributos
        aninhados. Em seguida, o próximo hífen (no mesmo nível de indentação)
        é a próxima entrada nessa lista. Uma lista pode ser uma lista de
        elementos de valor simples, ou os elementos podem ser listas ou
        dicionários.</para>
      </sect3>

      <sect3 id="sections_OfHPCCValues">
        <title>Sessões do HPCC Systems Values.yaml</title>

        <para>A primeira seção do arquivo <emphasis>values.yaml</emphasis>
        descreve valores globais. Global se aplica, em geral, a tudo.</para>

        <para><programlisting># Default values for hpcc.
global:
  # Settings in the global section apply to all HPCC components in all subcharts
</programlisting>No trecho do arquivo <emphasis>values.yaml</emphasis> do HPCC
        Systems fornecido (acima), <emphasis>global:</emphasis> é o dicionário
        de nível superior. Conforme observado nos comentários, as
        configurações na seção global se aplicam a todos os componentes do
        HPCC Systems. Note pelo nível de indentação que os outros valores
        estão aninhados nesse dicionário global.</para>

        <para>Itens definidos na seção global são compartilhados entre todos
        os componentes.</para>

        <para>Alguns exemplos de valores globais no arquivo values.yaml
        fornecido são as seções de armazenamento e segurança.</para>

        <programlisting>storage:
  planes:</programlisting>

        <para>e também</para>

        <para><programlisting>security:
  eclSecurity:
    # Possible values:
    # allow - functionality is permitted
    # deny - functionality is not permitted
    # allowSigned - functionality permitted only if code signed
    embedded: "allow"
    pipe:  "allow"
    extern: "allow"
    datafile: "allow"</programlisting>Nos exemplos acima,
        <emphasis>storage:</emphasis> e <emphasis>security:</emphasis> são
        valores globais do gráfico.</para>
      </sect3>
    </sect2>

    <sect2 id="HPCCSystems_YAML_Usage">
      <title>Utilização do HPCC Systems values.yaml</title>

      <para>O arquivo <emphasis>values.yaml</emphasis> do HPCC Systems é usado
      pelo Helm chart para controlar como o HPCC Systems é implantado. O
      <emphasis>values.yaml</emphasis> do HPCC Systems fornecido é destinado a
      ser um guia de instalação do tipo quick start, que não é apropriado para
      uso prático não trivial. Você deve personalizar sua implantação para
      algo mais adequado às suas necessidades específicas.</para>

      <para>Informações adicionais sobre implantações personalizadas são
      abordadas em seções anteriores, assim como na documentação do Kubernetes
      e Helm.</para>

      <sect3 id="merging_AND_Overrides">
        <title>Mesclagem e Sobreposição</title>

        <para>Ter vários arquivos YAML, como um para logging, outro para
        armazenamento, e ainda outro para segredos, permite uma configuração
        granular. Esses arquivos de configuração podem todos estar sob
        controle de versão, onde podem ser versionados, verificados, etc. Isso
        tem o benefício de definir/mudar apenas a área específica necessária,
        enquanto garante que áreas não modificadas permaneçam
        intocadas.</para>

        <para>A regra a ser lembrada aqui, ao aplicar vários arquivos YAML, é
        que os últimos sempre sobrescreverão os valores nos primeiros. Eles
        sempre são mesclados em sequência. Os valores são sempre mesclados na
        ordem em que são fornecidos na linha de comando do Helm.</para>

        <para>Outro ponto a considerar é que, quando há um dicionário global
        como root: e seu valor é redefinido em um segundo arquivo (como um
        dicionário), ele não será sobrescrito. Você simplesmente não pode
        sobrescrever um dicionário. Você pode redefinir um dicionário e
        defini-lo como nulo, o que efetivamente apagará o primeiro.</para>

        <para><emphasis role="bold">ATENÇÃO</emphasis>: Se você tivesse uma
        definição global (como storage.planes) e a mesclasse onde ela fosse
        redefinida, isso apagaria todas as definições nessa lista.</para>

        <para>Another means to wipe out every value in a list is to pass in an
        empty set denoted by a [ ] such as this example:</para>

        <para><programlisting>bundles: []</programlisting>Isso apagaria
        quaisquer propriedades definidas para bundles.</para>

        <sect4 id="GenerallyApplicable">
          <title>Aplicações gerais</title>

          <para>Esses itens são geralmente aplicáveis aos nossos arquivos YAML
          do HPCC Systems Helm.</para>

          <itemizedlist>
            <listitem>
              <para>Todos os nomes devem ser únicos.</para>
            </listitem>

            <listitem>
              <para>Todos os prefixos devem ser únicos.</para>
            </listitem>

            <listitem>
              <para>Serviços devem ser únicos.</para>
            </listitem>

            <listitem>
              <para>Arquivos YAML são mesclados em sequência.</para>
            </listitem>
          </itemizedlist>

          <para>Em relação aos componentes do HPCC Systems, os componentes são
          principalmente listas. Se você tiver uma lista de valores vazia
          denotada por [ ], isso invalidaria essa lista em outros
          lugares.</para>
        </sect4>
      </sect3>

      <sect3 id="Additional_YMLUSage">
        <title>Utilização adicional</title>

        <para>Os componentes do HPCC Systems são adicionados ou modificados
        passando valores de substituição. Os valores do Helm chart são
        substituídos, seja passando arquivos de substituição usando -f (para
        arquivo de substituição) ou via --set, onde você pode substituir um
        único valor. Esses valores passados são sempre mesclados na mesma
        ordem em que são fornecidos na linha de comando do Helm.</para>

        <para>Por exemplo:</para>

        <para><programlisting>helm install myhpcc hpcc/hpcc -f myoverrides.yaml</programlisting>Substitui
        quaisquer valores no <emphasis>values.yaml</emphasis> fornecido,
        passando valores definidos em
        <emphasis>myoverrides.yaml</emphasis>.</para>

        <para>Você também pode usar --set conforme o seguinte exemplo:</para>

        <programlisting>helm install myhpcc hpcc/hpcc --set storage.daliStorage.plane=dali-plane</programlisting>

        <para>Para sobrepor somente um valor em específico.</para>

        <para>É até possível combinar substituições de arquivo e de valor
        único, por exemplo:</para>

        <programlisting>helm install myhpcc hpcc/hpcc -f myoverrides.yaml --set storage.daliStorage.plane=dali-plane</programlisting>

        <para>No exemplo anterior, a flag --set substitui o valor para
        storage.daliStorage.plane (se) definido no
        <emphasis>myoverrides.yaml</emphasis>, que substituiria qualquer
        configuração do arquivo <emphasis>values.yaml</emphasis> e resultaria
        na definição de seu valor para <emphasis>dali-plane</emphasis>.</para>

        <para>Se a flag <emphasis>--set</emphasis> for usada no comando helm
        install ou helm upgrade, esses valores são simplesmente convertidos
        para YAML no lado do cliente.</para>

        <para>Você pode especificar as flags de substituição várias vezes. A
        prioridade será dada ao último (mais à direita) arquivo
        especificado.</para>
      </sect3>

      <sect3 id="YGlobalExpertsets">
        <title>Configurações Global/Expert</title>

        <para>A seção 'expert' em 'global' do values.yaml deve ser usada para
        definir configurações de baixo nível, de teste ou de desenvolvedor.
        Esta seção do helm chart é destinada a ser usada para opções
        personalizadas, de baixo nível ou de depuração, portanto, na maioria
        das implantações, deve permanecer vazia.</para>

        <para>Este é um exemplo de como a seção global/avançada pode se
        parecer:</para>

        <programlisting>global:
  expert:
    numRenameRetries: 3 
    maxConnections: 10 
    keepalive: 
      time: 200 
      interval: 75 
      probes: 9 
</programlisting>

        <para>NOTA: Alguns componentes (como o DfuServer e Thor) também
        possuem uma área de configurações 'avançadas' (veja o esquema de
        valores) que pode ser usada para configurações relevantes em uma base
        por instância de componente, em vez de defini-las globalmente.</para>

        <para>As seguintes opções estão disponíveis:</para>

        <para><variablelist>
            <varlistentry>
              <term>numRenameRetries</term>

              <listitem>
                <para>(unsigned) Se definido para um número positivo, a
                plataforma tentará novamente renomear um arquivo físico em
                caso de falha (após um curto atraso). Normalmente, isso não
                deveria ser necessário, mas em alguns sistemas de arquivos
                pode ajudar a mitigar problemas onde o arquivo acabou de ser
                fechado e não foi exposto corretamente na camada posix.</para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term>maxConnections</term>

              <listitem>
                <para>(unsigned) Esta é uma configuração do Servidor DFU. Se
                definido, irá limitar o número máximo de conexões paralelas e
                streams de partição que estarão ativas ao mesmo tempo. Por
                padrão, um trabalho DFU executará tantas conexões/streams
                ativas quanto houver partições envolvidas no spray, limitado a
                um máximo absoluto de 800. A configuração maxConnections pode
                ser usada para reduzir essa concorrência. Isso pode ser útil
                em alguns cenários onde a concorrência está causando
                congestionamento na rede e desempenho degradado.</para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term>keepalive</term>

              <listitem>
                <para>(time: sem sinal, interval: sem sinal, probes: sem
                sinal) Veja o exemplo de keepalive acima. Se definido, essas
                configurações irão substituir as configurações padrão de
                keepalive do soquete do sistema cada vez que a plataforma
                criar um soquete. Isso pode ser útil em alguns cenários se as
                conexões forem fechadas prematuramente por fatores externos
                (por exemplo, firewalls). Um exemplo disso é que instâncias do
                Azure irão fechar soquetes que permaneceram ociosos por mais
                de 4 minutos quando conectados fora de suas redes.</para>
              </listitem>
            </varlistentry>
          </variablelist></para>
      </sect3>
    </sect2>
  </sect1>
</chapter>
