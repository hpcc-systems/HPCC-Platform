import{_ as a,a as t,o,ag as n}from"./chunks/framework.Do1Zayaf.js";const d=JSON.parse('{"title":"ECL Apache Kafka Plugin","description":"","frontmatter":{},"headers":[],"relativePath":"plugins/kafka/README.md","filePath":"plugins/kafka/README.md","lastUpdated":1761843274000}'),s={name:"plugins/kafka/README.md"};function i(r,e,c,l,h,p){return o(),t("div",null,e[0]||(e[0]=[n(`<h1 id="ecl-apache-kafka-plugin" tabindex="-1">ECL Apache Kafka Plugin <a class="header-anchor" href="#ecl-apache-kafka-plugin" aria-label="Permalink to &quot;ECL Apache Kafka Plugin&quot;">​</a></h1><p>This is the ECL plugin to access <a href="https://kafka.apache.org" target="_blank" rel="noreferrer">Apache Kafka</a>, a publish-subscribe messaging system. ECL string data can be both published to and consumed from Apache Kafka brokers.</p><p>Client access is via a third-party C++ plugin, <a href="https://github.com/edenhill/librdkafka" target="_blank" rel="noreferrer">librdkafka</a>.</p><h2 id="installation-and-dependencies" tabindex="-1">Installation and Dependencies <a class="header-anchor" href="#installation-and-dependencies" aria-label="Permalink to &quot;Installation and Dependencies&quot;">​</a></h2><p><a href="https://github.com/edenhill/librdkafka" target="_blank" rel="noreferrer">librdkafka</a> is included as a git submodule in HPCC-Platform. It will be built and integrated automatically when you build the HPCC-Platform project.</p><p>The recommended method for obtaining Apache Kafka is via <a href="https://kafka.apache.org/downloads.html" target="_blank" rel="noreferrer">download</a>.</p><p>Note that Apache Kafka has its own set of dependencies, most notably <a href="https://zookeeper.apache.org" target="_blank" rel="noreferrer">zookeeper</a>. The Kafka download file does contain a Zookeeper installation, so for testing purposes you need to download only Apache Kafka and follow the excellent <a href="https://kafka.apache.org/documentation.html#quickstart" target="_blank" rel="noreferrer">instructions</a>. Those instructions will tell you how to start Zookeeper and Apache Kafka, then test your installation by creating a topic and interacting with it.</p><p><em>Note:</em> Apache Kafka version 0.8.2 or later is recommended.</p><h2 id="plugin-configuration" tabindex="-1">Plugin Configuration <a class="header-anchor" href="#plugin-configuration" aria-label="Permalink to &quot;Plugin Configuration&quot;">​</a></h2><p><em>Expand the section that matches your runtime environment (bare metal vs containerized)</em></p><p>&lt;details&gt; &lt;summary&gt;Configuring in a Bare Metal Environment&lt;/summary&gt;</p><p>The Apache Kafka plugin uses sensible default configuration values but these can be modified via configuration files.</p><p>There are two types of configurations: Global and per-topic. Some configuration parameters are applicable only to publishers (producers, in Apache Kafka&#39;s terminology), others only to consumers, and some to both. Details on the supported configuration parameters can be found on the <a href="https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md" target="_blank" rel="noreferrer">librdkafka configuration page</a>.</p><p>A configuration file is a simple text document with a series of key/value parameters, formatted like:</p><pre><code>key=value
key=value
...
key=value
</code></pre><p>A &#39;#&#39; character at the beginning of a line denotes a comment. Note that this is the only kind of comment supported in configuration files.</p><p>Whenever a new connection is created (either publisher or consumer) the plugin will scan for configuration files. All configuration files will reside in the HPCC configuration directory, which is <code>/etc/HPCCSystems</code>. The global configuration file should be named <code>kafka_global.conf</code>. Per-topic configuration files are also supported, and they can be different for a publisher or a consumer. For a publisher, the naming convention is <code>kafka_publisher_topic_\\&lt;TopicName\\&gt;.conf</code> and for a consumer it is <code>kafka_consumer_topic_\\&lt;TopicName\\&gt;.conf</code>. In both cases, <code>\\&lt;TopicName\\&gt;</code> is the name of the topic you are publishing to or consuming from.</p><p>Settings that affect the protocol used to connect to the Kafka broker (such as using SSL) should be placed only in the global configuration file, not in any per-topic configuration file.</p><p>Configuration parameters loaded from a file override those set by the plugin with one exception: the <code>metadata.broker.list</code> setting, if found in a configuration file, is ignored. Apache Kafka brokers are always set in ECL.</p><p>The following configuration parameters are set by the plugin for publishers, overriding their normal default values:</p><pre><code>queue.buffering.max.messages=1000000
compression.codec=snappy
message.send.max.retries=3
retry.backoff.ms=500
</code></pre><p>The following configuration parameters are set by the plugin for consumers, overriding their normal default values:</p><pre><code>compression.codec=snappy
queued.max.messages.kbytes=10000000
fetch.message.max.bytes=10000000
auto.offset.reset=smallest
</code></pre><p>&lt;/details&gt;</p><p>&lt;details&gt; &lt;summary&gt;Configuring in a Containerized Environment&lt;/summary&gt;</p><p>The Apache Kafka plugin uses sensible default configuration values but these can be modified via configuration entries in the HPCC Systems Helm chart.</p><p>Configuration entries can be placed in the global section of your Helm chart (which means that the settings are applied everywhere) or they can be placed within a component (meaning, they apply to only that component). The latter is useful for settings things differently for differently-configured Thors, for instance.</p><p>Note that global and per-component configuration settings are <strong>merged</strong>. Per- component settings override global settings. Further details regarding the merge are at the end of this section.</p><p>There are two types of configurations: Global and per-topic. Some configuration parameters are applicable only to publishers (producers, in Apache Kafka&#39;s terminology), others only to consumers, and some to both. Details on the supported configuration parameters can be found on the <a href="https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md" target="_blank" rel="noreferrer">librdkafka configuration page</a>.</p><p>A configuration block is a Helm entry with a series of key/value parameters, formatted like this:</p><pre><code>plugins:
  kafka:
    global:
      - name: key
        value: value
      - name: key
        value: value
      ...
      - name: key
        value: value
</code></pre><p>This block can be added to the globals section of the Helm chart or to a specific component.</p><p>Whenever a new connection is created (either publisher or consumer) the plugin will scan for configuration entries. The global configuration block should be named <code>global</code>. Per-topic configuration blocks are also supported, and they can be different for a publisher or a consumer. For a publisher, the naming convention is <code>publisher_topic_\\&lt;TopicName\\&gt;</code> and for a consumer it is <code>consumer_topic_\\&lt;TopicName\\&gt;</code>. In both cases, <code>\\&lt;TopicName\\&gt;</code> is the name of the topic you are publishing to or consuming from.</p><p>Settings that affect the protocol used to connect to the Kafka broker (such as using SSL) should be placed only in the global configuration block, not in any per-topic configuration block.</p><p>Configuration parameters loaded from a Helm chart override those set by the plugin with one exception: the <code>metadata.broker.list</code> setting, if found in a configuration block, is ignored. Apache Kafka brokers are always set in ECL.</p><p>If configuration blocks are found in multiple locations, their keys/values are <strong>merged</strong>. The order of merging is:</p><pre><code>1. Kafka global settings from the chart&#39;s Global/plugins section
2. Kafka global settings from the per-component section
3. Kafka consumer/producer settings from the chart&#39;s Global/plugins section
4. Kafka consumer/producer settings from the per-component section
</code></pre><p>The following configuration parameters are set by the plugin for publishers, overriding their normal default values:</p><pre><code>- name: queue.buffering.max.messages
  value: 1000000
- name: compression.codec
  value: snappy
- name: message.send.max.retries
  value: 3
- name: retry.backoff.ms
  value: 500
</code></pre><p>The following configuration parameters are set by the plugin for consumers, overriding their normal default values:</p><pre><code>- name: compression.codec
  value: snappy
- name: queued.max.messages.kbytes
  value: 10000000
- name: fetch.message.max.bytes
  value: 10000000
- name: auto.offset.reset
  value: smallest
</code></pre><p>&lt;/details&gt;</p><h2 id="publishing-messages-with-the-plugin" tabindex="-1">Publishing messages with the plugin <a class="header-anchor" href="#publishing-messages-with-the-plugin" aria-label="Permalink to &quot;Publishing messages with the plugin&quot;">​</a></h2><p>Publishing string messages begins with instantiating an ECL module that defines the Apache Kafka cluster and the topic into which the messages will be posted. The definition of the module is:</p><pre><code>KafkaPublisher(VARSTRING topic, VARSTRING brokers = &#39;localhost&#39;) := MODULE
    ...
END
</code></pre><p>The module requires you to designate a topic by name and, optionally, at least one Apache Kafka broker. The format of the broker is <code>BrokerName[:port]</code> where <code>BrokerName</code> is either an IP address or a DNS name of a broker. You can optionally include a port number if the default Apache Kafka broker port is not used. Multiple brokers can be listed, separated by commas. Only one broker in an Apache Kafka cluster is required; the rest can be discovered once a connection is made.</p><p>Example instantiating a publishing module:</p><pre><code>p := kafka.KafkaPublisher(&#39;MyTopic&#39;, &#39;10.211.55.13&#39;);
</code></pre><p>The module contains an exported function for publishing a message, defined as:</p><pre><code>BOOLEAN PublishMessage(CONST UTF8 message, CONST UTF8 key = &#39;&#39;);
</code></pre><p>The module function requires a string message and allows you to specify a &#39;key&#39; that affects how Apache Kafka stores the message. Key values act a lot like the expression argument in ECL&#39;s DISTRIBUTE() function: Messages with the same key value wind up on the same Apache Kafka partition within the topic. This can affect how consumers retrieve the published messages. More details regarding partitions and how keys are used can be found Apache Kafka&#39;s <a href="https://kafka.apache.org/documentation.html#introduction" target="_blank" rel="noreferrer">introduction</a>. If a key value is not supplied than the messages are distributed among the available partitions for that topic.</p><p>Examples:</p><pre><code>p.PublishMessage(&#39;This is a test message&#39;);
p.PublishMessage(&#39;A keyed message&#39;, &#39;MyKey&#39;);
p.PublishMessage(&#39;Another keyed message&#39;, &#39;MyKey&#39;);
</code></pre><p>Note that keys are not retrieved by the ECL Apache Kafka consumer. They are used only to determine how the messages are stored.</p><p>You can find out how many partitions are available in a publisher&#39;s topic by calling the following module function:</p><pre><code>partitionCount := p.GetTopicPartitionCount();
</code></pre><p><code>GetTopicPartitionCount()</code> returns zero if the topic has not been created or there are has been an error.</p><h2 id="consuming-messages-with-the-plugin" tabindex="-1">Consuming messages with the plugin <a class="header-anchor" href="#consuming-messages-with-the-plugin" aria-label="Permalink to &quot;Consuming messages with the plugin&quot;">​</a></h2><p>As with publishing, consuming string messages begins with instantiating an ECL module that defines the Apache Kafka cluster and the topic from which the messages will be read. The definition of the module is:</p><pre><code>KafkaConsumer(VARSTRING topic,
              VARSTRING brokers = &#39;localhost&#39;,
              VARSTRING consumerGroup = &#39;hpcc&#39;) := MODULE
    ...
END
</code></pre><p>The module requires you to designate a topic by name. Optionally, you may also cite at least one Apache Kafka broker and a consumer group. The format and requirements for a broker are the same as for instantiating a KafkaPublisher module. Consumer groups in Apache Kafka allow multiple consumer instances, like Thor nodes, to form a &quot;logical consumer&quot; and be able to retrieve messages in parallel and without duplication. See the &quot;Consumers&quot; subtopic in Apache Kafka&#39;s <a href="https://kafka.apache.org/documentation.html#introduction" target="_blank" rel="noreferrer">introduction</a> for more details.</p><p>Example:</p><pre><code>c := kafka.KafkaConsumer(&#39;MyTopic&#39;, &#39;10.211.55.13&#39;);
</code></pre><p>The module contains an exported function for consuming messages, defined as:</p><pre><code>DATASET(KafkaMessage) GetMessages(INTEGER4 maxRecords);
</code></pre><p>This function returns a new dataset containing messages consumed by the topic defined in the module. The layout for that dataset is:</p><pre><code>KafkaMessage := RECORD
    UNSIGNED4   partition;
    INTEGER8    offset;
    UTF8        message;
END;
</code></pre><p>Example retrieving up to 10,000 messages:</p><pre><code>myMessages := c.GetMessages(10000);
</code></pre><p>After you consume some messages it may be beneficial to track the last-read offset from each Apache Kafka topic partition. The following module function does that:</p><pre><code>DATASET(KafkaMessageOffset) LastMessageOffsets(DATASET(KafkaMessage) messages);
</code></pre><p>Basically, you pass in the just-consumed message dataset to the function and get back a small dataset containing just the partition numbers and the last-read message&#39;s offset. The layout of the returned dataset is:</p><pre><code>KafkaMessageOffset := RECORD
    UNSIGNED4   partitionNum;
    INTEGER8    offset;
END;
</code></pre><p>Example call:</p><pre><code>myOffsets := c.LastMessageOffsets(myMessages);
</code></pre><p>If you later find out that you need to &quot;rewind&quot; your consumption -- read old messages, in other words -- you can use the data within a KafkaMessageOffset dataset to reset your consumers, making the next <code>GetMessages()</code> call pick up from that point. Use the following module function to reset the offsets:</p><pre><code>UNSIGNED4 SetMessageOffsets(DATASET(KafkaMessageOffset) offsets);
</code></pre><p>The function returns the number of partitions reset (which should equal the number of records you&#39;re handing the function).</p><p>Example call:</p><pre><code>numPartitionsReset := c.SetMessageOffsets(myOffsets);
</code></pre><p>You can easily reset all topic partitions to their earliest point with the following module function:</p><pre><code>UNSIGNED4 ResetMessageOffsets();
</code></pre><p>This function returns the number of partitions reset.</p><p>Example call:</p><pre><code>numPartitionsReset := c.ResetMessageOffsets();
</code></pre><p>You can find out how many partitions are available in a consumers&#39;s topic by calling the following module function:</p><pre><code>partitionCount := c.GetTopicPartitionCount();
</code></pre><p><code>GetTopicPartitionCount()</code> returns zero if the topic has not been created or there are has been an error.</p><h2 id="complete-ecl-examples" tabindex="-1">Complete ECL Examples <a class="header-anchor" href="#complete-ecl-examples" aria-label="Permalink to &quot;Complete ECL Examples&quot;">​</a></h2><p>The following code will publish 100K messages to a topic named &#39;MyTestTopic&#39; on an Apache Kafka broker located at address 10.211.55.13. If you are running a single-node HPCC cluster and have installed Kafka on the same node, you can use &#39;localhost&#39; instead (or omit the parameter, as it defaults to &#39;localhost&#39;).</p><h3 id="publishing" tabindex="-1">Publishing <a class="header-anchor" href="#publishing" aria-label="Permalink to &quot;Publishing&quot;">​</a></h3><pre><code>IMPORT kafka;

MyDataLayout := RECORD
    UTF8    message;
END;

ds := DATASET
    (
        100000,
        TRANSFORM
            (
                MyDataLayout,
                SELF.message := U8&#39;Test message &#39; + (UTF8)COUNTER
            ),
        DISTRIBUTED
    );

p := kafka.KafkaPublisher(&#39;MyTestTopic&#39;, brokers := &#39;10.211.55.13&#39;);

APPLY(ds, ORDERED(p.PublishMessage(message)));
</code></pre><h3 id="consuming" tabindex="-1">Consuming <a class="header-anchor" href="#consuming" aria-label="Permalink to &quot;Consuming&quot;">​</a></h3><p>This code will read the messages written by the publishing example, above. It will also show the number of partitions in the topic and the offsets of the last-read messages.</p><pre><code>IMPORT kafka;

c := kafka.KafkaConsumer(&#39;MyTestTopic&#39;, brokers := &#39;10.211.55.13&#39;);

ds := c.GetMessages(200000);
offsets := c.LastMessageOffsets(ds);
partitionCount := c.GetTopicPartitionCount();

OUTPUT(ds, NAMED(&#39;MessageSample&#39;));
OUTPUT(COUNT(ds), NAMED(&#39;MessageCount&#39;));
OUTPUT(offsets, NAMED(&#39;LastMessageOffsets&#39;));
OUTPUT(partitionCount, NAMED(&#39;PartitionCount&#39;));
</code></pre><h3 id="resetting-offsets" tabindex="-1">Resetting Offsets <a class="header-anchor" href="#resetting-offsets" aria-label="Permalink to &quot;Resetting Offsets&quot;">​</a></h3><p>Resetting offsets is useful when you have a topic already published with messages and you need to reread its messages from the very beginning.</p><pre><code>IMPORT kafka;

c := kafka.KafkaConsumer(&#39;MyTestTopic&#39;, brokers := &#39;10.211.55.13&#39;);

c.ResetMessageOffsets();
</code></pre><h2 id="behaviour-and-implementation-details" tabindex="-1">Behaviour and Implementation Details <a class="header-anchor" href="#behaviour-and-implementation-details" aria-label="Permalink to &quot;Behaviour and Implementation Details&quot;">​</a></h2><h3 id="partitioning-within-apache-kafka-topics" tabindex="-1">Partitioning within Apache Kafka Topics <a class="header-anchor" href="#partitioning-within-apache-kafka-topics" aria-label="Permalink to &quot;Partitioning within Apache Kafka Topics&quot;">​</a></h3><p>Topic partitioning is covered in Apache Kafka&#39;s <a href="https://kafka.apache.org/documentation.html#introduction" target="_blank" rel="noreferrer">introduction</a>. There is a performance relationship between the number of partitions in a topic and the size of the HPCC cluster when consuming messages. Ideally, the number of partitions will exactly equal the number of HPCC nodes consuming messages. For Thor, this means the total number of workers rather than the number of nodes, as that can be different in a multi-worker setup. For Roxie, the number is always one. If there are fewer partitions than nodes (workers) then not all of your cluster will be utilized when consuming messages; if there are more partitions than nodes (workers) then some nodes will be performing extra work, consuming from multiple partitions. In either mismatch case, you may want to consider using the ECL DISTRIBUTE() function to redistribute your data before processing.</p><p>When messages are published without a &#39;key&#39; argument to a topic that has more than one partition, Apache Kafka will distribute those messages among the partitions. The distribution is not perfect. For example, if you publish 20 messages to a topic with two partitions, one partition may wind up with 7 messages and the other with 13 (or some other mix of message counts that total 20). When testing your code, be aware of this behavior and always request more messages than you publish. In the examples above, 100K messages were published but up to 200K messages were requested. This ensures that you receive all of the messages you publish. This is typically not an issue in a production environment, as your requested consumption message count is more a function of how much data you&#39;re willing to process in one step than with how many messages are actually stored in the topic.</p><p>Be aware that, by default, Apache Kafka will automatically create a topic that has never been seen before if someone publishes to it, and that topic will have only one partition. Both actions -- whether a topic is automatically created and how many partitions it will have -- are configurable within Apache Kafka.</p><h3 id="publisher-connections" tabindex="-1">Publisher Connections <a class="header-anchor" href="#publisher-connections" aria-label="Permalink to &quot;Publisher Connections&quot;">​</a></h3><p>This plugin caches the internal publisher objects and their connections. Publishing from ECL, technically, only writes the messages to a local cache. Those messages are batched and set to Apache Kafka for higher performance in a background thread. Because this batching can extend far beyond the time ECL spends sending the data to the local cache, the objects (and their connections) need to hang around for some additional time. The upside is that the cached objects and connections will be reused for subsequent publish operations, speeding up the entire process.</p><h3 id="consumer-connections" tabindex="-1">Consumer Connections <a class="header-anchor" href="#consumer-connections" aria-label="Permalink to &quot;Consumer Connections&quot;">​</a></h3><p>Unlike publisher objects, one consumer object is created per thread for each connection. A connection is to a specific broker, topic, consumer group, and partition number combination. The consumer objects and connections live only as long as needed.</p><h3 id="saved-topic-offsets" tabindex="-1">Saved Topic Offsets <a class="header-anchor" href="#saved-topic-offsets" aria-label="Permalink to &quot;Saved Topic Offsets&quot;">​</a></h3><p>By default, in a bare-metal environment, consumers save to a file the offset of the last-read message from a given topic, consumer group, and partition combination. The offset is saved so that the next time the consumer is fired up for that particular connection combination, the consumption process can pick up where it left off. The file is saved to the HPCC engine&#39;s data directory which is typically <code>/var/lib/HPCCSystems/mythor/</code>, <code>/var/lib/HPCCSystems/myroxie/</code> or <code>/var/lib/HPCCSystems/myeclagent/</code> depending on the engine you&#39;re using (the exact path may be different if you have named an engine differently in your HPCC configuration). The format of the saved offset filename is <code>\\&lt;TopicName\\&gt;-\\&lt;PartitionNum\\&gt;-\\&lt;ConsumerGroup\\&gt;.offset</code>.</p><p>Note that saving partition offsets is engine-specific in a bare-metal environment. One practical consideration of this is that you cannot have one engine (e.g. Thor) consume from a given topic and then have another engine (e.g. Roxie) consume the next set of messages from that topic. Both engines can consume messages without a problem, but they will not track each other&#39;s last-read positions. Note that in a containerized environment different engines will use each others&#39; offsets provided that they use the same consumer group.</p>`,110)]))}const f=a(s,[["render",i]]);export{d as __pageData,f as default};
