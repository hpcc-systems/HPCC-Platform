import{_ as a,a as s,o as t,ag as o}from"./chunks/framework.Do1Zayaf.js";const h=JSON.parse('{"title":"Kubernetes Scaling","description":"","frontmatter":{},"headers":[],"relativePath":"helm/examples/scaling/README.md","filePath":"helm/examples/scaling/README.md","lastUpdated":1761843274000}'),n={name:"helm/examples/scaling/README.md"};function r(i,e,l,c,p,d){return t(),s("div",null,e[0]||(e[0]=[o(`<h1 id="kubernetes-scaling" tabindex="-1">Kubernetes Scaling <a class="header-anchor" href="#kubernetes-scaling" aria-label="Permalink to &quot;Kubernetes Scaling&quot;">​</a></h1><p>A major feature provided by Kubernetes environments is the ability to automatically scale workload resources (such as a Deployment or StatefulSet), to match actual workloads. Scaling workload resources or available nodes dynamically is a powerful way to manage performance and associated costs.</p><p>Kubernetes provides 3 main types of scaling: Horizontal (HPA), Vertical (VPA), and Cluster. HPA adjusts the replica count of a given application, whereas VPA adjusts the resource requests and limits of a container. The number of nodes available to a cluster is also adjustable.</p><h2 id="horizontal-pod-autoscaler-hpa" tabindex="-1">Horizontal Pod Autoscaler (HPA) <a class="header-anchor" href="#horizontal-pod-autoscaler-hpa" aria-label="Permalink to &quot;Horizontal Pod Autoscaler (HPA)&quot;">​</a></h2><p>HPAs automatically adjusts a workload resource to handle actual demand by increasing or decreasing the number of pods as opposed to adjusting resources available to running pods. When the actual load recedes, and current pods count is above the declared minimum, the HPA directs the affected workload resource to scale back down.</p><p>HPA Kubernetes objects are directly associated with the workload resource to be automatically scaled, along with the desired min and max pod count, and a metric threshold used to determine when to scale up or down. There are 3 major metric types tracked by HPAs, resource (CPU, Memory), custom (specific to an appliation within the container), and external (stemming from outside the cluster).</p><p>For HPAs to function, a source of metrics must be available. Basic resource metrics (CPU, Memory) are collected and made available by The Kubernetes Metrics Server. This server must be configured and deployed for resource metrics based HPAs to function. Metric Server deployment notes can be foud <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/" target="_blank" rel="noreferrer">here</a></p><p>See this document for <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/" target="_blank" rel="noreferrer">K8s HPA details</a>.</p><h3 id="hpcc-systems-hpa-support" tabindex="-1">HPCC Systems HPA support <a class="header-anchor" href="#hpcc-systems-hpa-support" aria-label="Permalink to &quot;HPCC Systems HPA support&quot;">​</a></h3><p>Some of HPCC Systems&#39; components are candidates for HPA configuration (ESP, ECLCCServer, etc.) and can be configured directly within the HPCC helm deployment.</p><p>Example ESP application &#39;eclwatch&#39; configured to horizontally auto-scale from 1 up to 3 replicas based on cpu utilization of 80:</p><div class="language-console vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">console</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  esp:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">- name: eclwatch</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  application: eclwatch</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  auth: none</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  replicas: 1</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  hpa:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    minReplicas: 1</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    maxReplicas: 3</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    metrics:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    - type: Resource</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      name: &quot;cpu&quot;</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      target: </span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        type: Utilization</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        value: &quot;80&quot;</span></span></code></pre></div><p>Otherwise, an HPA can be created manually utilizing the kubectl command line tool.</p><p>Example HPA to autoscale the eclwatch deployment based on CPU utilization at 80%, with a count of pods from 1 to a maxium of 3:</p><div class="language-console vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">console</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  kubectl autoscale deployment eclwatch --cpu-percent=80 --min=1 --max=3</span></span></code></pre></div><p>HPA behavior can be further configured by the hpa.behavior section for both up and down scaling. This section is optional and default values are assigned. Exact definitions are provided in the &quot;behavior&quot; section of the Kubernetes HPA syntax doc linked below.</p><p>HPCC Systems supports the Kubernetes HPA version 2 sytax as defined <a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/#HorizontalPodAutoscalerSpec" target="_blank" rel="noreferrer">here</a></p><h2 id="vertical-pod-autoscaler-vpa" tabindex="-1">Vertical Pod Autoscaler (VPA) <a class="header-anchor" href="#vertical-pod-autoscaler-vpa" aria-label="Permalink to &quot;Vertical Pod Autoscaler (VPA)&quot;">​</a></h2><p>Vertical Pod Autoscaler (VPA) is a Kubernetes feature that helps determine the appropriate ammount of resource requests associated with application pods. Kubernetes scheduler does not re-evaluate the pod’s resource requirements once they have been deployed with a given set of requests. If the pod is initially over-allocated resources, those resources can go unused and the cost will be unnecessarily affected. On the other hand, if a pod is not afforded sufficient resources, performance will suffer and pods might be removed. VPA helps allocate the appropriate ammount of resources.</p><p>When enabled and configured VPA sets the requests automatically based on usage and allow proper scheduling onto nodes allowing suffient resources be made available for each pod. It will also maintain ratios between limits and requests that were specified in initial containers configuration.</p><p>Similarly to HPA, the Kubernetes Metrics Server must be deployed on the cluster in order to make the VPAs funtional. Read more about <a href="https://github.com/kubernetes-sigs/metrics-server" target="_blank" rel="noreferrer">Metrics Server</a>.</p><p>The k8s VPA needs to be installed as well. Read more about <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#installation" target="_blank" rel="noreferrer">K8s VPA Installing</a></p><p>VPAs must be manually created, as HPCC does not currently support integrated VPA configuration.</p><h2 id="cluster-autoscaler" tabindex="-1">Cluster Autoscaler <a class="header-anchor" href="#cluster-autoscaler" aria-label="Permalink to &quot;Cluster Autoscaler&quot;">​</a></h2><p>Cluster Autoscaler (CA) automatically increases or decreases the number of cluster nodes due to insufficient resources to run a pod (adds a node) or when a node remains underutilized, and its pods can be assigned to another node (removes a node).</p>`,25)]))}const m=a(n,[["render",r]]);export{h as __pageData,m as default};
