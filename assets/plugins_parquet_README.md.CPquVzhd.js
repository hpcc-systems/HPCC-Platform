import{_ as e,a,o as n,ag as s}from"./chunks/framework.Do1Zayaf.js";const u=JSON.parse('{"title":"Parquet Plugin for HPCC Systems","description":"","frontmatter":{},"headers":[],"relativePath":"plugins/parquet/README.md","filePath":"plugins/parquet/README.md","lastUpdated":1761843274000}'),i={name:"plugins/parquet/README.md"};function l(r,t,o,d,p,c){return n(),a("div",null,t[0]||(t[0]=[s(`<h1 id="parquet-plugin-for-hpcc-systems" tabindex="-1">Parquet Plugin for HPCC Systems <a class="header-anchor" href="#parquet-plugin-for-hpcc-systems" aria-label="Permalink to &quot;Parquet Plugin for HPCC Systems&quot;">​</a></h1><p>The Parquet Plugin for HPCC Systems is a powerful tool designed to facilitate the fast transfer of data stored in a columnar format to the ECL (Enterprise Control Language) data format. This plugin provides seamless integration between Parquet files and HPCC Systems, enabling efficient data processing and analysis.</p><h2 id="installation" tabindex="-1">Installation <a class="header-anchor" href="#installation" aria-label="Permalink to &quot;Installation&quot;">​</a></h2><p>The Parquet Plugin comes bundled with the HPCC Platform, so there&#39;s no need for a separate download. When you install or update HPCC Systems, you&#39;ll automatically have access to the latest version of the Parquet Plugin.</p><p>If the Parquet plugin is missing, you can either ensure it&#39;s enabled by turning on the <code>-DUSE_PARQUET=ON</code> option during the build, or follow the instructions below to install it manually.</p><p>The plugin uses vcpkg and can be installed by creating a separate build directory from the platform and running the following commands:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>cd ./parquet-build</span></span>
<span class="line"><span>cmake -DPARQUETEMBED=ON ../HPCC-Platform</span></span>
<span class="line"><span>make -j4 package</span></span>
<span class="line"><span>sudo dpkg -i ./hpccsystems-plugin-parquetembed_\\&lt;version\\&gt;.deb</span></span></code></pre></div><h2 id="documentation" tabindex="-1">Documentation <a class="header-anchor" href="#documentation" aria-label="Permalink to &quot;Documentation&quot;">​</a></h2><p><a href="https://www.doxygen.nl/index.html" target="_blank" rel="noreferrer">Doxygen</a> can be used to create nice HTML documentation for the code. Call/caller graphs are also generated for functions if you have <a href="https://www.graphviz.org/download/" target="_blank" rel="noreferrer">dot</a> installed and available on your path.</p><p>Assuming <code>doxygen</code> is on your path, you can build the documentation via:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>cd plugins/parquet</span></span>
<span class="line"><span>doxygen Doxyfile</span></span></code></pre></div><h2 id="features" tabindex="-1">Features <a class="header-anchor" href="#features" aria-label="Permalink to &quot;Features&quot;">​</a></h2><p>The Parquet Plugin offers the following main functions:</p><h3 id="regular-files" tabindex="-1">Regular Files <a class="header-anchor" href="#regular-files" aria-label="Permalink to &quot;Regular Files&quot;">​</a></h3><h4 id="_1-reading-parquet-files" tabindex="-1">1. Reading Parquet Files <a class="header-anchor" href="#_1-reading-parquet-files" aria-label="Permalink to &quot;1. Reading Parquet Files&quot;">​</a></h4><p>The Read function allows ECL programmers to create an ECL dataset from both regular and partitioned Parquet files. It leverages the Apache Arrow interface for Parquet to efficiently stream data from ECL to the plugin, ensuring optimized data transfer.</p><p>In order to read a Parquet file it is necessary to define the record structure of the file you intend to read with the names of the fields as stored in the Parquet file and the type that you wish to read them as. It is possible for a Parquet file to have field names that contain characters that are incompatible with the ECL field name definition. For example, ECL field names are case insensitive causing an issue when trying to read Parquet fields with uppercase letters. To read field names of this type an XPATH can be passed as seen in the following example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>layout := RECORD</span></span>
<span class="line"><span>    STRING name;</span></span>
<span class="line"><span>    STRING job_id {XPATH(&#39;jobID&#39;)};</span></span>
<span class="line"><span>END;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>dataset := ParquetIO.Read(layout, &#39;/source/directory/data.parquet&#39;);</span></span></code></pre></div><h4 id="_2-writing-parquet-files" tabindex="-1">2. Writing Parquet Files <a class="header-anchor" href="#_2-writing-parquet-files" aria-label="Permalink to &quot;2. Writing Parquet Files&quot;">​</a></h4><p>The Write function empowers ECL programmers to write ECL datasets to Parquet files. By leveraging the Parquet format&#39;s columnar storage capabilities, this function provides efficient compression and optimized storage for data. There is an optional argument that sets the overwrite behavior of the plugin. The default value is false meaning it will throw an error if the target file already exists. If overwrite is set to true the plugin will check for files that match the target path passed in and delete them first before writing new files.</p><p>The Parquet Plugin supports all available Arrow compression types. Specifying the compression when writing is optional and defaults to Uncompressed. The options for compressing your files are Snappy, GZip, Brotli, LZ4, ZSTD, Uncompressed.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>overwriteOption := TRUE;</span></span>
<span class="line"><span>compressionOption := &#39;Snappy&#39;;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>ParquetIO.Write(inDataset, &#39;/output/directory/data.parquet&#39;, overwriteOption, compressionOption);</span></span></code></pre></div><h3 id="partitioned-files-tabular-datasets" tabindex="-1">Partitioned Files (Tabular Datasets) <a class="header-anchor" href="#partitioned-files-tabular-datasets" aria-label="Permalink to &quot;Partitioned Files (Tabular Datasets)&quot;">​</a></h3><p>The Parquet plugin supports both Hive Partitioning and Directory Partitioning. Hive partitioning uses a key-value partitioning scheme for selecting directory names. For example, the file under <code>dataset/year=2017/month=01/data0.parquet</code> contains only data for which the year equals 2017 and the month equals 01. The second partitioning scheme, Directory Partitioning, is similar, but rather than having key-value pairs the partition keys are inferred in the file path. For example, instead of having <code>/year=2017/month=01/day=01</code> the file path would be <code>/2017/01/01</code>.</p><h4 id="_1-reading-partitioned-files" tabindex="-1">1. Reading Partitioned Files <a class="header-anchor" href="#_1-reading-partitioned-files" aria-label="Permalink to &quot;1. Reading Partitioned Files&quot;">​</a></h4><p>For reading partitioned files, pass in the target directory to the read function of the type of partition you are using. For directory partitioning, a list of the field names that make up the partitioning schema is required because it is not included in the directory structure like hive partitioning.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>github_dataset := ParquetIO.HivePartition.Read(layout, &#39;/source/directory/partitioned_dataset&#39;);</span></span>
<span class="line"><span></span></span>
<span class="line"><span>github_dataset := ParquetIO.DirectoryPartition.Read(layout, &#39;source/directory/partitioned_dataset&#39;, &#39;year;month;day&#39;)</span></span></code></pre></div><h4 id="_2-writing-partitioned-files" tabindex="-1">2. Writing Partitioned Files <a class="header-anchor" href="#_2-writing-partitioned-files" aria-label="Permalink to &quot;2. Writing Partitioned Files&quot;">​</a></h4><p>To select the fields that you wish to partition your data on pass in a string of semicolon seperated field names. If the fields you select create too many subdirectories you may need to partition your data on different fields. The rowSize field defaults to 100000 rows and determines how many rows to put in each part of the output files. Writing a partitioned file to a directory that already contains data will fail unless the overwrite option is set to true. If the overwrite option is set to true and the target directory is not empty the plugin will first erase the contents of the target directory before writing the new files.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ParquetIO.HivePartition.Write(outDataset, rowSize, &#39;/source/directory/partioned_dataset&#39;, overwriteOption, &#39;year;month;day&#39;);</span></span>
<span class="line"><span></span></span>
<span class="line"><span>ParquetIO.DirectoryPartition.Write(outDataset, rowSize, &#39;/source/directory/partioned_dataset&#39;, overwriteOption, &#39;year;month;day&#39;);</span></span></code></pre></div><h2 id="apache-parquet-arrow-to-ecl-type-mappings" tabindex="-1">Apache Parquet/Arrow to ECL Type Mappings <a class="header-anchor" href="#apache-parquet-arrow-to-ecl-type-mappings" aria-label="Permalink to &quot;Apache Parquet/Arrow to ECL Type Mappings&quot;">​</a></h2><h3 id="ecl-record-to-arrow-schema-mappings" tabindex="-1">ECL Record to Arrow Schema Mappings <a class="header-anchor" href="#ecl-record-to-arrow-schema-mappings" aria-label="Permalink to &quot;ECL Record to Arrow Schema Mappings&quot;">​</a></h3><table tabindex="0"><thead><tr><th style="text-align:left;">ECL Record Type</th><th style="text-align:left;">Apache Arrow/Parquet Type</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;">BOOLEAN</td><td style="text-align:left;">Boolean</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">INTEGER</td><td style="text-align:left;">Int64</td><td style="text-align:left;">Defaults to 8 bytes (64-bit). Can be explicitly sized using INTEGER1, INTEGER2, INTEGER4, or INTEGER8</td></tr><tr><td style="text-align:left;">INTEGER8</td><td style="text-align:left;">Int64</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">UNSIGNED</td><td style="text-align:left;">UInt64</td><td style="text-align:left;">Defaults to 8 bytes (64-bit). Can be explicitly sized using UNSIGNED1, UNSIGNED2, UNSIGNED4, or UNSIGNED8</td></tr><tr><td style="text-align:left;">UNSIGNED8</td><td style="text-align:left;">UInt64</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">REAL4</td><td style="text-align:left;">Float</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">REAL8</td><td style="text-align:left;">Double</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">DECIMAL</td><td style="text-align:left;">LargeString</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">STRING</td><td style="text-align:left;">LargeString</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">VARSTRING</td><td style="text-align:left;">LargeString</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">UTF8</td><td style="text-align:left;">LargeString</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">DATA</td><td style="text-align:left;">LargeBinary</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">DATA[n]</td><td style="text-align:left;">FixedSizeBinary[n]</td><td style="text-align:left;">Where n is the fixed length</td></tr><tr><td style="text-align:left;">SET OF</td><td style="text-align:left;">LargeList</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">RECORD</td><td style="text-align:left;">Struct</td><td style="text-align:left;"></td></tr></tbody></table><h3 id="default-supported-parquet-arrow-types-for-ecl" tabindex="-1">Default Supported Parquet/Arrow Types for ECL <a class="header-anchor" href="#default-supported-parquet-arrow-types-for-ecl" aria-label="Permalink to &quot;Default Supported Parquet/Arrow Types for ECL&quot;">​</a></h3><table tabindex="0"><thead><tr><th style="text-align:left;">Apache Parquet / Arrow Type</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;">Boolean</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">Int8, Int16, Int32, Int64</td><td style="text-align:left;">Size in ECL depends on the Arrow type</td></tr><tr><td style="text-align:left;">UInt8, UInt16, UInt32, UInt64</td><td style="text-align:left;">Size in ECL depends on the Arrow type</td></tr><tr><td style="text-align:left;">Float16 (Half Float)</td><td style="text-align:left;">Stored as REAL4 or REAL8. Processed in 64-bit buffer internally</td></tr><tr><td style="text-align:left;">Float</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">Double</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">Decimal128</td><td style="text-align:left;">Precision and scale in the Parquet file schema should match the ECL record definition</td></tr><tr><td style="text-align:left;">Decimal256</td><td style="text-align:left;">Precision and scale in the Parquet schema should match the ECL record definition. Note that arrow::Decimal256 supports larger precision and scale than the 64-digit maximum in ECL.</td></tr><tr><td style="text-align:left;">Date32, Date64</td><td style="text-align:left;">Stored as days since epoch</td></tr><tr><td style="text-align:left;">Time32, Time64</td><td style="text-align:left;">Stored as milliseconds or microseconds since midnight (time of day)</td></tr><tr><td style="text-align:left;">Timestamp</td><td style="text-align:left;">Stored as microseconds since epoch</td></tr><tr><td style="text-align:left;">Duration</td><td style="text-align:left;">Measure of elapsed time in either seconds, milliseconds, microseconds or nanoseconds</td></tr><tr><td style="text-align:left;">LargeString</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">LargeBinary</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">FixedSizeBinary</td><td style="text-align:left;">Fixed-length binary data</td></tr><tr><td style="text-align:left;">LargeList</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">FixedSizeList</td><td style="text-align:left;">With fixed number of elements</td></tr><tr><td style="text-align:left;">Struct</td><td style="text-align:left;">Nested structure</td></tr></tbody></table><p>For more detailed information about Apache Arrow data types, refer to the <a href="https://arrow.apache.org/docs/cpp/api/datatype.html" target="_blank" rel="noreferrer">Apache Arrow Documentation</a>.</p><h2 id="example-usage" tabindex="-1">Example Usage <a class="header-anchor" href="#example-usage" aria-label="Permalink to &quot;Example Usage&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>IMPORT Parquet;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Record definition with both fixed-size and large binary fields</span></span>
<span class="line"><span>BinaryTypesRecord := RECORD</span></span>
<span class="line"><span>    UNSIGNED1 id;</span></span>
<span class="line"><span>    DATA10 fixedBinary;  // Fixed-size binary (10 bytes)</span></span>
<span class="line"><span>    DATA largeBinary;    // Large binary (variable size)</span></span>
<span class="line"><span>END;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Sample dataset</span></span>
<span class="line"><span>sampleData := DATASET([</span></span>
<span class="line"><span>    {1, (DATA10)123.45, (DATA)6789.01},</span></span>
<span class="line"><span>    {2, (DATA10)234.56, (DATA)7890.12}</span></span>
<span class="line"><span>], BinaryTypesRecord);</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Write and read using ParquetIO</span></span>
<span class="line"><span>ParquetIO.Write(sampleData, &#39;/var/lib/HPCCSystems/mydropzone/BinaryTypes.parquet&#39;);</span></span>
<span class="line"><span>readData := ParquetIO.Read(BinaryTypesRecord, &#39;/var/lib/HPCCSystems/mydropzone/BinaryTypes.parquet&#39;);</span></span>
<span class="line"><span></span></span>
<span class="line"><span>OUTPUT(readData);</span></span></code></pre></div><p>This example demonstrates how to define an ECL record structure with fixed-size and variable-size binary fields, create a sample dataset, write it to a Parquet file, and then read it back using ParquetIO, showcasing the handling of different binary data types in HPCC Systems.</p>`,39)]))}const g=e(i,[["render",l]]);export{u as __pageData,g as default};
