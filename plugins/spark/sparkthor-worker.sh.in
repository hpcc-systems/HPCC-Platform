#!/bin/bash
################################################################################
#    HPCC SYSTEMS software Copyright (C) 2018 HPCC SystemsÂ®.
#
#    Licensed under the Apache License, Version 2.0 (the "License");
#    you may not use this file except in compliance with the License.
#    You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS,
#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    See the License for the specific language governing permissions and
#    limitations under the License.
################################################################################

_command=${1:-start}
_component=${2:-mysparkthor}

#### Uncomment following lines for verbose debugging
#exec 3>&2 2>@LOG_PATH@/${_component}/sparkthor-worker.${_command}.debug
#set -x

export logfile="@LOG_PATH@/${_component}/sparkthor-worker.log"
source @INSTALL_DIR@/etc/init.d/hpcc_common

if [[ "${_command}" == "start" ]]; then
    mkdir -p @RUNTIME_PATH@/${_component} @LOCK_PATH@ @PID_PATH@ @LOG_PATH@/${_component}
fi

cd @RUNTIME_PATH@/${_component}

MASTER_IP="$(@ADMIN_PATH@/configgen -env @CONFIG_DIR@/@ENV_XML_FILE@ -listall -c ${_component} | awk -F "," '{print $3;}')"
    
log "rsync -e \"ssh -o LogLevel=QUIET -o StrictHostKeyChecking=no\" --timeout=60 ${MASTER_IP}:@RUNTIME_PATH@/${_component}/spark-hpcc-env.sh @RUNTIME_PATH@/${_component}/spark-hpcc-env.sh"
rsync_att=3
rsync_stat=1
while [[ $rsync_stat -ne 0 && $rsync_att -gt 0 ]] ; do
    rsync -e "ssh -o LogLevel=QUIET -o StrictHostKeyChecking=no" --timeout=60 ${MASTER_IP}:@RUNTIME_PATH@/${_component}/spark-hpcc-env.sh spark-hpcc-env.sh
    rsync_stat=$?
    ((rsync_att--))
    log "rsync returns ${rsync_stat}"
done
if [ ! -f @RUNTIME_PATH@/${_component}/spark-hpcc-env.sh ] ; then
    log "Error, $workersfname file missing"
    exit 1
fi
source @RUNTIME_PATH@/${_component}/spark-hpcc-env.sh

MASTER_URL="spark://${MASTER_IP}:${SPARK_MASTER_PORT}"

#external provided spark-hadoop file
@INSTALL_DIR@/externals/spark-hadoop/sbin/${_command}-slave.sh ${MASTER_URL}
